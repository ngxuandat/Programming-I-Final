So hello everybody and welcome to Deep Learning for Coders, Lesson One.
This is the fourth year that we've done this, but it's a very different and very special
version for a number of reasons.
The first reason it's different is because we are bringing it to you live from day number
one of a complete shutdown.
Oh, not a complete shutdown, but nearly a complete shutdown of San Francisco.
We're going to be recording it over the next two months in the midst of this global pandemic.
So if things seem a little crazy sometimes in this course, I apologize.
So that's why this is happening.
The other reason it's special is because it's, we're trying to make this our definitive version,
right.
Since we've been doing this for a while now, we've finally got to the point where we almost
feel like we know what we're talking about.
To the point that Sylvain and I have actually written a book and we've actually written
a piece of software from scratch, called the fastai library version 2.
We've written a peer-reviewed paper about this library.
So this is kind of designed to be like the version of the course that is hopefully going
to last a while.
The syllabus is based very closely on this book, right.
So if you want to read along properly as you go, please buy it.
And I say “please buy it” because actually the whole thing is also available for free
in the form of Jupyter notebooks.
And that is thanks to the huge generosity of O'Reilly Media, who have let us do that.
So you'll be able to see on the website for the course how to kind of access all this,
but here is the fastbook repo where you can read the whole damn thing.
At the moment as you see, it's a draft, but by the time you read this, it won't be.
So we have a big request here which is - the deal is this - you can read this thing for
free as Jupyter notebooks, but that is not as convenient as reading it on a Kindle or
in a paper book or whatever.
So, please don't turn this into a PDF, right.
Please don't turn it into a form designed more for reading, because kind of the whole
point is that you'll buy it.
Don't take advantage of O'Reilly's generosity by creating the thing that you know they're
not giving you for free.
And that's actually explicitly the license under which we're providing this as well.
So it's mainly a request, being a decent human being.
If you see somebody else not being a decent human being and stealing the book version
of the book, please tell them, “Please don't do that, it's not nice.”
And don't be that person.
So either way, you can read along with the syllabus in the book.
There's a couple of different versions of these notebooks, right.
There is the, there's the full notebook that has the entire prose, pictures, everything.
Now we actually wrote a system to turn notebooks into a printed book and sometimes that looks
kind of weird.
For example, here's a weird looking table and if you look in the actual book, it actually
looks like a proper table, right.
So sometimes you'll see like little weird bits, okay, they are not mistakes they are
bits where we can add information to help our book turn into a proper nice book so just
just ignore them.
Now when I say we, who is we?
While I mentioned one important part of the we is Sylvain.
Sylvain is my co-author of the book and fastai version 2 library, so he is my partner in
crime here.
The other key “we” here is Rachel Thomas and so maybe Rachel you can come and say hello.
She is the co-founder of fastai.
Hello yes I am the co-founder of fastai and I am also, lower sorry taller than Jeremy,
and I am the founding director of the Center for Applied Data Ethics at the University
of San Francisco.
Really excited to be a part of this course and I will be the voice you hear asking questions
from the forums.
Rachel and Sylvain are also the people in this group who actually understand math.
I am a mere philosophy graduate.
Rachel has a PhD.
Sylvain has written 10 books about math so if the math questions come along it's possible
I may pass them along.
But it is very nice to have an opportunity to work with people who understand this topic
so well.
Yes Yes Rachael, did you wanna sure oh thank you.
As Rachel mentioned the other area where she is you know real, has real world-class expertise
is data ethics, she is the founding director of the Centre for Applied Data Ethics, at
the University of San Francisco.
Thank you.
We are gonna be talking about data ethics throughout the course because well we happen
to think it's very important and so for those parts, although I'll generally be presenting
them they will be on the whole based on Rachel's Rachel's work because she actually knows what
she's talking about.
Although thanks to her I kind of know a bit about what I am talking about too.
Right, so that's that.
So should you be here, is there any point in you attempting to understand (I thought
I pressed the right button), understand deep learning.
Ok so what do you know should you should you be here.
Is there any point you attempting to learn deep learning or are you too stupid or you
don't have enough fast resources or whatever, because that's what a lot of people are telling
us.
They are saying you need teams of PhDs and massive data centers full of GPUs, otherwise
it's pointless.
Don't worry that is not at all true, couldn't be further from the truth.
In fact that the vast majority, sorry, a lot of world-class research and world-class industry
projects have come out of fastai alumni and fastai library-based projects and and elsewhere
which are created on a single GPU using a few dozen or a few hundred data points from
people that have no graduate level technical expertise, or in my case, I have no undergraduate
level technical expertise.
I'm just a philosophy major.
So there is - and we'll see it throughout the course -- but there is lots and lots and
lots of clear empirical evidence that you don't need lots of math, you don't need lots
of data, you don't need lots of expensive computers to do great stuff, with deep learning.
So just bear with us.
You'll be fine.
To do this course, you do need to code.
Preferably, you know how to code in Python.
But if you've done other languages, you can learn Python.
If the only languages you've done is something like Matlab, where you've used it more like
a scripty kind of thing, you might find it a bit - You will find it a bit heavier going.
But that's okay, stick with it.
You can learn Python as you go.
Is there any point learning deep learning?
Is it any good at stuff?
If you are hoping to build a brain, that is an AGI, I cannot promise we're gonna help
you with that.
And AGI stands for: Artificial General Intelligence.
Thank you.
What I can tell you though, is that in all of these areas, deep learning is the best-known
approach, to at least many versions of all of these things.
So it is not speculative at this point whether this is a useful tool.
It's a useful tool in lots and lots and lots of places.
Extremely useful tool.
And in many of these cases, it is equivalent to or better than human performance.
At least according to some particular narrow definition of things that humans do in these
kinds of areas.
So deep learning is pretty amazing.
And if you want to pause the video here and have a look through and try and pick some
things out that you think might look interesting and type that keyword and “deep learning”
into Google.
And you'll find lots of papers and examples and stuff like that.
Deep learning comes from a background of neural networks.
As you'll see deep learning is just a type of neural network learning.
A deep one.
We'll describe exactly what that means later.
And neural networks are certainly not a new thing.
They go back at least to 1943, when McCulloch and Pitts created a mathematical model of
an artificial neuron.
And got very excited about where that could get to.
And then in the 50's, Frank Rosenblatt then built on top of that - he basically created
some subtle changes to that mathematical model.
And he thought that with these subtle changes “we could witness the birth of the machine
that is capable of perceiving, recognizing and identifying its surroundings without any
human training or control”.
And he oversaw the building of this - extraordinary thing.
The Mark 1 Perceptron at Cornell.
So that was I think, this picture was 1961.
Thankfully nowadays, we don't have to build neural networks by running the damn wires
from neuron to neuron (artificial neuron to artificial neuron).
But you can kind of see the idea; lot of connections going on.
And you'll hear the word connection a lot, in this course, because that's what it's all
about.
Then we had the first AI winter, as it was known.
Which really, to a strong degree happened because an MIT professor named Marvin Minsky
and Papert wrote a book called perceptrons about Rosenblatt's invention in which they
pointed out that a single layer of these artificial neuron devices, actually couldn't learn some
critical things.
It was like impossible for them to learn something as simple as the Boolean XOR operator.
In the same book, they showed that using multiple layers of the devices actually would fix the
problem.
People ignore - didn't notice that part of the book.
And only noticed the limitation and people basically decided that neural networks are
gonna go nowhere.
And they kind of largely disappeared, for decades.
Until, in some ways, 1986.
A lot happened in the meantime but there was a big thing in 1986, which is: MIT released
a thing called a book, a series of two volumes of book called, “Parallel Distributed Processing”.
In which they described this thing they call parallel distributed processing, where you
have a bunch of processing units, that have some state of activation and some output function
and some pattern of connectivity and some propagation rule and some activation rule
and some learning rule, operating in an environment.
And then they described how things that met these requirements, could in theory, do all
kinds of amazing work.
And this was the result of many, many researchers working together.
There was a whole group involved in this project, which resulted in this very, very important
book.
And so, the interesting thing to me, is that if you - as you go through this course come
back and have a look at this picture and you'll see we are doing exactly these things.
Everything we're learning about really is how do you do each of these eight things?
That is interesting that they include the environment because that's something which
very often, data scientists ignore.
Which is - you build a model, you've trained it, it's learned something.
What's the context it works in?
And we're talking about that, quite a bit over the next couple of lessons as well.
So in the 80's, during and after this was released people started building in this second
layer of neurons, avoiding Minsky's problem.
And in fact, it was shown, that it was mathematically provable, that by adding that one extra layer
of neurons, it was enough to allow any mathematical model to be approximated to any level of accuracy,
with these neural networks.
And so that was like the exact opposite of the Minsky thing.
That was like: “Hey there's nothing we can't do.
Provably there's nothing we can't do.”
And so that was kind of when I started getting involved in neural networks.
So I was - a little bit later.
I guess I was getting involved in the early 90s.
And they were very widely used in industry.
I was using them for very boring things like targeted marketing for retail banks.
They tended to be big companies with lots of money that were using them.
And it certainly though was true that often the networks were too big or slow to be useful.
They were certainly useful for some things, but they - you know they never felt to me
like they were living up to the promise for some reason.
Now what I didn't know, and nobody I personally met knew was that actually there were researchers
that had shown 30 years ago that to get practical good performance, you need more layers of
neurons.
Even though mathematically, theoretically, you can get as accurate as you want with just
one extra layer.
To do it with good performance, you need more layers.
So when you add more layers to a neural network you get deep learning.
So deep doesn't mean anything like mystical.
It just means more layers.
More layers than just adding the one extra one.
So thanks to that, neural nets are now living up to their potential.
As we saw in that like what's deep learning good at thing.
So we could now say that Rosenblatt was right.
We have a machine that's capable of perceiving, recognising and identifying its surroundings
without any human training or control.
That is - That's definitely true.
I don't think there's anything controversial about that statement based on the current
technology.
So we're gonna be learning how to do that.
We're gonna be learning how to do that in exactly the opposite way, of probably all
of the other math and technical education you've had.
We are not gonna start with a two-hour lesson about the sigmoid function or a study of linear
algebra or a refresher course on calculus.
And the reason for that, is that people who study how to teach and learn have found that
is not the right way to do it for most people.
For most people - So we work a lot based on the work of Professor David Perkins from Harvard
and others who work at similar things, who talk about this idea of playing the whole
game.
And so playing the whole game is like it's based on the sports analogy if you're gonna
teach somebody baseball.
You don't take them out into a classroom and start teaching them about the physics of a
parabola, and how to stitch a ball, and a three-part history of 100 years of baseball
politics, and then 10 years later, you let them watch a game.
And then 20 years later, you let them play a game.
Which is kind of like how math education is being done, right?
Instead with baseball, step one is to say, hey, let's go and watch some baseball.
What do you think?
That was fun, right?
See that guy there...he took a run there…before the other guy throws a ball over there...hey,
you want to try having a hit?
Okay, so you're going to hit the ball, and then I have to try to catch it, then he has
to run,run,run over there...and so from step one, you are playing the whole game.
And just to add to that, when people start, they often may not have a full team, or be
playing the full nine innings, but they still have a sense of what the game is, a kind of
a big picture idea.
So, there is lots and lots of reasons that this helps most human beings, (though) not
everybody, right?
There's a small percentage of people who like to build things up from the foundations and
the principles, and not surprisingly, they are massively overrepresented in a university
setting, because the people who get to be academics are the people who thrive with,
(according) to me, the upside down way of how things are taught.
But outside of universities most people learn best in this top-down way, where you start
with the full context.
So step number two in the seven principles, and I'm only going to mention the first three,
is to make the game worth playing.
Which is like, if you're playing baseball, you have a competition.
You know, you score, you try and win, you bring together teams from around the community
and you have people try to beat each other.
And you have leaderboards, like who's got the highest number of runs or whatever.
So this is all about making sure that the thing you're doing, you're doing it properly.
You're making it the whole thing, you're providing the context and the interest.
So, for the fastai approach to learning deep learning, what this means is that today we're
going to train models end to end.
We're going to actually train models, and they won't just be crappy models.
They will be state-of-the-art world-class models from today, and we can try to have
you build your own state-of-the-art world-class models from either today or next lesson, depending
on how things go.
Then, number three in the seven principles from Harvard is, work on the hard parts.
Which is kind of like this idea of practice, deliberate practice.
Work on the hard parts means that you don't just swing a bat at a ball every time, you
know, you go out and just muck around.
You train properly, you find the bit that you are the least good at, you figure out
where the problems are, you work damn hard at it.
So, in the deep learning context, that means that we do not dumb things down.
Right?
By the end of the course, you will have done the calculus.
You will have done the linear algebra.
You will have done the software engineering of the code, right?
You will be practicing these things which are hard, so it requires tenacity and commitment.
But hopefully, you'll understand why it matters because before you start practicing something
you'll know why you need that thing because you'll be using it.
Like to make your model better, you'll have to understand that concept first.
So for those of you used to a traditional university environment, this is gonna feel
pretty weird and a lot of people say: “that they regret (you know after a year of studying
fastai) that they spent too much time studying theory, and not enough time training models
and writing code.
That's the kind of like, the number one piece of feedback we get from people who say, “I
wish I've done things differently.”
It's that.
So please try to, as best as you can, since you're here, follow along with this approach.
We are gonna be using a software stack - Sorry Rachel.
Yes?
I just need to say one more thing about the approach.
I think since, so many of us spent so many years with the traditional educational approach
of bottom-up, that this can feel very uncomfortable at first.
I still feel uncomfortable with it sometimes, even though I'm committed to the idea.
And that, some of it is also having to catch yourself and being okay with not knowing the
details.
Which I think can feel very unfamiliar, or even wrong when you're kind of new to that.
Of like: “Oh wait, I'm using something and I don't understand every underlying detail.”
But you kind of have to trust that we're gonna get to those details later.
So I can't empathise because I did not spend lots of time doing that.
But I will tell you this - teaching this way is very, very, very hard.
And I very often find myself jumping back into a foundations first approach.
Because it's just so easy to be like: “Oh you need to know this.
You need to know this.
You need to do this.
And then you can know this.”
That's so much easier to teach.
So I do find this much much more challenging to teach, but hopefully it's worth it.
We spent a long long time figuring out how to get deep learning into this format.
But one of the things that helps us here, is the software we have available.
If you haven't used Python before - it's ridiculously flexible and expressive and easy-to-use language.
We have plenty of bits about it we don't love but on the whole we love the overall thing.
And we think it's - Most importantly, the vast, vast, vast majority of deep learning
practitioners and researchers are using Python.
On top of Python, there are two libraries that most folks are using today; PyTorch and
TensorFlow.
There's been a very rapid change here.
TensorFlow was what we were teaching until a couple of years ago.
It's what everyone was using until a couple of years ago.
It got super bogged down, basically TensorFlow got super bogged down.
This other software called PyTorch came along that was much easier to use and much more
useful for researchers and within the last 12 months, the percentage of papers at major
conferences that uses PyTorch has gone from 20% to 80% and vice versa, those that use
TensorFlow have gone on from 80% to 20%.
So basically all the folks that are actually building the technology were all using, are
now using, PyTorch and you know industry moves a bit more slowly but in the next year or
two you will probably see a similar thing in industry.
Now, the thing about PyTorch is it's super super flexible and really is designed for
flexibility and developer friendliness, certainly not designed for beginner friendliness and
it's not designed for what we say, it doesn't have higher level API's, by which I mean there
isn't really things to make it easy to build stuff quickly using PyTorch.
So to deal with that issue, we have a library called fastai that sits on top of PyTorch.
Fastai is the most popular higher level API for PyTorch.
It is, because our courses are so popular, some people are under the mistaken impression
that fastai is designed for beginners or for teaching.
It is designed for beginners and teaching, as well as practitioners in industry and researchers.
The way we do this makes sure that it's, that it's the best API for all of those people
as we use something called a layered API and so there's a peer-reviewed paper that Sylvain
and I wrote that described how we did that and for those of you that are software engineers,
it will not be at all unusual or surprising.
It's just totally standard software engineering practices, but they are practices that were
not followed in any deep learning library we had seen.
Just basically lots of re-factoring and decoupling and so by using that approach, it's allowed
us to build something which you can do super low-level research, you can do state-of-the-art
production models and you can do kind of super easy, beginner, but beginner world-class models.
So that's the basic software stack, there's other pieces of software we will be learning
about along the way.
But the main thing I think to mention here is it actually doesn't matter.
If you learn this software stack and then at work you need to use TensorFlow and Keras,
you will be able to switch in less than a week.
Lots and lots of students have done that, it's never been a problem.
The important thing is to learn the concepts and so we're going to focus on those concepts
and by using an API which minimizes the amount of boilerplate you have to use, it means you
can focus on the bits that are important.
The actual lines of code will correspond much more to the actual concepts you are implementing.
You are going to need a GPU machine.
A GPU is a Graphics Processing Unit, and specifically, you need an Nvidia GPU.
Other brands of GPU just aren't well supported by any Deep Learning libraries.
Please don't buy one.
If you already have one you probably shouldn't use it.
Instead you should use one of the platforms that we have already got set up for you.
It's just a huge distraction to be spending your time doing, like, system administration
on a GPU machine and installing drivers and blah blah blah.
And run it on Linux.
Please.
That's what everybody's doing, not just us, everybody's running it on Linux.
Make life easy for yourself.
It's hard enough to learn Deep Learning without having to do it in a way that you are learning,
you know, all kinds of arcane hardware support issues.
There's a lot of free options available and so, please, please use them.
If you're using an option that is not free don't forget to shut down your instance.
So what's gonna be happening is you gonna be spinning up a server that lives somewhere
else in the world, and you're gonna be connecting to it from your computer and training and
running and building models.
Just because you close your browser window doesn't mean your server stops running on
the whole.
Right?
So don't forget to shut it down because otherwise you're paying for it.
Colab, is a great system which is free.
There's also a paid subscription version of it.
Be careful with Colab.
Most of the other systems we recommend save your work for you automatically and you can
come back to it at any time.
Colab doesn't.
So be sure to check out the Colab platform thread on the forums, to learn about that.
So, I mention the forums...
The forums are really, really important because that is where all of the discussion and set
up and everything happens.
So for example if you want help with setup here.
You know there is a setup help thread and you can find out, you know, how to best set
up Colab, and you can see discussions about it and you can ask questions, and please remember
to search before you ask your question, right?
Because it's probably been asked before, unless you're one of the very, very earliest people
who are doing the course.
So, once you…
So, step one is to get your server set up by just following the instructions from the
forums or from the course website.
And the course website will have lots of step-by-step instructions for each platform.
They will vary in price, they will vary in speed, they will vary in availability, and
so forth.
Once you are finished following those instructions
The last step of those instructions will end up showing you something like this: a course
v4 folder, so a version four of our course.
By the time you see this video, this is likely to have more stuff in it, but it will have
an NB's standing for notebooks folder.
So you can click on that, and that will show you all of the notebooks for the course.
What I want you to do is scroll bottom and find the one called app Jupyter.
Click on that, and this is where you can start learning about Jupyter notebook.
What's Jupyter Notebook?
Jupyter Notebook is something where you can start typing things, and press Shift-Enter,
and it will give you an answer.
And so the thing you're typing is python code, and the thing that comes out is a result of
that code.
And so you can put in anything in python.
X equals three times four.
X plus one, and as you can see, it displays a result anytime there's a result to display.
So for those of you that have done a bit of coding before, you will recognise this as
a REPL.
R-E-P-L, read, evaluate, print, loop.
Most languages have some kind of REPL.
The Jupyter notebook REPL is particularly interesting, because it has things like headings,
graphical outputs, interactive multimedia.
It's a really astonishing piece of software.
It's won some really big awards.
I would have thought the most widely used REPL, outside of shells like bash.
It's a very powerful system.
We love it.
We've written our whole book in it, we've written the entire FASTAI library with it,
we do all our teaching with it.
It's extremely unfamiliar to people who have done most of their work in IDE.
You should expect it to feel as awkward as perhaps the first time you moved from a GUI
to a command line.
It's different.
So if you're not familiar with REPL-based systems, it's gonna feel super weird.
But stick with it, because it really is great.
The kind of model going on here is that, this webpage I'm looking at, is letting me type
in things for a server to do, and show me the results of computations a server is doing.
So the server is off somewhere else.
It's not running on my computer right?
The only thing running on the computer is this webpage.
But as I do things, so for example if I say X equals X times three.
This is updating the servers state.
There's this state.
It's like what's currently the value of X and so I can find out, now X is something
different.
So you can see, when I did this line here, it didn't change the earlier X plus one, right?
So that means that when you look at a Jupyter notebook, it's not showing you the current
state of your server.
It's just showing you what that state was, at the time that you printed that thing out.
It's just like if you use a shell like bash.
And you type “ls”.
And then you delete a file.
That earlier “ls” you printed doesn't go back and change.
That's kind of how REPLs generally work.
Including this one.
Jupyter notebook has two modes.
One is edit mode, which is when I click on a cell and I get a flashing cursor and I can
move left and right and type.
Right?
There's not very many keyboard shortcuts to this mode.
One useful one is “control” or “command” + “/”. Which will comment and uncomment.
The main one to know is “shift” + “enter” to actually run the cell.
At that point there is no flashing cursor anymore.
And that means that I'm now in command mode.
Not edit mode.
So as I go up and down, I'm selecting different cells.
So in command mode as we move around, we're now selecting cells.
And there are now lots of keyboard shortcuts you can use.
So if you hit “H” you can get a list of them.
That, for example - And you'll see that they're not on the whole, like “control” or “command”
with something they're just the letter on its own.
So if you use like Vim, you'll be more familiar with this idea.
So for example if I hit “C” to copy and “V” to paste.
Then it copies the cell.
Or “X” to cut it.
“A” to add a new cell above.
And then I can press the various number keys, to create a heading.
So number two will create a heading level II.
And as you can see, I can actually type formatted text not just code.
The formatted text I type is in Markdown.
Like so.
My numbered one work.
There you go.
So that's in Markdown.
If you haven't used Markdown before, it's a super super useful way to write formatted
text.
That is used very, very, very widely.
So learn it because it's super handy.
And you need it for Jupiter.
So when you look at our book notebooks.
For example, you can see an example of all the kinds of formatting and code and stuff
here.
So you should go ahead and go through the “app_jupyter”.
And you can see here how you can create plots for example.
And create lists of things.
And import libraries.
And display pictures and so forth.
If you wanna create a new notebook, you can just go “New” “Python 3” and that
creates a new notebook.
Which by default is just called “Untitled” so you can then rename it to give it whatever
name you like.
And so then you'll now see that, in the list here, “newname”.
The other thing to know about Jupiter is that it's a nice easy way to jump into a terminal
if you know how to use a terminal.
You certainly don't have to for this course at least for the first bit.
If I go to a new terminal, You can see here I have a terminal.
One thing to note is for the notebooks are attached to a Github repository.
If you haven't used Github before that's fine.
but basically they're attached to a server where from time to time we will update the
notebooks on it.
And we will see you'll see on the course website, in the forum we tell you how to make sure
you have the most recent versions.
When you grab our most recent version you don't want to conflict with or overwrite your
changes.
So as you start experimenting it's not a bad idea to like select a notebook and click duplicate
and then start doing your work in the copy.
And that way when you get an update of our latest course materials, it's not gonna interfere
with the experiments you've been running.
So there are two important repositories to know about.
One is the fast book repository which we saw earlier, which is kind of the full book with
all the outputs and pros and everything.
And then the other one is the course V4 repository.
And here is the exact same notebook from the course V4 repository.
And for this one we remove all of the pros and all of the pictures and all of the outputs
and just leave behind the headings and the code.
In this case you can see some outputs because I just ran that code, most of it.
There won't be any.
No, No, I guess we have left outputs.
I'm not sure to keep that or not.
So you may or may not see the outputs.
So the idea with this is.
This is properly the version that you want to be experimenting with.
Because it kind of forces you to think about like what's going on as you do each step,
rather than just reading it and running it without thinking.
We kind of want you to do it in a small bare environment in which you thinking about like
what did the book say why was this happening and if you forget anything then you kind of
go back to the book.
The other thing to mention is both the course V4 version and the fast book version at the
end have a questionnaire.
And a quite a few folks have told us you know that in amongst the reviewers and stuff that
they actually read the questionnaire first.
We spent many, many weeks writing the questionnaires, Sylvain and I.
And the reason for that is because we try to think about like what do we want you to
take away from each notebook.
So you kind of read the questionnaire first.
You can find out what are the things we think are important.
What are the things you should know before you move on.
So rather than having like a summary section at the end saying at the end of this you should
know, blah blah blah, we instead have a questionnaire to do the same thing, so please make sure
you do the questionnaire before you move onto the next chapter.
You don't have to get everything right, and most of the time answering the questions is
as simple as going back to that part of the notebook and reading the prose, but if you've
missed something, like do go back and read it because these are the things we are assuming
you know.
So if you don't know these things before you move on, it could get frustrating.
Having said that, if you get stuck after trying a couple of times, do move onto the next chapter,
do two or three more chapters and then come back.
maybe by the time you've done a couple more chapters, you know, you will get some more
perspective.
We try to re-explain things multiple times in different ways, so it's okay if you tried
and you get stuck, then you can try moving on.
Alright, so, let's try running the first part of the notebook.
So here we are in 01 intro, so this is chapter 1 and here is our first cell.
So I click on the cell and by default, actually, there will be a header in the toolbar as you
can see.
You can turn them on or off.
I always leave them off myself and so to run this cell, you can either click on the play,
the run button or as I mentioned, you can hit shift enter.
So for this one this i'll just click and as you can see this star appears, so this says
I'm running and now you can see this progress bar popping up and that is going to take a
few seconds and so as it runs to print out some results.
Don't expect to get exactly the same results as us, there is some randomness involved in
training a model, and that's okay.
Don't expect to get exactly the same time as us.
If this first cell takes more than five minutes unless you have a really old GPU that is probably
a bad sign.
You might want to hop on the forums and figure out what's going wrong or maybe it only has
windows which really doesn't work very well for this moment.
Don't worry that we don't know what all the code does yet.
We are just making sure that we can train a model . So here we are, it's finished running
and so as you can see, it's printed out some information and in this case it's showing
me that there is an error rate of 0.005 at doing something.
What is the something it's doing?
Well, what it's doing here is it's actually grabbing a dataset, we call the pets dataset,
which is a dataset of pictures of cats and dogs.
And it's trying to figure out; which ones are cats and which ones are dogs.
And as you can see, after about well less than a minute, it's able to do that with a
0.5% error rate.
So it can do it pretty much perfectly.
So we've trained our first model.
We have no idea how.
We don't know what we were doing.
But we have indeed trained our model.
So that's a good start.
And as you can see, we can train models pretty quickly on a single computer.
Which you know - Many of which you can get for free.
One more thing to mention is, if you have a Mac - doesn't matter whether you have Windows
or Mac or Linux in terms of what's running in the browser.
But if you have a Mac, please don't try to use that GPU.
Mac's actually - Apple doesn't even support Nvidia GPUs anymore.
So that's really not gonna be a great option.
So stick with Linux.
It will make life much easier for you.
Right, actually the first thing we should do is actually try it out.
So if - I claim we've trained a model that can pick cats from dogs.
Let's make sure we can.
So let's - Check out this cell.
This is interesting.
Right?
We've created a widgets dot file upload object and displayed it.
And this is actually showing us a clickable button.
So as I mentioned this is an unusual REPL.
We can even create GUIs, in this REPL.
So if I click on this file upload.
And I can pick “cat”.
There we go.
And I can now turn that uploaded data into an image.
There's a cat.
And now I can do predict, and it's a cat.
With a 99.96% probability.
So we can see we have just uploaded an image that we've picked out.
So you should try this.
Right?
Grab a picture of a cat.
Find one from the Internet or go and take a picture of one yourself.
And make sure that you get a picture of a cat.
This is something which can recognise photos of cats, not line drawings of cats.
And so as we'll see, in this course.
These kinds of models can only learn from the kinds of information you give it.
And so far we've only given it, as you'll discover, photos of cats.
Not anime cats, not drawn cats, not abstract representations of cats but just photos.
So we're now gonna look at; what's actually happened here?
And you'll see at the moment, I am not getting some great information here.
If you see this, in your notebooks, you'll have to go: file, trust notebook.
And that just tells Jupiter that it's allowed to run the code necessary to display things,
to make sure there isn't any security problems.
And so you'll now see the outputs.
Sometimes you'll actually see some weird code like this.
This is code that actually creates outputs.
So sometimes we hide that code.
Sometimes we show it.
So generally speaking, you can just ignore the stuff like that and focus on what comes
out.
So I'm not gonna go through these.
Instead I'm gonna have a look at it - same thing over here on the slides.
So what we're doing here is; we're doing machine learning.
Deep learning is a kind of machine learning.
What is machine learning?
Machine learning is, just like regular programming, it's a way to get computers to do something.
But in this case, it's pretty hard to understand how you would use regular programming to recognise
dog photos from cat photos.
How do you kind of create the loops and the variable assignments and the conditionals
to create a program that recognises dogs vs cats in photos.
It's super hard.
Super super hard.
So hard, that until kind of the deep learning era, nobody really had a model that was remotely
accurate at this apparently easy task.
Because we can't write down the steps necessary.
So normally, you know, we write down a function that takes some inputs and goes through our
program.
Produces some results.
So this general idea where the program is something that we write (the steps).
Doesn't seem to work great for things like recognising pictures.
So back in 1949, somebody named Arthur Samuel started trying to figure out a way to solve
problems like recognising pictures of cats and dogs.
And in 1962, he described a way of doing this.
Well first of all he described the problem: “Programming a computer for these kinds
of computations is at best a difficult task.
Because of the need to spell out every minute step of the process in exasperating detail.
Computers are giant morons which all of us coders totally recognise.”
So he said, okay, let's not tell the computer the exact steps, but let's give it examples
of a problem to solve and figure out how to solve it itself.
And so, by 1961 he had built a checkers program that had beaten the Connecticut state champion,
not by telling it the steps to take to play checkers, but instead by doing this, which
is: “arrange for an automatic means of testing the effectiveness of a weight assignment in
terms of actual performance and a mechanism for altering the weight assignment so as to
maximise performance.”
This sentence is the key thing.
And it's a pretty tricky sentence so you can spend some time on it.
The basic idea is this; instead of saying inputs to a program and then outputs.
Let's have inputs to a - let's call the program now model.
It is the same basic idea.
Inputs to a model and results.
And then we're gonna have a second thing called weights.
And so the basic idea is that this model is something that creates outputs based not only
on, for example, the state of a checkers board, but also based on some set of weights or parameters
that describe how that model is going to work.
So the idea is, if we could, like, enumerate all the possible ways of playing checkers,
and then kind of describe each of those ways using some set of parameters or what Samuel
called weights.
Then if we had a way of checking how effective a current weight assignment is in terms of
actual performance, in other words, does that particular enumeration of a strategy for playing
checkers end up winning or losing games, and then a way to alter the weight assignment
so as to maximise the performance.
So then oh let's try increasing or decreasing each one of those weights one at a time to
find out if there is a slightly better way of playing checkers and then do that lots
of lots of times then eventually such a procedure could be made entirely automatic and then
the machine so programmed would learn from its experience so this little paragraph is,
is the thing.
This is machine learning a way of creating programs such that they learn, rather than
programmed.
So if we had such a thing, then we would basically now have something that looks like this: you
have inputs and weights again going into a model, creating results, i.e. you won or you
lost, and then a measurement of performance.
So remember that was this key step and then the second key step is a way to update the
weights based on the measured performance and then you could look through this process
and create a) train a machine learning model so this is the abstract idea.
So after it ran for a while, right, it's come up with a set of weights which it's pretty
good, right, we can now forget the way it was trained and we have something that is
just like this, right, except the word program is now replaced with the word model.
So a trained model can be used just like any other computer program.
So the idea is we are building a computer program not by putting up the steps necessary
to do the task, but by training it to learn to do the task at the end of which it's just
another program and so this is what's called inference right is using a trained model as
a program to do a task such as playing checkers so machine learning is training programs developed
by allowing a computer to learn from its experience rather than through manually coding.
Ok how would you do this for image recognition, what is that model and that set of weights
such that as we vary them it could get better and better at recognising cats versus dogs,
I mean for checkers
It's not too hard to imagine how you could kind of enumerate, depending on different
kinds of “how far away the opponent's piece is from your piece,” what should you do
in that situation.
How should you weigh defensive versus aggressive strategies, blah blah blah.
Not at all obvious how you do that for image recognition.
So what we really want, is some function in here which is so flexible that there is a
set of weights that could cause it to do anything.
A real--like the world's most flexible possible function--and turns out that there is such
a thing.
It's a neural network.
So we'll be describing exactly what that mathematical function is in the coming lessons.
To use it, it actually doesn't really matter what the mathematical function is.
It's a function which is, we say, “parameterised” by some set of weights by which I mean, as
I give it a different set of weights it does a different task, and it can actually do any
possible task: something called the universal approximation theorem tells us that mathematically
provably, this functional form can solve any problem that is solvable to any level of accuracy.
If you just find the right set of weights.
Which is kind of restating what we described earlier in that, like, how do we deal with
Minsky (the Marvin Minsky) problem so neural networks are so flexible that if you could
find the right set of weights they can solve any problem including “Is this a car or
is this dog.”
So that means you need to focus your effort on the process of training that is finding
good weights, good weight assignments to use Samuel's terminology.
So how do you do that?
We want a completely general way to do this--to update the weights based on some measure of
performance, such as how good is it at recognising cats versus dogs.
And luckily it turns out such a thing exists!
And that thing is called stochastic gradient descent (or SGD).
Again, we'll look at exactly how it works, we'll build it ourselves from scratch, but
for now we don't have to worry about it.
I will tell you this, though, neither SGD nor neural nets are at all mathematically
complex.
They nearly entirely are addition and multiplication.
The trick is it just a lot of them--like billions of them--so many more than we can intuitively
grasp.
They can do extraordinarily powerful things, but they're not rocket science at all.
They are not complex things, and we will see exactly how they work.
So that's the Arthur Samuel version, right?
Nowadays we don't use quite the same terminology, but we use exactly the same idea.
So that function that sits in the middle,
we call an architecture.
An architecture is the function that we're adjusting the weights to get it to do something.
That's the architecture, that's the functional form of the model.
Sometimes people say model to mean architecture, so don't let that confuse you too much.
But, really the right word is architecture.
We don't call them weights; we call them parameters.
Weights has a specific meaning- it's quite a particular kind of parameter.
The things that come out of the model, the architecture with the parameters, we call
them predictions.
The predictions are based on two kinds of inputs: independent variables that's the data,
like the pictures of the cats and dogs, and dependent variables also known as labels,
which is like the thing saying “this is a cat”, “this is a dog”, “this is
a cat”.
So, that's your inputs.
So, the results are predictions.
The measure of performance, to use Arthur Samuel's word, is known as the loss.
So, the loss is being calculated from the labels on the predictions and then there's
the update back to the parameters.
Okay, so, this is the same picture as we saw, but just putting in the words that we use
today.
So, this picture- if you forget, if I say these are the parameters of this used for
this architecture to create a model- you can go back and remind yourself what they mean.
What are the parameters?
What are the predictions?
What is the loss?
Okay, the loss of some function that measures the performance of the model in such a way
that we can update the parameters.
So, it's important to note that deep learning and machine learning are not magic, right?
The model can only be created where you have data showing you examples of the thing that
you're trying to learn about.
It can only learn to operate on the patterns that you've seen in the input used to train
it, right?
So, if we don't have any line drawings of cats and dogs, then there's never going to
be an update to the parameters that makes the architecture and so the architect and
the parameters together is the model.
So, to say the model, that makes the model better at predicting line drawings of cats
and dogs because they just, they never received those weight updates because they never received
those inputs.
Notice also that this learning approach only ever creates predictions.
It doesn't tell you what to do about it.
That's going to be very important when we think about things like a recommendation system
of like “what product do we recommend to somebody”?
Well, I don't know- we don't do that, right?
We can predict what somebody will say about a product we've shown them, but we're not
creating actions.
We're creating predictions.
That's a super important difference to recognize.
It's not enough just to have examples of input data like pictures of dogs and cats.
We can't do anything without labels.
And so very often, organisations say: “we don't have enough data”.
Most of the time they mean: “we don't have enough labelled data”.
Because if a company is trying to do something with deep learning, often it's because they're
trying to automate or improve something they're already doing.
Which means by definition they have data about that thing or a way to capture data about
that thing.
Cus they're doing it.
Right?
But often the tricky part is labelling it.
So for example in medicine.
If you're trying to build a model for radiology.
You can almost certainly get lots of medical images about just about anything you can think
of.
But it might be very hard to label them according to malignancy of a tumour or according to
whether or not meningioma is present or whatever, because these kinds of labels are not necessarily
captured in a structured way, at least in the US medical system.
So that's an important distinction that really impacts your kind of strategy.
So then a model, as we saw from the PDP book, a model operates in an environment.
Right?
You roll it out and you do something with.
And so then, this piece of that kind of PDP framework is super important.
Right?
You have a model that's actually doing something.
For example, you've built a predictive policing model that predicts (doesn't recommend actions)
it predicts where an arrest might be made.
This is something a lot of jurisdictions in the US are using.
Now it's predicting that, based on data and based on labelled data.
And in this case it's actually gonna be using (in the US) for example data where, I think,
depending on whether you're black or white, black people in the US, I think, get arrested
something like seven times more often for say marijuana possession than whites.
Even though the actual underlying amount of marijuana use is about the same in the two
populations.
So if you start with biased data and you build a predictive policing model.
Its prediction will say: “oh you will find somebody you can arrest here” based on some
biased data.
So then, law enforcement officers might decide to focus their police activity on the areas
where those predictions are happening.
As a result of which they'll find more people to arrest.
And then they'll use that, to put it back into the model.
Which will now find: “oh there's even more people we should be arresting in the black
neighbourhoods” and thus it continues.
So this would be an example of how a model interacting with its environment creates something
called a positive feedback loop.
Where the more a model is used, the more biased the data becomes, making the model even more
biased and so forth.
So one of the things to be super careful about with machine learning is; recognising how
that model is actually being used and what kinds of things might happen as a result of
that.
I was just going to add that this is an example of proxies because here arrest is being used
as a proxy for crime, and I think that pretty much in all cases, the data that you actually
have is a proxy for some value that you truly care about.
And that difference between the proxy and the actual value often ends up being significant.
Thanks, Rachel.
That's a really important point.
Okay, so let's finish off by looking at what's going on with this code.
So the code we ran is, basically -- one, two, three, four, five, six -- lines of code.
So the first line of code is an import line.
So in Python you can't use an external library until you import from it.
Normally in place, people import just the functions and classes that they need from
the library.
But Python does provide a convenient facility where you can import everything from a module,
which is by putting a start there.
Most of the time, this is a bad idea.
Because, by default, the way Python works is that if you say import star, it doesn't
only import the things that are interesting and important in the library you're trying
to get something from.
But it also imports things from all the libraries it used, and all the libraries they used,
and you end up kind of exploding your namespace in horrible ways and causing all kinds of
bugs.
Because fastai is designed to be used in this REPL environment where you want to be able
to do a lot of quick rapid prototyping, we actually spent a lot of time figuring out
how to avoid that problem so that you can import star safely.
So, whether you do this or not, is entirely up to you.
But rest assured that if you import star from a fastai library, it's actually been explicitly
designed in a way that you only get the bits that you actually need.
One thing to mention is in the video you see it's called “fastai2.”
That's because we're recording this video using a prerelease version.
By the time you are watching the online, the MOOC, version of this, the 2 will be gone.
Something else to mention is, there are, as I speak, four main predefined applications
in fastai, being vision, text, tabular and collaborative filtering.
We'll be learning about all of them and a lot more.
For each one, say here's vision, you can import from the .all, kind of meta-model, I guess
we could call it.
And that will give you all the stuff that you need for most common vision applications.
So, if you're using a REPL system like Jupyter notebook, it's going to give you all the stuff
right there that you need without having to go back and figure it out.
One of the issues with this is a lot of the python users don't.
If they look at something like untar_data, they would figure out where it comes from
by looking at the import line.
And so if you import star, you can't do that anymore.
The good news, in a REPL, you don't have to.
You can literally just type the symbol, press SHIFT - ENTER and it will tell you exactly
where it came from.
As you can see.
So that's super handy.
So in this case, for example, to do the actual building of the dataset, we called ImageDataLoaders.from_name_func.
I can actually call the special doc function to get the documentation for that.
As you can see, it tells me exactly everything to pass in, what all the defaults are, and
most importantly, not only what it does, but SHOW IN DOCS pops me over to the full documentation
including an example.
Everything in the fastAI documentation has an example and the cool thing is: the entire
documentation is written in Jupyter Notebooks.
So that means you can actually open the Jupyter Notebook for this documentation and run the
line of code yourself and see it actually working and look at the outputs and so forth.
Also in the documentation, you'll find that there are a bunch of tutorials.
For example, if you look at the vision tutorial, it will cover lots of things but one of the
things we will cover is, as you can see in this case, pretty much the same kind of stuff
we are actually looking at in Lesson 1.
So there is a lot of documentation in fastAI and taking advantage of it is a pretty good
idea.
It is fully searchable and as I mentioned, perhaps most importantly, every one of these
documentation pages is also a fully interactive Jupyter Notebook.
So, looking through more of this code, the first line after the import is something that
uses untar_ data.
That will download a dataset, decompress it, and put it on your computer.
If it is already downloaded, it won't download it again.
If it is already decompressed it won't decompress it again.
And as you can see, fastAI already has predefined access to a number of really useful datasets.
such as this PETS dataset.
Datasets are a super important part, as you can imagine of deep learning.
We will be seeing lots of them.
And these are created by lots of heroes (and heroines) who basically spend months or years
collating data that we can use to build these models.
The next step is to tell fastAI what this data is and we will be learning a lot about
that.
But in this case, we are basically saying, ‘okay, it contains images'.
It contains images that are in this path.
So untar_data returns the path that is whereabouts it has been decompressed to.
Or if it is already decompressed, it tells us where it was previously decompressed to.
We have to tell it things like ‘okay, what images are actually in that path'.
One of the really interesting ones is label_func.
How do you tell, for each file, whether it is a cat or a dog.
And if you actually look at the ReadME for the original dataset, it uses a slightly quirky
thing which is they said, ‘oh, anything where the first letter of the filename is
an uppercase is a cat'.
That's what they decided.
So we just created a little function here called is_cat that returns the first letter,
is it uppercase or not.
And we tell fastai that's how you tell if it's a cat.
We'll come back to these two in a moment.
So the next thing, now we've told it what the data is.
We then have to create something called a learner.
A learner is a thing that learns, it does the training.
So you have to tell it what data to use.
Then you have to tell it what architecture to use.
I'll be talking a lot about this in the course.
But, basically, there's a lot of predefined neural network architectures that have certain
pros and cons.
And for computer vision, the architecture is called ResNet.
Just a super great starting point, and so we're just going to use a reasonably small
one of them.
So these are all predefined and set up for you.
And then you can tell fastai what things you want to print out as it's training.
And in this case, we're saying “oh, tell us the error, please, as you train”.
So then we can call this really important method called fine_tune that we'll be learning
about in the next lesson which actually does the training.
valid_pct does something very important.
It grabs, in this case, 20% of the data (.2 proportion), and does not use it for training
a model.
Instead, it uses it for telling you the error rate of the model.
So, always in fastai this metric, error_rate, will always be calculated on a part of the
data which has not been trained with.
And the idea here, and we'll talk a lot about more about this in future lessons.
But the basic idea here is we want to make sure that we're not overfitting.
Let me explain.
Overfitting looks like this.
Let's say you're trying to create a function that fits all these dots, right.
A nice function would look like that, right.
But you could also fit, you can actually fit it much more precisely with this function.
Look, this is going much closer to all the dots than this one is.
So, this is obviously a better function.
Except, as soon as you get outside where the dots are, especially if you go off the edges,
it's obviously doesn't make any sense.
So, this is what you'd call an overfit function.
So, overfitting happens for all kinds of reasons.
We use a model that's too big or we use not enough data.
We'll be talking all about it, right.
But, really the craft of deep learning is all about creating a model that has a proper
fit.
And the only way you know if a model has a proper fit is by seeing whether it works well
on data that was not used to train it.
And so, we always set aside some of the data to create something called a validation set.
The validation set is the data that we use not to touch it at all when we're training
a model, but we're only using it to figure out whether the model's actually working or
not.
One thing that Sylvain mentioned in the book, is that one of the interesting things about
studying fastai is you learn a lot of interesting programming practices.
And so I've been programming, I mean, since I was a kid, so like 40 years.
And Sylvain and I both work really, really hard to make python do a lot of work for us
and to use, you know, programming practices which make us very productive and allow us
to come back to our code, years later and still understand it.
And so you'll see in our code we'll often do things that you might not have seen before.
And so we, a lot of students who have gone through previous courses say they learned
a lot about coding and python coding and software engineering from the course.
So, yeah check, when you see something new, check it out and feel free to ask on the forums
if you're curious about why something was done that way.
One thing to mention is, just like I mentioned import star is something most Python programmers
don't do cause most libraries don't support doing it properly.
We do a lot of things like that.
We do a lot of things where we don't follow a traditional approach to python programming.
Because I've used so many languages over the years, I code not in a way that's specifically
pythonic, but incorporates like ideas from lots of other languages and lots of other
notations and heavily customised our approach to python programming based on what works
well for data science.
That means that the code you see in fastai is not probably, not gonna fit with, the kind
of style guides and normal approaches at your workplace, if you use Python there.
So, obviously, you should make sure that you fit in with your organization's programming
practices rather than following ours.
But perhaps in your own hobby work, you can follow ours and see if you find that are interesting
and helpful, or even experiment with that in your company if you're a manager and you
are interested in doing so.
Okay, so to finish, I'm going to show you something pretty interesting, which is, have
a look at this code untar data, image data loaders from name func, learner, fine tune.
Untar data, segmentation date loaders, from label func, learner, fine tune.
Almost the same code, and this has built a model that does something, whoa totally different!
It's something which has taken images.
This is on the left, this is the labeled data.
It's got images with color codes to tell you whether it's a car, or a tree, or a building,
or a sky, or a line marking or a road.
And on the right is our model, and our model has successfully figured out for each pixel,
is that a car, line marking, a road.
Now it's only done it in, under 20 seconds right.
So it's a very small quick model.
It's made some mistakes -- like it's missing this line marking, and some of these cars
it thinks is house, right?
But you can see so if you train this for a few minutes, it's nearly perfect.
But you can see the basic idea is that we can very rapidly, with almost exactly the
same code, create something not that classifies cats and dogs but does what's called segmentation:
figures out what every pixel and image is.
Look, here's the same thing:
from import star text loaders from folder
learner learn fine-tune
Same basic code.
This is now something where we can give it a sentence and it can figure out whether that
is expressing a positive or negative sentiment, and this is actually giving a 93% accuracy
on that task in about 15 minutes on the IMDb dataset, which contains thousands of full-length
movie reviews (in fact, 1000- to 3000-word reviews).
This number here that we got with the same three lines of code would have been the best
in the world for this task in a very, very, very popular academics dataset in like 2015
I think.
So we are creating world-class models, in our browser, using the same basic code.
Here's the same basic steps again:
from import star untar data
tabular data loaders from csv
learner fit
This is now building a model that is predicting salary based on a csv table containing these
columns.
So this is tabular data.
Here's the same basic steps
from import * untar data
collab data loaders from csv learner fine-tune
This is now building something which predicts, for each combination of a user and a movie,
what rating do we think that user will give that movie, based on what other movies they've
watched and liked in the past.
This is called collaborative filtering and is used in recommendation systems.
So here you've seen some examples of each of the four applications in fastai.
And as you'll see throughout this course, the same basic code and also the same basic
mathematical and software engineering concepts allow us to do vastly different things using
the same basic approach.
And the reason why is because of Arthur Samuel.
Because of this basic description of what it is you can do if only you have a way to
parameterize a model and you have an update procedure which can update the weights to
make you better at your loss function, and in this case we can use neural networks, which
are totally flexible functions.
So that's it for this first lesson.
It's a little bit shorter than our other lessons going to be and the reason for that is that
we are as I mentioned at the start of a global pandemic here, or at least in the West (in
other countries they are much further into it).
So we spent some time talking about that at the start of the course and you can find that
video elsewhere.
So in the future lessons there will be more on deep learning.
So, what I suggest you do over the next week, before you work on the next lesson, is just
make sure that you can spin up a GPU server, you can shut down when it's finished and that
you can run all of the code here and, as you go through, see if this is using Python in
a way you recognise, use the documentation, use that doc function, do some search on the
fastai doc, see what it does, see if you can actually grab the fastai documentation notebooks
themselves and run them.
Just try to get comfortable, that you can if you can know your way around.
Because the most important thing to do with this style of learning, this top-down learning,
is to be able to run experiments and that means you need to be able to run code.
So my recommendation is: don't move on until you can run the code, read the chapter of
the book, and then go through the questionnaire.
We still got some more work to do about validation sets and test sets and transfer learning.
So you won't be able to do all of it yet but try to to all the parts you can, based on
what we've seen of the course so far.
Rachel, anything you want to add before we go.
Okay, so thanks very much for joining us for lesson one everybody and are really looking
forward to seeing you next time where we will learn about transfer learning and then we
will move on to creating an actual production version of an application that we can actually
put out on the Internet and you can start building apps that you can show your friends
and they can start playing with.
Bye Everybody

So, hello everybody, and welcome back to Practical Deep Learning for Coders.
This is lesson two, and in the last lesson we started training our first models.
We didn't really have any idea how that training was really working, but we were looking at
a higher level at what was going on.
And we learned about “What is machine learning?” and “How does that work?” and we realized
that based on how machine learning worked that there are some fundamental limitations
on what it can do, and we've talked about some of those limitations.
And we also talked about how after you've trained a machine learning model, you end
up with a program which behaves much like a normal program or something: with inputs
and a thing in the middle and outputs.
So today we're gonna finish up talking about that and we're going to then look at how we
get those models into production and what some of the issues with doing that might be.
I wanted to remind you that there are two sets of books--sorry two sets of notebooks--available
to you.
One is the fastbook repo (the full actual notebooks containing all the text of the O'Reilly
book) and so this lets you see everything that I'm telling you in much more detail,
and then as well as that there's the course v4 repo which contains exactly the same notebooks
but with all the prose stripped away to help you study.
So that's where you really want to be doing your experiment and your practice and so maybe
as you listen to the video you can kind of switch back and forth between the video and
reading or do one and then the other, and then put it away and have a look at the course
v4 notebooks and try to remember like “Okay, what was this section about?” and run the
code, and see what happens and change it and so forth.
So we were looking at this line of code where we looked at how we created our data by passing
in information--perhaps most importantly some way to label the data--and we talked about
the importance of labeling.
And in this case, this particular dataset whether it's a cat or a dog, you can tell
by whether it's an uppercase or a lowercase letter in the first position.
That's just how this dataset (that they tell you when the readme) works.
And we also looked particularly at this idea of “valid percent equals 0.2,” and like
“What does that mean?
It creates a validation set.” and that was something I wanted to talk more about.
The first thing I want to do though is point out that this particular labeling function
returns something that's either true or false.
And actually this data set as we'll see later also contains the actual breed of 37 different
cat and dog breeds, so you can also grab that from the filename.
In each of those two cases we're trying to predict a category “Is it a cat, or is it
a dog?” or “Is it a German Shepherd, or a Beagle, or a Ragdoll cat, or whatever?”
When you're trying to predict a category, so when the label is a category, we call that
a classification model.
On the other hand, you might try to predict how old is the animal, or how tall is it,
or something like that, which is like a continuous number that could be like 13.2 or 26.5 or
whatever.
Anytime you're trying to predict a number, your label is a number you call that regression.
Okay?
So those are the two main types of model classification and regressions.
This is very important jargon to know about.
So the regression model attempts to predict one or more numeric quantities such as temperature,
or location, or whatever.
This is a bit confusing, because sometimes people use the word regression as a shortcut
to a particular, for a…
Like an abbreviation for a particular kind of model, called linear regression.
That's super confusing, because that's not what regression means.
Linear regression is just a particular kind of regression but I just wanted to warn you
of that.
When you start talking about regression a lot of people will assume you're talking about
linear regression even though that's not what the word means.
All right, so I wanted to talk about this valid percent zero point two thing.
So as we described valid percent grabs, in this case, twenty percent of the data, if
it's zero point two, and puts it aside like in a separate bucket and then when you train
your model, your model doesn't get to look at that data at all.
That data is only used to decide, to show you how accurate your model is.
So if you train for too long, and or with not enough data, and/or a model with too many
parameters, after a while the accuracy of your model will actually get worse, and this
is called overfitting.
Right?
So we use the validation set to ensure that we're not overfitting.
The next line of code that we looked at is this one, where we created something called
a learner.
We'll be learning a lot more about that, but a learner is basically, or is, something which
contains your data and your architecture that is the mathematical function that you're optimizing,
and so a learner is the thing that tries to figure out what are the parameters which best
cause this function to match the labels in this data.
So we’ll be talking a lot more about that, but basically this particular function ResNet34
is the name of a particular architecture which is just very good for computer vision problems.
In fact the name really is ResNet and then 34 tells you how many layers there are.
So you can use ones with bigger numbers here to get more parameters that will take to train,
take more memory, more likely to overfit, but could also create more complex models.
Right now though I wanted to focus on this part here which is metrics equals error rate.
This is where you list the functions that you want to be the...
That you want to be called with your data.
With your validation data and print it out after each epoch, and epoch is what we call
it when you look at every single image in the data set once.
And so after you've looked at every image in the data set once we print out some information
about how you're doing and the most important thing we print out is the result of calling
these metrics so error rate is the name of a metric and it's a function that just prints
out what percent of the validation set are being incorrectly classified by your model.
So our metric is a function that measures the quality of the predictions using the validation
set so error rates one another common metric is accuracy which is just 1 minus error rate
so very important to remember from last week we talked about loss.
Arthur Samuel had this important idea in machine learning that we need some way to figure out
how good our how well our model is doing so that when we change the parameters we can
figure out which set of parameters make that performance measurement get better or worse,
that performance measurement is called the loss.
The loss is not necessarily the same as your metric.
The reason why is a bit subtle and we'll be seeing it in a lot of detail once we delve
into the math in the coming lessons but basically you need a function you need a loss function
where if you change the parameters by just a little bit up or just a little bit down
you can see if the loss gets a little bit better or a little bit worse and it turns
out that error rate and accuracy doesn't tell you that at all because you might change the
parameters by such a small amount that none of your dog's predictions start becoming cats
and none of your cat predictions start becoming dogs.
So like your predictions don't change so your error rate doesn't change.
Loss and metric are closely related but the metric is the thing that you care about the
loss is the thing which your computer is using as the measurement of performance to decide
how to update your parameters.
So we measure overfitting by looking at the metrics on the validation set.
So fast AI always uses the validation set to print out your metrics and overfitting
is like the key thing that machine learning is about it's all about how do we find a model
which fits the data not just for the data that we're training with but for data that
the training algorithm hasn't seen before.
So overfitting results when our model is basically “cheating”.
A model can cheat by saying oh I've seen this exact picture before and I remember that that's
a picture of a cat.
So it might not have learnt what cats look like in general it just remembers you know
that images one four and eight are cats and two and three and five are dogs and learns
nothing actually about what they really look like.
So that's the kind of cheating that we're trying to avoid we don't want it to memorize
our particular data set.
So we split off our validation data and what most of this are words you're seeing on the
screen are from the book okay so I just copied and pasted them.
So if we split off our validation data and make sure that our model never sees it during
training, it's completely untainted by it so we can't possibly cheat.
Not quite true!
We can cheat, the way we could cheat is we could run we could fit a model look at the
result and the validation set, change something a little bit fit another model look at the
validation set change something a little bit we could do that like a hundred times until
we find something with the validation set looks the best.
But now we might have fit the validation set, right?
So if you want to be really rigorous about this you should actually set aside a third
bit of data called the test set that is not used for training and it's not used for your
metrics.
It's actually, you don't look at it until the whole project has finished.
And this is what's used on competition platforms like Kaggle.
On Kaggle, after the competition finishes your performance will be measured against
a data set that you have never seen.
And so, that's a really helpful approach and it's actually a great idea to do that like
even if you're not doing the modeling yourself.
So if you're if you're looking at vendors and you're just trying to decide today go
with IBM or Google or Microsoft and they're all showing you how great their models are,
what you should do is you should say, “Okay you go and build your models and I am going
to hang on to 10% of my data and I'm not going to let you see it at all and when you're all
finished, come back and then I'll run your model on the 10% of data you've never seen”.
Now pulling out your validation and test sets is a bit subtle though.
Here's an example of a simple little data set and this comes from a fantastic blog post
that Rachel wrote that we will link to about creating effective validation sets.
And you can see basically you have some kind of seasonal data set.
Now if you just say, “Okay, fas.ai, I want to model that I want to create a my dataloader
using a valid_percent of 0.2”, it would do this.
It would delete randomly some of the dots, right?
Now, this isn't very helpful because it's we can still cheat because these dots are
right in the middle of other dots and this isn't what would happen in practice.
What would happen in practice is we would want to predict this is sales by date right
we want to predict the sales for next week.
Not the sales for 14 days ago 18 days ago and 29 days ago, okay?
So what you actually need to do to create an effective validation set here is not do
it randomly but instead chop off the end, right?
And so this is what happens in all Kaggle competitions pretty much that involve time,
for instance, is the thing that you have to predict is the next like two weeks or so after
the last data point that they give you and this is what you should do also for your test
set so again if you've got vendors that you're looking at you should say to them okay after
you're all done modeling we're going to check your model against data that is one week later
than you've ever seen before.
And you won't be able to retrain or anything because that's what happens in practice, right?
Okay.
There's a question, I've heard people describe overfitting as training error being below
validation error does this rule of thumb end up being roughly the same as yours?
Okay, so that's a great question.
So, I think what they mean there is training loss versus validation loss.
Because we don't print training error so we do print at the end of each epoch the value
of your loss function for the training set and the value of the loss function for the
validation set.
And if you train for long enough, that's so so if it's training mostly your training loss
will go down and your validation loss will go down.
Because by definition, loss function is defined such as a lower loss function is a better
model.
If you start overfitting, your training loss will keep going down, right?
Because like why wouldn't it?
You know, you're getting better and better parameters.
But your validation loss will start to go up because actually you started fitting to
the specific data points in the training set and so it's not going to actually get better.
It's going to get it's not going to get better for the validation set it'll start to get
worse.
However, that does not necessarily mean that you're overfitting or at least not overfitting
in a bad way as we'll see it's actually possible to be at a point where the validation loss
is getting worse but the validation accuracy or error or metric is still improving.
So I'm not going to describe how that would happen mathematically yet because we need
to learn more about loss functions but we will.
But for now just realize that the important thing to look at is your metric getting worse,
not your loss function getting worse.
Thank you for that fantastic question.
The next important thing we need to learn about is called transfer learning.
So the next line of code said learn.fine_tune.
Why does it say learn.fine_tune?
Fine tune is what we do when we are transfer learning so transfer learning is using a pre-trained
model for a task that is different to what it was originally trained for.
So more jargon to understand our jargon.
Let's look at that.
What's a pre-trained model?
So what happens is remember I told you the architecture we're using is called ResNet-34?
So when we take that ResNet-34 that's just a just a mathematical function okay with lots
of parameters that we're going to fit using machine learning.
There's a big data set called ImageNet, that contains 1.3 million pictures of a thousand
different types of thing, whether it be mushrooms or animals or airplanes or hammers or whatever.
There's a competition or there used to be a competition that runs every year to see
who could get the best accuracy on the ImageNet competition.
And the models that did really well, people would take those specific values of those
parameters and they would make them available on the internet for anybody to download.
So if you download that you don't just have an architecture now you have a trained model.
You have a model that can recognize a thousand categories of thing in images.
Which probably isn't very useful unless you happen to want something that recognizes those
exact thousand categories of thing.
But it turns out you can rather you can start with those weights in your model and then
train some more epochs on your data and you'll end up with a far far more accurate model
than you would if you didn't start with that pre- trained model and we'll see why in just
a moment, right?
But this idea of transfer learning, it's kind of, it makes intuitive sense, right?
ImageNet already has some cats and some dogs in it and it's you know it can say this is
a cat and this is a dog, but you want to maybe do something that recognizes lots of breeds
that aren't in ImageNet.
Well, for it to be able to recognize cats versus dogs versus airplanes versus hammers
it has to understand things like: what does metal look like?
What does fur look like?
What do ears look like?
You know, so it can say like oh this breed of animal, this breed of dog has pointy ears
and oh this thing is metal so it can't be a dog.
So all these kinds of concepts get implicitly learned by a pre-trained model.
So if you start with a pre-trained model then you don't have to learn all these features
from scratch, and so transfer learning is the single most important thing for being
able to use less data and less compute and get better accuracy.
So that's a key focus for the fastai library and a key focus for this course.
There's a question: I am a bit confused on the differences between loss, error, and metric.
Sure, so error is just one kind of metric so there's lots of different possible labels
you could have.
Let's say you were trying to create a model which could predict how old a cat or dog is.
So the metric you might use is: on average, how many years were you off by?
So that would be a metric.
On the other hand if you're trying to predict whether this is a cat or a dog your metric
would be: what percentage of the time am I wrong?
So that latter metric is called the error rate.
Okay so error is one particular metric.
It's a thing that measures how well you're doing and it's like it should be the thing
that you most care about.
So you write a function or use one of fastai's predefined ones which measures how well you're
doing.
Loss is the thing that we talked about in Lesson One so I'll give a quick summary but
go back to lesson one if you don't remember.
Arthur Samuel talked about how a machine learning model needs some measure of performance which
we can look at: when we adjust our parameters up or down does that measure of performance
get better or worse?
And as I mentioned earlier, some metrics possibly won't change at all if you move the parameters
up and down just a little bit.
So they can't be used for this purpose of adjusting the parameters to find a better
measure of performance.
So quite often we need to use a different function we call this the loss function and
the loss function is the measure of performance that the algorithm uses to try to make the
parameters better and it's something which should kind of track pretty closely to the
the metric you care about but it's something which, as you change the parameters a bit,
the loss should always change a bit.
And so there's a lot of hand waving there because we need to look at some of the math
of how that works and we'll be doing that in the next couple of lessons.
Thanks for their great questions.
Okay so fine tuning is a particular transfer learning technique where the -- oh and you're
still showing your picture and not the slides.
So fine-tuning is a transfer learning technique where the weights (this is not quite the right
word we should say the parameters) where the parameters of a pre-trained model are updated
by training for additional epochs using a different task to that used for pre-training.
So pre-training the task might have been ImageNet classification and then our different task
might be recognizing cats versus dogs.
So the way by default fastai does fine tuning is that we use one epoch, which, remember,
is one looking at every image in the data set once.
One epoch to fit just those parts of the model necessary to get the particular part of the
model that's especially for your data set working.
And then we use as many epochs as you asked for to fit the whole model.
And so this is more if you for those people who might be a bit more advanced we'll see
exactly how this works later on in the lessons.
So why does transfer learning work, and why does it work so well?
The best way in my opinion to look at this is to see this paper by Zeiler and Fergus,
who were actually 2012 ImageNet winners and interestingly their key insights came from
their ability to visualize what's going on inside a model.
And so visualization very often turns out to be super important to getting great results.
What they were able to do was they looked -- remember I told you like a resnet 34 has
34 layers?
They looked at something called AlexNet which was the previous winner of the competition,
which only had seven layers.
At the time that was considered huge and so they took the seven layer model and they said
what does the first layer of parameters look like?
And they figured it out how to draw a picture of them right?
And so the first layer had lots and lots of features but here are nine of them one two
three four five six seven eight nine.
And here's what nine of those pictures look like.
One of them was something that could recognize diagonal lines from top left to bottom right.
One of them could find diagonal lines from bottom left to top right.
One of them could find gradients that went from the top of orange to the bottom of blue.
Some of them were able you know, one of them was specifically for finding things that were
green, and so forth right.
So for each of these nine, they're called filters or features.
So then something really interesting they did was they looked at each one of these,
each one of these filters, each one of these features, and we'll learn kind of mathematically
about what these actually mean in the coming lessons but for now, let's just recognize
them and saying oh there's something that looks at diagonal lines and something that
looks at gradients and they found in the actual images in imagenet specific examples of parts
of photos that match that filter.
So for this top left filter here are nine actual patches of real photos that match that
filter and as you can see they're all diagonal lines.
And so here's the for the green one here's parts of actual photos that match the green
one.
So layer one is super super simple and one of the interesting things to note here is
that something that can recognize gradients and patches of color and lines is likely to
be useful for lots of other tasks as well not just imagenet.
So you can kind of see how something that can do this might also be good at many many
other computer vision tasks as well.
This is layer 2, layer 2 takes the features of layer 1 and combines them.
So it can not just find edges that can find corners or repeating curving patterns or semi
circles or full circles.
And so you can see for example here's a, it's kind of hard to exactly visualize these layers
after layer 1.
You kind of have to show examples of what the filters look like.
But here you can see examples of parts of photos that these, this layer 2 circular filter
has activated on.
And as you can see it's found things, with circles.
So interestingly this one which is this kind of blotchy gradient seems to be very good
at finding sunsets.
And this repeating vertical pattern is very good at finding, like curtains and wheat fields
and stuff.
So the further we get, layer three then gets to combine all the kinds of features in layer
two.
And remember we're only seeing so anything here are twelve of the features but actually
there's probably hundreds of them.
I don't remember exactly in alex net but there's lots.
But by the time we get to layer three by combining features from layer two it already has something
which is finding text.
So this is a feature which can find bits of image that contain text.
It's already got something which can find repeating geometric patterns.
And you see this is not just like a matching specific pixel patterns.
This is like a semantic concept.
It can find repeating circles or repeating squares or repeating hexagons.
Great.
So it's really like computing, it's not just matching a template.
And remember we know that neural networks can solve any possible computable function.
So it can certainly do that.
So layer four gets to combine all the filters from layer three anyway at once.
And so by layer four we have something that can find dog faces for instance.
So you can kind of see how each layer we get like more applicatively more sophisticated
features.
And so that's why these deep neural networks can be so incredibly powerful.
It's also why transfer learning can work so well.
Because like, if we wanted something that can find books.
And I don't think there's a book category in imagenet.
Well it's actually already got something that can find text as an earlier filter which I
guess it must be using to find maybe there's a category for library or something or a bookshelf.
So when you use transfer learning you can take advantage of all of these pre-learnt
features to find things that are as combinations of these or existing features.
That's why transfer learning can be done so much more quickly and so much less data than
traditional approaches.
One important thing to realize then is that these techniques for computer vision are not
just good at recognizing photos; there's all kinds of things you can turn into pictures,
for example these are sounds that have been turned into pictures by representing their
frequencies over time and it turns out that if you convert a sound into these kinds of
pictures you can get basically state-of-the-art results at sound detection just by using the
exact same resnet learner that we've already seen.
We need to highlight that it's 945 so if you want to take a break soon?
A really cool example from I think our very first year of running fastai; one of our students
created pictures, they worked at Splunk in anti-fraud, and they created pictures of users
moving their mouse and, if I remember correctly as they moved their mouse he basically drew
a picture of where the mouse moved and the color depended on how fast they moved and
these circular blobs is where they clicked the left or the right mouse button.
At Splunk what he did actually for the course, as a project for the course, is he tried to
see whether he could use this these pictures with exactly the same approach we saw in lesson
1 to create an anti-fraud model, and it worked so well that Splunk ended up patenting a new
product based on this technique and you can actually check it out there's a blog post
about it on the internet where they describe this breakthrough anti-fraud approach which
literally came from one of our really amazing and brilliant and creative students after
lesson one of the course.
Another cool example of this is looking at different viruses and again turning them into
pictures and you can kind of see how they've got here this is from a paper, check out the
book for the citation, they've got three examples of a particular virus called VB.AT and another
example of a particular virus called Fakerean and you can see in each case the pictures
all look kind of similar and that's why again they can get state-of-the-art results in virus
detection; by turning the program signatures into pictures and putting it through image
recognition.
So in the book you'll find a list of all of the terms, all of the most important terms,
we've seen so far and what they mean I'm not going to read through them but I want you
to please because these are the terms that we're going to be using from now on and you've
got to know what they mean because if you don't you're going to be really confused because
I'll be talking about labels and architectures and models and parameters and they have very
specific exact meanings and I'll be using those exact meanings, so please review this.
So to remind you this is where we got to; we ended up with Arthur Samuels overall approach
and we replaced his terms with our terms so we have an architecture which contains parameters
as inputs, well parameters and the data as inputs so that the architecture plus the parameters
are the model, with the inputs they used to calculate predictions, they are compared to
the labels with a loss function and that loss function is used to update the parameters
many many times to make them better and better until the loss gets nice and super low.
So this is the end of chapter 1 of the book.
It's really important to look at the questionnaire because the questionnaire is the thing where
you can check whether you have taken away from this book, this chapter the stuff that
we hope you have.
So go through it and anything that you're not sure about, the answer is in the text
so just go back to earlier in the book and in the chapter you will find the answers.
There's also a further research section after each questionnaire, for the first couple of
chapters they're actually pretty simple hopefully they're pretty fun and interesting; they're
things where to answer the question it's not enough to just look in the chapter, you actually
have to go and do your own thinking and experimenting and googling and so forth.
In later chapters some of these further research things are pretty significant projects that
might take a few days or even weeks and so check them out because hopefully they'll be
a great way to expand your understanding of the material.
So something that Sylvain points out in the book is that if you really want to make the
most of this then after each chapter please take the time to experiment with your own
project and within the books we provide and then see if you can redo the notebooks on
a new dataset.
Perhaps for chapter one that might be a bit hard because we haven't really shown how to
change things but for chapter two, which we're going to start next, you'll absolutely be
able to do that.
Okay so let's take a 5 minute break and we'll come back at 9:55 San Francisco time.
Okay so welcome back everybody and I think we've got a couple of questions to start with
so Rachel please take it away.
Sure, are filters independent by that I mean if filters are pre-trained might they become
less good and detecting features of previous images when fine-tuned?
Oh that is a great question, so assuming I understand the question correctly, if you
start with say an imagenet model and then you fine-tune it on dogs versus cats for a
few epochs and you get something that's very good at recognizing dogs versus cats it's
going to be much less good as an imagenet model after that, so it's not going to be
very good at recognizing aeroplanes or hammers or whatever.
This is called catastrophic forgetting in the literature, the idea that as you see more
images about different things to what you saw earlier that you start to forget what
the things you saw earlier are.
So if you want to fine-tune something which is good at a new task but also continues to
be good at the previous task you need to keep putting in examples of the previous task as
well.
What are the differences between parameters and hyper parameters?
If I am feeding an image of a dog as an input and then changing the hyper parameters of
batch size in the model what would be an example of a parameter?
So the parameters are the things that are described in lesson one that Arthur Samuel
described as being the things which change what the model does, what the architecture
does.
So we start with this infinitely flexible function, the thing called a neural network,
that can do anything at all and the way you get it to do one thing versus another thing
is by changing its parameters.
They are the numbers that you pass into that function so there's two types of numbers you
pass into the function: there's the numbers that represent your input, like the pixels
of your dog, and there's the numbers that represent their learnt parameters.
So in the example of something that's not a neural net, but like a checkers playing
program like Arthur Samuel might have used back in the early 60s and late 50s, those
parameters may have been things like: if there is a opportunity to take a piece versus an
opportunity to get to the end of a board how much more value should I consider one versus
the other.
You know it's twice as important or it's three times as important -- that two versus three
-- that would be an example of a parameter.
In a neural network, parameters are a much more abstract concept and so a detailed understanding
of what they are will come in the next lesson or two, but it's the same basic idea: they’re
the numbers which change what the model does to be something that recognizes malignant
tumors, versus cats versus dogs versus colorizes black and white pictures.
Whereas the hyperparameter is the choices about what numbers do you pass to the function,
to the actual fitting function to decide how that fitting process happens.
There's a question, “I'm curious about the pacing of this course.
I'm concerned that all the material may not be covered.”
Depends what you mean by all the material.
We certainly won't cover everything in the world, so yeah we'll cover what we can.
We’ll cover what we can in seven lessons; we're certainly not covering the whole book
if that's what you're wondering.
The whole book will be covered in either two or three courses.
In the past it's generally been two courses to cover about the amount of stuff in the
book but we'll see how it goes, because the book’s pretty big -- 500 pages.
So when you say two courses, you mean fourteen lesson?
Fourteen, yes it would be like 14 or 21 lessons to get through the whole book.
Although having said that, by the end of the first lesson hopefully there'll be kind of
like enough momentum and understanding that reading the book independently will be more
useful and you'll have also kind of gained a community of folks on the forums that you
can hang out with and ask questions of and so forth.
So in in the second part of the course we're going to be talking about putting stuff in
production and so to do that, we need to understand like what are the capabilities and limitations
of deep learning?
What are the kinds of projects that even make sense to try to put in production?
And you know one of the key things I should mention in the book and in this course is
that the first two or three lessons and chapters, there's a lot of stuff which is designed not
just for the coders but for, for everybody.
There's lots of information about, what are the practical things you need to know to make
deep learning work.
And so one of them, things you need to know is, “well what's deep learning actually
good at at the moment?”
So I'll summarize what the book says about this, but there are the kind of four key areas
that we have as applications in Fastai: computer vision, text, tabular, and what I've called
here “Recsys”, for recommendation systems and specifically a technique called collaborative
filtering which we briefly saw...
Sorry another question, are there any pre-trained weights available other than the ones from
Imagenet that we can use?
If yes, when should we use others and when Imagenet?
Oh that's a really great question.
So yes there are a lot of pre-trained models, and one way to find them..
And also you're currently just showing us..
Ok great.
One great way to find them is you can look up models zoo which is a common name for places
that have lots of different models.
And so here's lots of models zoos.
Or you can look for pre-trained models.
And so yeah, there's quite a few, unfortunately not as wide a variety as I would like that
most is still on Imagenet or similar kinds of general photos.
For example medical imaging there's hardly any.
There's a lot of opportunities for people to create domain-specific pre-trained models
it's it's still an area that's really underdone because not enough people are working on transfer
learning.
Okay, so as I was mentioning we've kind of got these four applications that we've talked
about a bit and deep learning is pretty, you know, pretty good at all of those tabular
data like spreadsheets and database tables is an area where deep learning is not always
the best choice but it's particularly good for things involving high cardinality variables,
that means variables that have like lots and lots of discrete levels like zip code or product
ID or something like that.
Deep learning is really pretty great for those in particular.
For text it's pretty great at things like classification and translation.
It's actually terrible for conversation and so that's that's been something that's been
a huge disappointment for a lot of companies I tried to create these like conversation
bots, but actually deep learning isn't good at providing accurate information it's good
at providing things that sound accurate and sound compelling but that we don't really
have great ways yet of actually making sure it's correct.
One big issue for recommendation systems collaborative filtering is that deep learning is focused
on making predictions which don't necessarily actually mean creating useful recommendations.
We'll see what that means in a moment.
Deep learning is also good at multimodal that means things where you've got multiple different
types of data so you might have some tabular data including a text column and an image,
then some collaborative filtering data and combining that all together is something that
deep learning is really good at.
So for example putting captions on photos is something which deep learning is pretty
good at, although again, it's not very good at being accurate.
So what you know might say this is a picture of two birds when it's actually a picture
of three birds and then this other category there's lots and lots of things that you can
do with deep learning by being creative about the use of these kinds of other application
based approaches, for example an approach that we developed for natural language processing
called ULMFit that we will be learning in the course.
It turns out that it's also fantastic you're doing protein analysis.
If you think of the different proteins as being different words and they're in a sequence
which has some kind of state and meaning it turns out that ULMFit works really well for
protein analysis.
So often it's about kind of being being creative.
So to decide like for the product that you're trying to build is deep learning gonna work
well for it, in the end you kind of just have to try it and see but if you if you do a search
you know hopefully you can find examples about the people that have tried something similar
even if you can't that doesn't mean it's not going to work.
So for example I mentioned the collaborative filtering issue where a recommendation and
a prediction are not necessarily the same thing.
You can see this on Amazon for example quite often.
So I bought a Terry Pratchett book and then Amazon tried for months to get me to buy more
Terry Pratchett books.
Now that must be because their predictive model said that people who bought one particular
Terry Pratchett book are likely to also buy a other Terry Pratchett books.
But from the point of view of like well is this going to change my buying behavior: probably
not, right, like if I liked that book I already know I like that author and I already know
that like they probably wrote other things so I'll go and buy it anyway.
So this would be an example of like Amazon probably not being very smart, up here they're
actually showing me collaborative filtering predictions rather than actually figuring
out how to optimize a recommendation.
So an optimized recommendation would be something more like your local human bookseller might
do, where they might say, “Oh! you like Terry Pratchett, well let me tell you about
other kind of comedy fantasy sci-fi writers on the similar vein who you might not have
heard about before”.
So the difference between recommendations and predictions is super important.
So I wanted to talk about a really important issue around interpreting models and for a
case study for this I thought we let's pick something that's actually super important
right now which is a model in this paper.
One of the things we're going to try and do in this course is learn how to read papers.
So here is a paper which you I would love for everybody to read called high temperature
and high humidity reduce the transmission of COVID-19.
Now this is a very important issue because if the claim of this paper is true then that
would mean that this is going to be a seasonal disease and if this is a seasonal disease
and it's going to have massive policy implications.
So let's try and find out how this was modeled and understand how to interpret this model.
So this is a key picture from the paper and what they've done here is they've taken a
hundred cities in China and they've plotted the temperature on one axis, in Celsius, and
R on the other axis, where R is a measure of transmissibility.
It says for each person that has this disease how many people on average will they infect.
So if R is under 1, then the disease will not spread.
If R is higher than like 2 it's going to spread incredibly quickly.
Basically R is going to, you know, any high R is going to create an exponential transmission
impact.
And you can see in this case they have plotted a best fit line through here.
Then they've made a claim that there's some particular relationship in terms of a formula
that R is 1.99 minus 0.023 times temperature.
So very obvious concern I would have looking at this picture is that this might just be
random, maybe there's no relationship at all but just if you picked a hundred cities at
random perhaps they were sometimes show this level of relationship.
So one simple way to kind of see that would be to actually do it in a spreadsheet.
So here is a spreadsheet.
What I did was I kind of eyeballed this data and I guessed what is the mean degrees centigrade.
I think it's about 5.
What about the standard deviation of centigrade.
I think it's probably about 5 as well.
And then I did the same thing for R. I think the mean R looks like it's about 1.9 to me.
And it looks like the standard deviation of R is probably about 0.5.
So what I then did was I just jumped over here and I created a random normal value,
so a random value from a normal distribution, so a bell curve, with that particular mean
and standard deviation of temperature and that particular mean and standard deviation
of R. And so this would be an example of a city that might be in this data set of a hundred
cities.
Something with 9 degrees Celsius and R of 1.1; so that would be 9 degrees Celsius and
R of 1.1, something about here.
So then I just copied that formula down 100 times.
So here are a hundred cities that could be in China right, where this is assuming that
there is no relationship between temperature and R right.
They are just random numbers and so each time I recalculate that so if I hit control equals
it will just recalculate it right.
I get different numbers okay because they're random.
And so you can see at the top here I've then got the average of all of the temperatures
and the average of all of the R and the average of all the temperatures varies and the average
of all of the R varies as well.
So then what I did was I copied those random numbers over here.
let's actually do it.
So I'll go copy these 100 random numbers and paste them here here here here.
And so now I've got 1 2 3 4 5 6 I've got 6 kind of groups of 100 cities.
All right and so let's stop those from randomly changing any more by just fixing them in stone
there.
Okay, so now that I've pasted them in, I've got 6 examples of what a hundred cities might
look like if there was no relationship at all between temperature and R. I've got their
mean temperature and R in each of those six examples.
What I've done, is you can see here, at least for the first one, is I've plotted it, right?
You can see, in this case, there's actually a slight positive slope.
I've actually calculated the slope for each, just by using the slope function in Microsoft
Excel.
You can see that actually, in this particular case, is just random - five times it's been
negative, and it's even more negative than their 0.023.
So you can like, it's kind of matching our intuition here, which is that the slope of
the line that we have here, is something that absolutely can often happen totally by chance.
It doesn't seem to be indicating any kind of real relationship at all.
If we wanted that slope to be more confident, we would need to look at more cities.
Here I've got 3,000 randomly generated numbers.
You can see here the slope is 0.00002, right?
It's almost exactly zero, which is what we'd expect, when there's actually no relationship
between C and R, and in this case there isn't - they're all random . Then if we look at
lots and lots of randomly generated cities, then we can say, oh yeah, there's no slope.
But when you only look at a hundred, as we did here, you're going to see relationships
totally coincidentally, very, very often.
So that's something that we need to be able to measure.
One way to measure that is we use something called a p-value.
A p-value, here's how a p-value works: we start out with something called a null hypothesis.
The null hypothesis is basically what's our starting point assumption.
Our starting point assumption might be, oh there's no relationship between temperature
and R. And then we gather some data and (Rachel: have you explained what R is?)
I have, yes.
R is the transmissibility of the virus.
So then we gather data of independent and dependent variables - in this case the independent
variable is the thing that we think might cause the dependent variable.
Here the independent variable would be temperature, the dependent variable would be R. So here
we've gathered data - there's the data that was gathered in this example, and then we
say what percentage of the time would we see this amount of relationship, which is a slope
of 0.023 by chance?
And as we've seen, one way to do that is by, what we would call, a simulation, which is
by generating random numbers - a 100 set pairs of random numbers, a bunch of times, and seeing
how often you see this relationship.
We don't actually have to do it though.
There's actually a simple equation we can use to jump straight to this number, which
is, what percent of the time would we see that relationship by chance?
And this is basically what that looks like.
We have the most likely observation, which in this case would be if there is no relationship
between temperature.
Then the most likely slope would be zero, and sometimes you get positive slopes by chance,
and sometimes you get pretty small slopes, and sometimes you get large negative slopes
by chance.
And so, the larger the number, the less likely it is to happen, whether it be on the positive
side or the negative side.
In our case, our question was - how often are we going to get less than negative 0.023?
It would actually be somewhere down here.
I actually copy this from Wikipedia, where they were looking for positive numbers, and
so they've colored in this area above the number.
This is the p-value, and we don't care about the math but there's a simple little equation
you can use to directly figure out this number - the p-value - from the data.
This is kind of how nearly all kind of medical research results tend to be shown, and folks
really focus on this idea of p-values.
And indeed, in this particular study as we’ll see in a moment, they reported p-values.
Probably a lot of you have seen p-values in your previous lives.
They come up in a lot of different domains.
Here's the thing - they are terrible.
You almost always shouldn't be using them.
Don't just trust me.
Trust the American Statistical Association.
They point out six things about p-values, and those include: p-values do not measure
the probability that the hypothesis is true, or, the probability that the data were produced
by random choice alone.
Now we know this because we just saw that, if we use more data, if we sample three thousand
random cities rather than a hundred, we get a much smaller value.
So p-values don't just tell you about how big a relationship is, but they actually tell
you about a combination of that, and, how much data did you collect.
So they don't measure the probability that the hypothesis is true.
So therefore, conclusions and policy decisions should not be based on whether a p-value passes
some threshold.
P-value does not measure the importance of a result, because, again, it could just tell
you that you collected lots of data, which doesn't tell you that the results are actually
of any practical import.
By itself, it does not provide a good measure of evidence.
Frank Harrell, who is somebody whom I read his book, and it's a really important part
of my learning.
He's a professor of biostatistics, has a number of great articles about this.
He says null hypothesis testing and p-values have done significant harm to science.
He wrote another piece called “null hypothesis significance testing never worked”.
I've shown you what p-values are so that you know why they don't work, not so that you
can use them.
But they're a super important part of machine learning because they come up all the time.
When people are saying, this is how we decide whether your drug worked, or whether there
is an epidemiological relationship, or whatever.
And indeed, p-values appear in this paper.
In the paper, they show the results of a multiple linear regression.
They put three stars next to any relationship which has a p-value of 0.01 or less.
There is something useful to say about a small p-value, like 0.01 or less.
Which is the thing that we're looking at did not, probably did not happen by chance, right?
The biggest statistical error people make all the time is that they see that a p-value
is not less than 0.05 and then they make the erroneous conclusion that no relationship
exists, right?
Which doesn't make any sense because like let's say you only had like three data points
then you almost certainly won't have enough data to have a p-value of less than 0.05 for
any hypothesis.
So like the way to check, is to go back and say, what if I picked the exact opposite null
hypothesis?
What if my null hypothesis was there is a relationship between temperature and R?
Then do I have enough data to reject that null hypothesis, alright?
And if the answer is no, then you just don't have enough data to make any conclusions at
all, alright?
So in this case they do have enough data to be confident that there is a relationship
between temperature and R. Now that's weird because we just looked at the graph, and we
did a little back of a bit of a back-of-the-envelope in Excel and we thought this is, could it,
could well be random.
So here's where the issue is.
The graph shows what we call a univariate relationship.
A univariate relationship shows the relationship between one independent variable and one dependent
variable, and that's what you can normally show on a graph.
But in this case they did a multivariate model in which they looked at temperature, and humidity,
and GDP per capita, and population density, and when you put all of those things into
the model then you end up with statistically significant results for temperature and humidity.
Why does that happen?
Well the reason that happens is because all these variations in the blue dots, is not
random.
There's a reason they're different, right?
And the reasons include, denser cities are going to have higher transmission, for instance,
and probably more humid will have less transmission.
So when you do a multivariate model, it actually allows you to be more confident of your results,
right?
But the p-value as noted by the American Statistical Association does not tell us whether this
is of practical importance.
The thing that tells us if this is of practical as importance, is the actual slope that's
found.
And so in this case the equation they come up with is that R = three point nine six eight
minus three point O point O three eight by temperature minus point zero two four by relative
humidity this is this equation is this practically important.
Well we can again do a little back of the envelope here, by just putting that into Excel.
Let's say there was one place it had a temperature of ten centigrade and a humidity of forty,
then if this equation is correct R would be about two point seven somewhere with the temperature
of 35 centigrade and a humidity of eighty will be about point eight.
So is this practically important?
Oh my god yes, right?
Two different cities, with different climates can be, if they're the same in every other
way, and this model is correct then one city would have no spread of disease (because R
is less than 1), one would have massive exponential explosion.
So we can see from this model that if the modeling is correct, then this is a highly
practically significant result.
So this is how you determine practical significance of your models is not with p-values but with
looking at kind of actual outcomes.
So how do you think about the practical importance of a model and how do you turn a predictive
model into something useful in production.
So I spent many many years thinking about this, and I actually created a with some other
great folks actually created a paper about it.
"Designing Great Data Products" And this is largely based on ten years of work I did at
a company I founded called Optimal Decisions Group.
And Optimal Decisions Group was focused on the question of helping insurance companies
figure out what prices to set.
And insurance companies up until that point had focused on predictive modeling.
Actuaries, in particular, spent their time trying to figure out how likely is it that
you're going to crash your car and if you do how much damage might you have and then
based on that try to figure out what price they should set for your policy.
So for this company, what we did was we decided to use a different approach which I ended
up calling the drivetrain approach which is described here to set insurance prices and
indeed to do all kinds of other things.
And so, for the Insurance example, the objective would be for an insurance company would be
how do I maximize my, let's say, five-year profit.
And then, what inputs can we control can we control which what I call levers - so in this
case it would be what price can I set.
And then data is data which can tell you as you change your levers how does that change
your objective.
So if I start increasing my price to people who are likely to crash their car, then we
will get less of them which means we have less costs, but at the same time, we'll also
have less revenue coming in, for example.
So to link up there kind of the levers to the objective via the data we collect, we
build models that described how the levers influence the objective.
And this is all like it seems pretty obvious when you say it like this but when we started
work with Optimal Decisions in 1999, nobody was doing this in insurance, Everybody in
insurance was simply doing a predictive model to guess how likely people were to crash their
car, and then pricing was set by like adding 20% or whatever.
It was just done in a very kind of naive way.
So what I did is I, you know, over many years took this basic process and tried to help
lots of companies figure out how to use it to turn predictive models into actions.
So the starting point in like actually getting value in a particular model is thinking about
what is it you're trying to do, and you know what are the sources of value in that thing
you're trying to do.
The levers - what are the things you can change?
Like what's the point of a predictive model if you can't do anything about it, right?
Figuring out ways to find what data you, you don't have, which ones suitable, what's available,
then thinking about what approaches to analytics you can then take.
And then super important, like well, can you actually implement, you know, those changes.
And super super important how do you actually change things as the environment changes.
And, you know, interestingly a lot of these things are areas where there's not very much
academic research.
There's a little bit.
And some of the papers that have been particularly around “maintenance” of like; How do you
decide when your machine learning model is kind of still okay?
How do you update it over time?
Have had like many many many many citations, but they don't pop up very often because a
lot of folks are so focused on the math.
You know.
And then there's the whole question of like “What constraints are in place across this
whole thing?”
So what you'll find in the book, is there is a whole appendix which actually goes through
every one of these six things.
And has a whole list of examples.
So this is an example of how to like think about value.
And lots of questions that companies and organizations can use to try and think about, you know,
all of these different pieces of the actual puzzle of getting stuff into production and
actually into an effective product.
We have a question.
Sure, just a moment.
So I was going to say, so do check out this appendix because it actually originally appeared
as a blog post and I think, except for my covid-19 posts that I did with Rachel, it's
actually the most popular blog post I've ever written.
It’s had hundreds of thousands of views.
And it kind of represents like 20 years of hard won insights about like how you actually
get value from machine learning and practice and what you actually have to ask.
So please check it out because hopefully you'll find it helpful.
So when we think about like think about this for the question of how should people think
about the relationship between seasonality and transmissibility of covid-19, you kind
of need to dig really deeply into the questions about like oh not just what what's that what
are those numbers in the data, but what does it really look like right.
So one of the things in the paper that they show is actual maps, right of temperature
and humidity and R right.
And you can see like, not surprisingly, that humidity and temperature in China are what
we would call auto-correlated.
Which is to say that places that are close to each other, in this case geographically,
have similar temperatures and similar humidities.
And so like this actually puts into the question a lot the p values that they have right.
Because you can't really think of these as a hundred totally separate cities.
Because the ones that are close to each other probably have very close behavior so maybe
you should think of them as like a small number of sets of cities, you know of kind of larger
geographies.
So these are the kinds of things that when you look actually into a model you need to
like think about what are, what are the limitations?
But then to decide like well, what does that mean?
What do I what do about that?
You need to think of it from this kind of utility point of view, this kind of end to
end, what are the actions I can take?
What are the order the results point of view?
Not just null hypothesis testing.
So in this case for example there are basically four possible key ways this could end up.
It could end up that there really is a relationship between temperature and R, or so that's but
the right hand side is.
Or there is no real relationship between temperature and R. And we might act on the assumption
that there is a relationship.
Or we might act on the assumption that there isn't a relationship.
And so you kind of want to look at each of these four possibilities and say like well
what would be the economic and societal consequences?
And you know there's gonna be a huge difference in lives lost and you know economies crashing
and whatever else - you know for each of these four.
The paper actually you know has shown, if their model is correct, what's the likely
R value in March for like every city in the world.
And the likely R value in July for every city in the world.
And so for example if you look at kind of New England and New York, the prediction here
is and also West, the other the very coast of the west coast is that in July the disease
will stop spreading.
Now you know if that happens, if they're right then, that's gonna be a disaster because I
think it's very likely in America and also the UK, that people will say “Oh turns out
this disease is not a problem you know.
It didn't really take off at all.
The scientists were wrong.”
People will go back to their previous day-to-day life and we could see what happened in 1918
flu virus of like the second go around.
When winter hits could be much worse than the start right.
So like there's these kind of like huge potential policy impacts depending on whether this is
true or false.
And so to think about it.
Yes?
I also just wanted to say that it would be it would be very irresponsible to think “oh
summer’s gonna solve it.
We don't need to act now.”
Just in that this is something growing exponentially and could do a huge huge amount of damage.
Yeah yes okay.
It already has done by the way.
If you assume that there will be seasonality and that summer will fix things then it could
lead you to be apathetic now.
If you assume there's no seasonality and then there is, then you could end up kind of creating
a larger level of expectation of destruction that actually happens and end up with your
population being even more apathetic you know so that they're you know.
Being wrong in any direction could be a problem.
So one of the ways we tend to deal with this, with with this kind of modeling is we try
to think about priors.
So our priors are basically things where we, you know rather than just having a null hypothesis,
we try and start with a guess as to like well what's what's more likely?
Right so in this case if memory serves correctly I think we know that like flu viruses become
inactive at 27 centigrade we know that like cold, the cold coronaviruses are seasonal.
The 1918 flu pandemic was seasonal.
In every country and city that’s been studied so far, there's been quite a few studies like
this.
They've always found climate relationships so far.
So maybe we'd say: “Well prior belief is that this thing is probably seasonal.”
And so then we’d say: “Well this particular paper adds some evidence to that.”
So it shows how incredibly complex it is to use a model in practice for in this case policy
discussions but also for organizational decisions.
Because, you know, there's always complexities, there's always uncertainties.
And so you actually have to think about the utilities, you know.
And your best guesses and try to combine everything together as best as you can.
Okay.
So with all that said.
It's still nice to be able to get our models up and running even if, you know - even just
a predictive model is sometimes useful on its own.
Sometimes it's useful to prototype something, and sometimes it's got to be part of some
bigger picture.
So rather than try to create some huge end-to-end model here.
We thought we would just show you how to get your Pytorch FastAI model up-and-running.
In as raw a form as possible.
So that from there, you can kind of build on top of it, as you like.
So to do that; we are going to download and curate our own dataset.
And you're going to do the same thing.
You're going to train your own model, on that dataset, and then you're going to create an
application, and then you're going to host it.
Right?
Now, there're lots of ways to curate an image dataset; you might have some photos on your
own computer, there might be stuff at work you can use.
One of the easiest though, is just to download stuff off the internet.
There’s lots of services for downloading stuff off the internet.
We're going to be using Bing Image Search here.
Because they're super easy to use.
A lot of the other kind of easy to use things require breaking the Terms of Service of websites.
So we're not going to show you how to do that.
But there’s lots of examples that do show you how to do that.
So you can check them out as well, if you want to.
Bing Image Search is actually pretty great at least at the moment.
These things change a lot, so keep an eye on our website to see if we've changed our
recommendation.
The biggest problem with Bing Image Search is that the signup process is a nightmare,
at least at the moment.
One of the hardest parts of this book is just signing up to their damn API.
Which requires going through Azure.
It's called Cognitive Services - Azure Cognitive Services.
So we'll make sure that all that information is on the website for you to follow through
just how to sign up.
So we're going to start from the assumption that you've already signed up.
But you can find it, just go: Bing, Bing Image Search API.
And at the moment they give you seven days with a pretty high quota for free.
And then after that, you can keep using it as long as you like but they kind of limit
it to like three transactions per second or something.
Which is still plenty.
You can still do thousands for free so it's at the moment it's pretty great even for free.
So what will happen is when you sign up for Bing Image Search, or any of these kind of
services, they'll give you an API key.
So just replace the ‘XXX’ here with the API key that they give you.
Okay, so that's now going to be called “key”.
In fact, let's do it over here.
Okay, so you'll put in your key and then there's a function we've created called search_images_bing
which is just a super tiny little function.
As you can see, it's just two lines of code -- I was just trying to save a little bit
of time, which will take some take your API key and some search term and return a list
of URLs that match that search term.
As you can see for using this particular service you have to install a particular package,
so we show you how to do that on the site as well.
So once you've done so you'll be able to run this and that will return by default I think
150 URLs.
Okay, so fast.ai comes with a download_url function, so let's just download one of those
images just to check and open it up.
And so what I did was I searched for “grizzly bear” and here I have a grizzly bear.
So then what I did was I said, okay, let's try and create a model that can recognize
grizzly bears versus black bears versus teddy bears, so that way I can find out.
I could set up some video recognition system near our campsite when we're out camping that
gives me bear warnings, but if it's a teddy bear coming then it doesn't warn me and wake
me up, because that would not be scary at all.
So then I just go through each of those three bear types, create a directory with the name
of grizzly or black or teddy bear searched Bing for that particular search term along
with bear and download.
And so download_images is a fast.ai function as well.
So after that I can call get_image_files which is a fast.ai function that will just return
recursively all of the image files inside this path.
And you can see it's given me bears/black/ and then lots of numbers.
So one of the things you have to be careful of is that a lot of the stuff you download
will turn out to be like not images at all and will break.
So you can call verify_images to check that all of these file names are actual images.
And in this case I didn't have any failed, so there's it's empty.
But if you did have some, then you would call Path.unlink to unlink.
Path.unlink is part of the Python standard library and it deletes a file.
And map is something that will call this function for every element of this collection.
This is part of a special fast.ai class called “L”.
It’s basically it's kind of a mix between the Python standard library list class and
a numpy array class, Then we'll be learning more about it later in this course, but it
basically tries to make it super easy to do kind of more functional-style programming
in Python.
So in this case it's going to unlink everything that's in the failed list, which is probably
what we want now, because there are all the images that fail to verify.
All right, so we've now got a path that contains a whole bunch of images and they're classified
according to black, grizzly, or teddy, based on what folder they're in. and so to create
so we're going to create a model.
and so to create a model the first thing we need to do is to tell fast.ai what kind of
data we have and how it’s structured.
Now in part in Lesson 1 of the course we did that by using what we call a factory method
which is we just said image_data_loader start from name, and it did it all for us.
Those factory methods are fine for beginners, but now we're into Lesson 2.
We're not quite beginners anymore, so we're going to show you the super super flexible
way to use data in whatever format you like, and it's called the DataBlock API.
And so the DataBlock API looks like this.
Here's the DataBlock API.
You tell fast.ai what your independent variable is and what your dependent variable is.
So what your labels are and what your input data is.
So in this case our input data are images and our labels are categories.
So category is going to be either grizzly, or black, or teddy.
So that's the first thing you tell it.
Now that's the block's parameter.
And then you tell it - how do you get a list of all of the, in this case file names, right.
And we just saw how to do that because we just called the function ourselves.
The function is called get_image_files.
So we tell it what function to use to get that list of items and then you tell it - how
do you split the data into a validation set and a training set.
And so we're going to use something called a RandomSplitter which just splits it randomly.
And we're going to point 30% of it into the validation set.
We're also going to set the random seed which ensures that every time we run this, the validation
set will be the same.
And then you say, okay, how do you label the data.
And this is the name of a function called parent_label.
And so that's going to look for each item at the name of the parent.
So this, this particular one would become a black bear.
Now this is like the most common way for image datasets to be represented, is that they get
put the different images get the files get put into folder according to their label.
And then finally here we've got something called item_tfms.
We'll be learning a lot more about transforms in a moment.
That these are basically functions that get applied to each image.
And so each image is going to be resized to 128 by 128 square.
So we're going to be learning more about DataBlock API soon.
But basically the process is going to be -- it's going to call whatever is get_items, which
is a list of image files.
And then it’s going to call get_x, get_y so in this case there's no get_x but there
is a get_y so it's just parent label.
And then it's going to call the create method for each of these two things - it's going
to create an image and it's going to create a category.
And so I'm going to call the item_tfms, which is resize.
And then the next thing it does is it puts it into something called a data loader.
A data loader is something that grabs a few images at a time (I think by default it’s
64) and puts them all into a single, it's called a batch.
It just grabs 64 images and sticks them all together.
And the reason it does that is it then puts them all onto the GPU at once so it can pass
them all to the model through the GPU in one go.
And that's going to let the GPU go much faster, as we'll be learning about.
And then finally (we don't use any here), we can have something called batch transforms,
which we will talk about later.
And then somewhere in the mineral about here conceptually is the splitter which is the
thing that splits into the training set and the validation set.
So this is a super flexible way to tell fast.ai how to work with your data.
And so at the end of that it returns an object of type DataLoaders.
That's why we always call these things DLs, right.
So, DataLoaders has a validation and a training DataLoader.
And a DataLoader as I just mentioned is something that grabs a batch of a few items at a time
and puts it on the GPU for you.
So this is basically the entire code of DataLoaders.
So the details don't matter, I just wanted to point out that like a lot of these concepts
in fast.ai, when you actually look at what they are, they’re incredibly simple little
things.
It's literally something that you just pass in a few data loaders to and it stores them
in an attribute.
And pass and gives you the first one back as .train and second one back as .valid.
So we can create our DataLoaders by first of all creating the DataBlock ,and then we
call the DataLoaders, passing in our path to create DLs.
And then you can call show_batch on that.
You can call show_batch pretty much anything in fast.ai to see your data.
And look, we've got some grizzlies, we've got a teddy, we've got a grizzly.
So you get the idea right.
I'm going to look at these different, I'm going to look at data augmentation next week
so, I'm going to skip over data augmentation and let's just jump straight into trading
your model.
So once we've got DLs, we can just like in Lesson 1, call cnn_learner to create a ResNet.
We’'re going to create a smaller ResNet this time, a ResNet18.
Again, asking for error rate, we can then call .fine_tune again.
So you see it's all the same lines of code we've already seen.
And you can see our error rate goes down from nine to one, so we've got 1% error and after
training for about 25 seconds.
So you can see you know we've only got 450 images we've trained for well less than a
minute and we only have let's look at the confusion matrix so we can say, “I want
to create a classification interpretation class; I want to look at the confusion matrix”
and the confusion matrix, as you can see, it's something that says “for things that
are actually black bears, how many are predicted to be black bears versus grizzly bears versus
teddy bears?”
So, the diagonal are the ones that are all correct and so it looks like we've got two
errors.
We've got one grizzly that was predicted to be black and one black that was predicted
to be grizzly.
A super, super useful method is “plot top losses” and that'll actually show me what
my errors actually look like.
So, this one here was predicted to be a grizzly bear but the label was “black bear”.
This one was the one that's predicted to be a black bear and the label was “grizzly
bear”.
These ones here are not actually wrong.
This is predicted to be “black” and it's actually black.
But, the reason they appear in this is because these are the ones that the model was the
least confident about.
Okay, so we're going to look at the image classifier cleaner next week.
Let's focus on how we then get this into production.
So, to get it into production, we need to export the model.
So, what exporting the model does is that it creates a new file, which by default is
called “export.pkl”, which contains the architecture and all of the parameters of
the model.
So, that is now something that you can copy over to a server somewhere and treat it as
a predefined program, right?
So, then the process of using your trained model on new data kind of in production is
called “inference”.
So, here I've created an inference learner by loading that learner back again, all right,
and so obviously it doesn't make sense to do it right next to after I've saved it in
a notebook.
But, I'm just showing you how it would work right.
So, this is something that you would do on your server- inference.
Remember that once you have trained a model, you can just treat it as a program- you can
pass inputs to it.
So, this is now our program.
This is our bear predictor.
So, I can now call “predict” on it and I can pass it an image and it will tell me-
here it is 99.999% sure- that this is a “grizzly”.
So, I think what we're going to do here is we're going to wrap it up here and next week
we'll finish off by creating an actual GUI for our bear classifier.
We will show how to run it for free on a service called “Binder” and, yeah, and then I
think we'll be ready to dive into some of the details of what's going on behind the
scenes.
Any questions or anything else before we wrap up, Rachel?
No.
Okay, great.
All right, thanks everybody.
So, we hopefully, yeah, I think from here on we've covered, you know, most of the key
kind of underlying foundational stuff from a machine-learning point of view that we're
going to need to cover.
So, we'll be able to, ready to dive into lower-level details of how deep learning works behind the scenes and I think that'll be starting from next week.
So, see you then.
So hello, and welcome to Lesson 3 of Practical Deep Learning for Coders. We were looking
at getting our model into production last week, and so we're going to finish off that
today, and then we're going to start to look behind the scenes at what actually goes on
when we train a neural network. We're going to look at the math of what's going on, and
we're going to learn about SGD and important stuff like. The order is slightly different
to the book: in the book there's a part in the book which says like “Hey, you can either
go to lesson 4 or lesson 3 now and then go back to the other one afterwards” so we're
doing lesson 4 and then lesson 3. Chapter 4 and then Chapter 3, I should say. You can
choose it whichever way you're interested in. Chapter 4 is the more technical chapter
about the foundations of how deep learning really works. Whereas Chapter 3 is all about
ethics, and so with the lessons we'll do that next week. So we're looking at 02 production
notebook, and we've got to look at the fastbook version (the one with…in fact everything
I'm looking at today will be in the fastbook version). And remember last week we had a
look at our bears, and we created this dataloaders object by using the datablock API which i
hope everybody's had a chance to experiment with this week--if you haven't, now's a good
time to do it! We kind of skipped over one of the lines a little, which is this itm_tfms.
So what this is doing here, when we said “Resize”: the images we downloaded from the internet
were lots of different sizes and lots of different aspect ratios some are tall and some are wide
some are square and some are big some are small. When you say resize for an item transform
it means each item (so an item in this case is one image) is going to be resized to 128x128
by squishing it or stretching it. And so we had a look at, you can always say show_batch
to see a few examples, and this is what they look like. Squishing and stretching isn't
the only way that we can resize remember we have to make everything into a square before
we kind of get it into our model. By the time it gets to our model everything has to be
the same size in each mini batch, so that's why... making it a square is not the only
way to do that, but it's the easiest way and it’s by far the most common way. Another
way to do this is we can create another datablock object, and we can make a datablock object
that's an identical copy of an existing datablock object where we can then change just some
pieces and we can do that by calling the “new” method which is super handy. So let's create
another datablock object, and this time with different item_transforms where we resize
using the “Squish” method. We have a question: what are the advantages of having square images
versus rectangular ones? That's a great question. Really, it’s simplicity. If you know all
of your images are rectangular, of a particular aspect ratio to start with, you may as well
just keep them that way. But if you've got some which are tall and some which are wide,
making them all square is kind of the easiest. Otherwise you would have to organize them
such as all of the tall ones ended up in a mini batch nor the wide ones ended up in a
mini batch, and then you'd have to then figure out what the best aspect ratio for each mini
batch is, and we actually have some research that does that in fastai2 ( but it's still
a bit clunky). I should mention... Okay, I just lied to you--the default is not actually
to squish or stretch: the default (I should have said, sorry) the default when we say
resize is actually just to grab the center. So actually all we're doing is we’re grabbing
the center of each image. So if we want to squish or stretch you can add the ResizeMethod.Squish
argument to Resize and you can now see that this black bear is now looking much thinner,
but we have got the kind of leaves that are around on each side for instance.
another question when you use the dls dot new method what can and cannot be changed
-- is it just the transforms? So it's not dls dot new it's bears dot new, right? So
we're not creating a new data loaders object; we're creating a new datablock object. I don't
remember off the top of my head so check the documentation and I'm sure somebody can pop
the answer into the into the forum. So you can see when we use dot squish that this grizzly
bear has got pretty kind of wide and weird-looking and this black bear has got pretty weird and
thin-looking and it's easiest kind of to see what's going on if we use ResizeMethod dot
pad, and what dot pad does as you can see is it just add some black bars around each
side. So you can see the grizzly bear was tall so then when we we stretched (squishing
and stretching are opposites of each other) so when we stretched it it ended up wide and
the black bear was originally a wide rectangle so it ended up looking kind of thin. You don’t
have to user to use zeros. Zeros means pad it with black. You can also say like reflect
to kind of have the pixels will kind of look a bit better that way if you use reflect.
All of these different methods have their own problems; the the pad method is kind of
the cleanest you end up with the correct size, you end up with all of the pixels, but you
also end up with wasted pixels so you kind of end up with wasted computation. The squish
method is the most efficient because you get all of the information you know and and nothing's
kind of wasted, but on the downside your neural nets going to have to learn to kind of like
recognize when something's being squished or stretched. And in some cases it might -- it
wouldn't even know, so if there's two objects you're trying to recognize, one of which tends
to be thin and one of which tends to be thick -- in other words they're the same -- they
could actually be impossible to distinguish. And then the default cropping approach actually
removes some information so in this case, you know, this grizzly bear here we actually
lost a lot of its legs, so if figuring it out, what kind of bear it was required looking
at its feet, well, we don't have its feet anymore. So they all have downsides. So there's
something else that you can do, a different approach, which is instead of to say resize,
you can say RandomResizedCrop. And actually this is the most common approach and what
random resize crop does is each time it actually grabs a different part of the image and kind
of zooms into it, right? So these, this is all the same image and we're just grabbing
a batch of four different versions of it and you can see some are kind of, you know, they're
all squished in different ways and we've kind of selected different subsets and so forth.
Now this kind of seems worse than any of the previous approaches because I'm losing information.
Like this one here -- I've actually lost a whole lot of its, of its back, right, but
the cool thing about this is that remember we want to avoid overfitting. And when you
see a different part of the animal each time, it's much less likely to overfit because you're
not seeing the same image on each epoch that you go around. That make sense? So, so this
random resized crop approach is actually super popular, and so min_scale 0.3 means we're
going to pick at least 30% of the pixels, of kind of the original size each time, and
then we’re going to like zoom in to that that square. So this idea of doing something
so that each time the model sees the image it looks a bit different to last time is called
data augmentation. And this is one type of data augmentation. It's probably the most
common, but there are others and one of the best ways to do data augmentation is to use
this aug_transforms function. And what aug_transforms does is it actually returns a list of different
augmentations. And so there are augmentations which change contrast, which change brightness,
which warps a perspective so you can see in this one here it looks like this bit’s much
closer to you and this moves much away from you because it's kind of been perspective
warped; it rotates them (see this one's actually being rotated), this one's been made really
dark, right?
These are batch transforms not item transforms. The difference is that item transforms happen
one image at a time and so the thing that resizes them all to the same size that has
to be an item transform. Pop it all into a mini batch, put it on the GPU and then a batch
transform happens to a whole mini batch at a time. And by putting these as batch transforms
the augmentation happens super fast because it happens on the GPU. And I don't know if
there's any other libraries as we speak which allow you to write your own GPU accelerated
transformations that run on the GPU in this way. So this is a super handy thing in first
AI 2. So you can check out the documentation or aug transforms and when you do you'll find
the documentation for all of the underlying transforms that it basically wraps. Right
so you can see if I shift tab, I don't remember if i have shown you this trick before - if
you go inside the parentheses of a function and hit shift tab a few times it'll pop open
a list of all of the arguments and so you can basically see you can say like oh can
I sometimes flip it left right, can I sometimes flip it up down, what's the maximum amount
I can rotate, zoom, change the lighting, warp the perspective and so forth. How can we add
different augmentations for train and validation sets? So the cool thing is that automatically
fastai will avoid doing data augmentation on the validation set. So all of these aug
transforms will only be applied to the training set with the exception of RandomResizedCrop.
RandomResizedCrop has a different behavior or each, the behavior for the training set
is what we just saw which is to randomly pick a subset and zoom into it and the behavior
for the validation set is just to grab the center, the largest center square that it
can. You can write your own transformations, they're just Python, they are just standard
Pytorch code. And by default it will only be applied to the training set. If you want
to do something fancy likeRandomResizedCrop where you actually have different things being
applied to each, you should come back to the next course to find out how to do that or
read the documentation. It's not rocket science but it's that's something most people need
to do. Um okay so last time we did bears.new with a RandomResizedCrop, mean scale of 0.5,
we added some transforms and we went ahead and trained. Actually since last week I’ve
rerun this notebook and it's on a different computer and I've got different images so
it's not all exactly the same but I still got a good confusion matrix. Of the black
bears 37 were classified correctly 2 were grizzly's and 1 was a teddy. Now plot top
losses is interesting you can see in this case there's some clearly kind of odd things
going on this is not a bear at all this looks like it's a drawing of a bear. Which it's
decided, is predicted as a Teddy but it's meant to be a drawing of a black bear. I can
certainly see the confusion. You can see how some parts that have been cut off we’ll
talk about how to deal with that later. Now one of the interesting things is that we didn't
really do much data cleaning at all before we built this model the only data cleaning
we did was just to validate that each image can be opened, there was that verify images
call. And the reason for that is it's actually much easier normally to clean your data after
you create a model and I'll show you how. We've got this thing called image classifier
cleaner where you can pick a category right and training set or validation set and then
what it will do is it will then list all of the images in that set and it will pick the
ones which is the least confident about, which is the most likely to be wrong, where the
loss is the worst to be more precise. And so this this is a great way to look through
your data and find problems. So in this case the first one is not a teddy or a brown bear
or a black bear it's a puppy dog, right. So this is a great cleaner because what I can
do is I can now click delete here, this one here looks a bit like an Ewok rather than
a teddy I'm not sure what do you think Rachel is it an Ewok ?
I'm going to call it an Ewok ok and so you can kind of go through okay that's definitely
not a teddy and so you can either say like oh that's wrong it's actually a grizzly bear
or it's wrong it's a black bear or I should delete it or by default is keep it right and
you can kind of keep going through until you think like okay they all seem to be fine maybe
that one's not and kind of once you get to the point where all seems to be fine you can
kind of say okay probably all the rest to fine too because they all have lower losses
so they all fit the kind of the mode of a teddy and so then I can run this code here
where I just go through cleaner.delete so that's all the things which I've selected
delete for and unlink them so unlink is just another way of saying delete a file that's
the Python name and then go through all the ones that we said change and we can actually
move them to the correct directory. If you haven't seen this before you might be surprised
that we've kind of created our own little GUI inside Jupiter notebook. Yeah you can
do this, and we built this with less than a screen of code, you can check out the source
code in the past AI notebooks so this is a great time to remind you that this is a great
time to remind you that fast.ai is built with notebooks and so if you go to the fast.ai
repo and clone it and then go to NBS you'll find all of the code of fast.ai written as
notebooks and they've got a lot of prose and examples and tests and so forth. So the best
place to learn about how this is implemented is to look at the notebooks rather than looking
at the module code. Okay, by the way sometimes you'll see like weird little comments like
this. These weird little comments are part of a development environment for Jupiter notebook
we use called nbdev which we built so Sylvain and I built this thing to make it much easier
for us to kind of create books and websites and libraries in Jupiter notebooks so this
particular one here hide means when this is turned into a book or into documentation don't
show this cell and the reason for that is because you can see I've actually got it in
the text right but I thought when you're actually running it it would be nice to have it sitting
here waiting for you to run directly so that's why it's shown in the notebook but not in
the in the book has shown differently. And you’ll also see things like s: with a quote
in the book that would end up saying Sylvain says and then what he says so there's kind
of little bits and pieces in the notebooks that just look a little bit odd and that's
because it's designed that way in order to show, in order to create stuff in them. Right,
so, then last week we saw how you can export that to a pickle file that contains all the
information from the model, and then on the server where you're going to actually do your
inference, you can then load that save file and you'll get back a learner that you can
call predict on. So predict, perhaps the most interesting part of predict is the third thing
that it returns which is a tensor, in this case containing three numbers. But the three
numbers there's three of them because we have three classes, teddy bear, grizzly bear and
black bear, all right? And so this doesn't make any sense until you know what the order
of the classes is, kind of in your data loaders. And you can ask the data loaders what the
order is by asking for its vocab. So a vocab in fast.ai is a really common concept it's
basically any time that you've got like a mapping from numbers to strings or discrete
levels the mapping is always taught in the vocab so here this shows us that the activation
of black bear is 1-e6, the activation for grizzly is 1 and the activation for teddy
is 1e-6, so very very confident that this particular one it was a grizzly not surprisingly
this was something called grizzly.JPEG
Umm so you need to know this... this mapping in order to display the correct thing, but
of course the data loaders object already knows that mapping, and it's all, the vocab,
and it's stored in with the loader, so that's how it knows to say grizzly automatically.
So the first thing it gives you is the human readable string that you'd want to display.
So this is kind of nice that with fast AI 2 you, you save this object, which has everything
you need for inference. It's got all the, you know, information about normalization,
about any kind of transformation steps, about what the vocab is, so it can display everything
correctly. Right. So now we want to deploy this as an app. Now if you've done some web
programming before then all you need to know is that this line of code, and this line of
code... So this is the line of codes you would call once when your application starts up,
and then this is the line of code you would call every time you want to do inference.
And there's also a batch version of it which you can look up if you're interested this
is just a ‘one at a time’. So there's nothing special if you're already a web programmer
or have access to a web programmer. These are you know... You just have to stick these
two lines of code somewhere and the three things you get back whether, the human readable
string if you're doing categorization, the index of that which in this case is one, is
grizzly, and the probability of each class. One of the things we really wanted to do in
this course though, is not assume that everybody is a web developer. Most data scientists aren't,
but gee wouldn't it be great if all data scientists could at least, like, prototype an application
to show off the thing they're working on. And so we've... Trying to kind of curate an
approach, which none of its stuff we've built, it's really as curated, which shows how you
can create a GUI and create a complete application in Jupyter notebook. So the key pieces of
technology we use to do this are, ipython widgets which is always called iPy widgets,
and Voila. iPy widgets which we import by default as widgets, and that's also what they
use in their own documentation, as GUI widgets. For example a file upload button. So if I
create this file upload button and then display it, I see, and we saw this in the last lesson
as well or maybe lesson one, an actual clickable button. So I can go ahead and click it, and
it says now, OK you've selected one thing. So how do I use that? Well these... Well these
widgets have all kinds of methods and properties. And the upload button has a data property,
which is an array, containing all of the images you uploaded. So you can pass that to PIL
image dot create and so dot create is kind of the standard factory method we use in fast
AI to create items and PIL image dot create is smart enough to be able to create an item
from all kinds of different things and one of the things that it can create it from is
a binary blob, which is what a file upload contains. So then we can display it and there's
our teddy. Right? So you can see how, you know, cells of Jupyter notebook can refer
to other cells that were created, that were... Kind of have GUI created data in them. So
let's hide that teddy away for a moment and the next thing to know about is that there's
a kind of widget called output and an output widget is... It's basically something that
you can fill in later. Right? So if I delete actually this part here. So I've now got an
output widget. Yeah, actually let’s do it this way around. And you can't see the output
widget even though I said please display it, because nothing is output. So then in the
next cell I can say with that output placeholder display a thumbnail of the image and you'll
see that the display will not appear here. It appears back here! Right? Because that's
how... That's where the placeholder was. So let's run that again to clear out that placeholder.
So we can create another kind of placeholder which is a label. The label is kind of a something
where you can put text in it. You can give it a value like, I don't know, please choose
an image.
okay so we've now got a label containing please choose an image. Let's create another button
to do a classification, now this is not a file upload button it's just a general button
so this button doesn't do anything, right, it doesn't do anything until we attach an
event handler to it. An event handler is a callback, we'll be learning all about callbacks
in this course, if you've ever done any GUI programming before or even web programming
you'll be familiar with the idea that you write a function which is the thing you want
to be called when the button is clicked on and then somehow you tell your framework that
this is the on click event. So here I go here's my button run, I say the on click event, the
button run is, we call this code and this code is going to do all the stuff we just
saw. I create an image from the upload, it's going to clear the output, display the image,
call predict and then replace the label with a prediction. There it all is. Now so that
hasn't done anything but I can now go back to this classify button which now has an event
handler attached to it, so watch this: click, boom, and look that's been filled in, thats
been filled in. Right, in case you missed it let's run this again, clear everything
out. Okay everything's gone, this is please choose an image, there's nothing here, I click
classify, bop, bop. Right so it's kind of amazing how our notebook has suddenly turned
into this interactive prototyping playground building applications and so once all this
works we can dump it all together and so the easiest way to dump things together is to
create a V box. So a V box is a vertical box and it's just it's just something that you
put widgets in and so in this case we're going to put the following widgets in, and so in
this case we going to put the following widgets; a label that says “select your bear”,
then an upload button, a run button an output placeholder and a label for predictions. But
let's run these again just to clear everything out so that we're not cheating and let's create
our V box. So as you can see it's just got all the all the pieces, right, we've got...oh
I accidentally ran the thing that displayed the bear, let's get rid of that. Okay so there
it is so now I can click upload, I can choose my bear and then I can click classify and
notice this is exactly, that this is, this is the same buttons as these buttons, they're
like two places we're viewing the same button, which is kind of a wild idea. So if I click
classify it's going to change this label and this label because they're actually both references
to the same label; look there we are. So this is our app right and so this is actually how
I built that image cleaner GUI, is just using these exact things and I built that image
cleaner GUI cell-by-cell in a notebook just like this and so you get this kind of interactive
experimental framework for building a GUI so if you're a data scientist who's never
done GUI stuff before this is a great time to get started because now you can you can
make actual programs. Now of course an actual program running inside a notebook is kind
of cool but what we really want is this program to run in a place anybody can run it that's
where Voila comes in. So Voila and needs to be installed, so you can just run these lines
or install it, it's listed in the prose and what voila does is it takes a notebook and
doesn't display anything except for the markdown, the ipython widgets and the outputs, right,
so all the code cells disappear and it doesn't give the person looking at that page the ability
to run their own code, they can only interact with the widgets, right, so what I did was
a copied and pasted that code from the notebook into a separate notebook which only has those
lines of code, right, so this is just the same lines of code that we saw before
and so this is a notebook, it's just a normal notebook, and then I installed Voila and then
when you do that if you navigate to this notebook but you replace “notebooks” up here with
Voila, it actually displays not the notebook but just as I said the markdown and the widgets.
So here I've got bear classifier and I can click upload, let's do a grizzly bear this
time, and this is a slightly different version I actually made this so there's no classify
button I thought it would be a bit more fancy to make it so when you click upload it just
runs everything, but as you can see there it all is, right, it's all working. So this
is the world's simplest prototype but it's, it's a proof-of-concept right so you can add
widgets with dropdowns and sliders and charts and you know, everything that you can have
in you know, an angular app or a react app or whatever and in fact there's, there's even
stuff which lets you use for example the whole Vue JS framework if you know that, it's a
very popular JavaScript framework, the whole Vue JS framework you can actually use it in
widgets and Voila. So now we want to get it so that this this app can be run by someone
out there in the world. So the voila documentation shows a few ways to do that, but perhaps the
easiest one is to use a system called Binder. So Binder is at mybinder.org and all you do
is you paste in your github repository name here, right, and this is all in the book,
so paste in your Github repo name, you change where it says file, we change that to URL,
you can see and then you put in the path which we were just experimenting with, right. So
you pop that here and then you say launch and what that does is it then gives you a
URL. So then this URL you can pass on to people and this is actually your interactive running
application, so Binder is free and so this is, you know, anybody can now use this to
take their Voila app and make it a publicly available web application. So try it, as it
mentions here the first time you do this Binder takes about five minutes to build your site
because it actually uses something called Docker to deploy the whole FastAI framework
and Python and blah, blah, blah, but once you've done that, that virtual machine will
keep running for, you know, as long as people are using it. It'll keep running for a while,
that virtual machine will keep running for a while as long as people are using it and
you know it's it's reasonably fast. So a few things to note here, being a free service
you won't be surprised to hear this is not using a GPU, its using a CPU, and so that
might be surprising but we're deploying to something which runs on a CPU. When you think
about it though, this makes much more sense to deploy to a CPU than a GPU the, just a
moment, the thing that's happening here is that I am passing along, let's go back to
my app; in my app I'm passing along a single image at a time, so when I pass along that
single image I don't have a huge amount of parallel work for a GPU to do. This is actually
something that a CPU is going to be doing more efficiently so we found that for folks
coming through this course, the vast majority of the time they wanted to deploy inference
on a CPU not a GPU because they're normally this doing one item at a time. It's way cheaper
and easier to deploy to a CPU and the reason for that is that you can just use any hosting
service you like because just remember this is just a, this is just a program at this
point, right, and you can use all the usual horizontal scaling, vertical scaling you know,
you can use Heroku, you can use AWS, you can use inexpensive instances super cheap and
super easy. Having said that there are times you might need to deploy to a GPU for example
maybe you're processing videos and so like a single video on on a CPU to process it might
take all day or you might be so successful that you have a thousand requests per second,
in which case you could like take 128 at a time, batch them together and put the whole
batch on the GPU and get the results back and pass them back around. You gotta be careful
of that right
because if your requests aren't coming fast enough, your user has to wait for a whole
batch of people to be ready to be processed. But you know conceptually, as long as your
site is popular enough that could work. The other thing to talk about is, you might want
to deploy to a mobile phone and the point in to a mobile phone our recommendation is
wherever possible do that by actually deploying to a server and then have a mobile phone talk
to the server over a network. Because if you do that, again you can just use a normal Pytorch
program on a normal server and normal network calls, it makes life super easy. When you
try to run a Pytorch app on a phone, you are suddenly now not in an environment where Pytorch
will run natively and so you'll have to like convert your program into some other form.
And there are other forms and the the main form that you convert it to is something called
ONNX which is specifically designed for kind of super high speed the high performance you
know approach that can run on both servers or on mobile phones and it does not require
the whole Python and Pytorch kind of runtime in place but it's much more complex than not
using it. It's harder to debug, it's harder to set it up, it's harder to maintain it.
So if possible keep things simple, and if you're lucky enough that you're so successful
that you need to scale it up to GPUs or and stuff like that then great, you know, hopefully
you've got the the finances at that point to justify, you know, spending money on an
ONNX expert, or serving expert or whatever. And there are various systems you can use,
ONNX runtime, and AWS Sagemaker where you can kind of say, here's my ONNX bundle and
it’ll serve it for you or whatever. Pytorch also has a mobile framework, same idea. So,
all right, so you've got, I mean it's kind of funny we're talking about two different
kinds of deployment here, one is deploying like a hobby application you know that you're
prototyping, showing off to your friends, explaining to your colleagues how something
might work, you know, a little interactive analysis, that's one thing. But maybe you're
actually prototyping something that you want to turn into a real product, or an actual
real part of your company's operations. When you're deploying, you know, something in real
life, there's all kinds of things you got to be careful of. One example of something
to be careful of is, let's say you did exactly what we just did. Which actually, this is
your homework, is to create your own application and I want you to create your own image search
application you can use my exact set of widgets and whatever if you want to, but better still
go to the ipywidgets website, and see what other widgets they have and try and come up
with something cool try and come and you know try and show off as best as you can and show
us on the forum. Now let's say you decided that you want to create an app that would
help the users of your app decide if they have healthy skin or unhealthy skin. So if
you did the exact thing we just did rather than searching for grizzly bear and teddy
bear and so forth on Bing, you would search for healthy skin and unhealthy skin. And so
here's what happens right, if I, and remember in our version we never actually looked at
being we just used the Bing API the Image Search API but behind the scenes it's just
using the website and so if I click healthy if I type healthy skin and say search, I actually
discover that the definition of healthy skin is young white women touching their face lovingly.
So that's what your your healthy skin classifier would learn to detect, right, and so this
is so this is a great example from Deb Raji and you should check out her paper, “Actionable
Auditing,” for lots of cool insights about model bias. But I mean here's here's like
a fascinating example of how if you weren't looking at your data carefully you you end
up with something that doesn't at all actually solve the problem you want to solve.
This is tricky. Right? Because the data that you train your algorithm on, if you're building
like a new product that didn't exist before, by definition you don't have examples of the
kind of data that's going to be used in real life. Right? So you kind of try to find some,
from somewhere, and if there and if you do that through like a Google search pretty likely
you're not going to end up with a set of data that actually reflects the kind of mix you
would see in real life. So you know the main thing here is to say be careful. Right? And,
and in particular for your test set, you know, that final set that you check on, really try
hard to gather data that reflects the real world. So that goes, you know, for example
for the healthy skin example, you might go and actually talk to a dermatologist and try
and find like ten examples of healthy and unhealthy skin or something. And that would
be your kind of gold standard test. Um. There's all kinds of issues you have to think about
in deployment. I can't cover all of them, I can tell you that this O'Reilly book called
‘Building Machine Learning Powered Applications’ is, is a great resource, and this is one of
the reasons we don't go into detail about AP [corrects], A/B testing and when should
we refresh our data and how we monitor things and so forth, is because that book has already
been written, so we don't want to rewrite it. I do want to mention a particular area
that I care a lot about though, which is, let's take this example, let's say you're
rolling out this bear detection system, and it's going to be attached to video cameras
around a campsite. It's going to warn campers of incoming bears. So if we used the model
that was trained with that data that we just looked at, you know, those are all very nicely
taken pictures of pretty perfect bears. Right? There's really no relationship to the kinds
of pictures you're actually going to have to be dealing with in your, in your campsite
bear detector, which has, it's going to have video and not images, it's going to be nighttime,
there's going to be probably low resolution security cameras, you need to make sure that
the performance of the system is fast enough to tell you about it before the bear kills
you. You know, there will be bears that are partially obscured by bushes or in lots of
shadow or whatever. None of which are the kinds of things you would see normally in
like internet pictures. So what we call this, we call this ‘out of domain data’. ‘Out
of domain data’ refers to a situation where the data that you are trying to do inference
on, is in some way different to the kind of data that you trained with. This is actually...
There's no perfect way to answer this question, and when we look at ethics, we’ll talk about
some really helpful ways to, to minimize how much this happens. For example, it turns out
that having a diverse team is a great way to kind of avoid being surprised by the kinds
of data that people end up coming up with, but really is just something you've got to
be super thoughtful about. Very similar to that is something called the ‘main shift’
and the ‘main shift’ is where maybe you start out with all of your data is ‘in domain
data0’, but over time the kinds of data that you're seeing changes and so over time
maybe raccoons start invading your campsite, and you weren't training on racoons before,
it was just a bear detector, and so that's called ‘domain shift’ and that's another
thing that you have to be very careful of. Rachel, is there a question? No, I was just
gonna add to that in saying that, all data is biased, so there's not kind of a, you know,
a form of de bias data, perfectly representative in all cases data, and that a lot of the proposals
around addressing this have kind of been converging to this idea, and that you see in papers like
Timnit Gebru’s ‘Datasheets for Datasets’ of just writing down a lot of the details
about your data set, and how it was gathered, and in which situations it's appropriate to
use, and how it was maintained, and so there, that's not that, you've totally eliminated
bias but that you're just very aware of the attributes of your data set so that you won't
be blindsided by them later. And there have been, kind of, several proposals in that school
of thought, which I, which I really like, around this idea of just kind of understanding
how your data was gathered and what its limitations are. Thanks Rachel.
So a key problem here is that you can't know the entire behavior of your neural network.
With normal programming you typed in the if statements and the loops and whatever, so
in theory you know what the hell it does. Although, it’s still sometimes surprising.
In this case you, you didn't tell it anything, you just gave it examples ‘alone from’,
and hoped that it learned something useful. There are hundreds of millions of parameters
in all of these neural networks, and so there's no way you can understand how they all combine
with each other to create complex behavior. So really like, there's a natural compromise
here is that we're trying to get sophisticated behavior, so like, like recognizing pictures.
S-+ophisticated enough behavior we can describe it and so the natural downside is you can't
expect the process that the thing is using to do that to be describable. You, for you
to be able to understand it. So our recommendation for kind of dealing with these issues is a
very careful deployment strategy, which I've summarized in this little graph, this little
chart here. The idea would be, first of all whatever it is that you're going to use the
model for, start out by doing it manually. So have a park ranger watching for bears.
Have the model running next to them and each time the park ranger sees a bear they can
check the model and see like, did it seem to have picked it up. So the model is not
doing anything. There's just a person who's like, running it and seeing would it have
made sensible choices, and once you're confident that it makes sense, that what it's doing
seems reasonable, you know, in those as close to the real-life situation as possible, then
deploy it in a time and geography limited way. So pick like one campsite, not the entirety
of California, and do it for, you know, one day and have somebody watching it super carefully.
Right? So now the basic bear detection is being done by the bear detector but there's
still somebody watching it pretty closely, and it's only happening in one campsite, for
one day, and so then as you say like: ‘Okay we haven't destroyed our company yet. Let’s
do two campsites for a week, and then let's do, you know, the entirety of Marin for a
month, and so forth.’ So this is actually what we did when I used to be at this company
called ‘Optimal Decisions’. ‘Optimal Decisions’ was a company that I founded
to do insurance pricing, and if you, if you change insurance prices by, you know, a percent
or two in the wrong direction, in the wrong way, you can basically destroy the whole company.
This has happened many times, you know. Insurers are companies that set prices. That's basically
the product that they provide. So when we deployed new prices for ‘Optimal Decisions’
we always did it by like saying like: ‘Okay we're going to do it for like five minutes
or everybody whose name ends with a D.’ You know? So we kind of try to find some group,
which hopefully would be fairly, you know, it would be different, but not too many of
them, and we would gradually scale it up, and you've got to make sure that when you're
doing this that you have a lot of really good reporting systems in place that you can recognize…
Are your customers yelling at you, are your computers burning up, you know, are your,
are your computers burning up, are your costs spiraling out of control, and so forth. So
it really requires great reporting systems. Does fast AI have methods built-in that provide
for incremental learning, i.e., improving the model slowly over time with a single data
point each time? Yeah, that's a great question. So this is a little bit different, which is
this is really about dealing with ‘domain shift’ and similar issues by continuing
to train your model as you do inference, and so the good news is, you don't need anything
special for that. It's basically just a transfer learning problem. So you can do this in many
different ways. Probably the easiest is just to say, like: ‘Okay, each night...’ Probably
the easiest is just to say: ‘Okay, each night, you know, at midnight we're going to
set off a task, which grabs all of the previous day's transactions, as mini-batches and trains
another epoch.’ And so yeah, that that actually works fine. You can basically think of this
as a fine tuning approach, where your pre-trained model is yesterday's model, and your fine-tuning
data is today's data.
So as you roll out your model, one thing to be thinking about super carefully is that
it might change the behavior of the system that it's a part of. And this can create something
called a ‘feedback loop’ and ‘feedback loops’ are one of the most challenging things
for, for real world model deployment, particularly of machine learning models, because they can
take a very minor issue and explode it into a really big issue. So, for example, think
about a predictive policing algorithm. It's an algorithm that was trained to recognize,
you know, basically trained on data that says whereabouts or arrests being made, and then
as you train that algorithm based on where arrests are being made, then you put in place
a system that sends police officers to places that the model says are likely to have crime,
which in this case where were, were there, where were arrests. Well, then more police
go to that place, find more crime, because the more police that are there the more they'll
see. They arrest more people, causing, you know, and then if you do this incremental
learning, like we're just talking about, then it's going to say: ‘Oh there's actually
even more crime here.’ And so tomorrow it sends even more police. And so in that situation
you end up like, the predictive policing algorithm ends up kind of sending all of your police
on one street block, because at that point all of the arrests are happening there, because
that's the only place you have policemen. Right? And I should say police officers. So
there's actually a paper about this issue called, ‘To predict and serve?’. And in
‘To predict and serve?’ the author's write this really nice phrase: ‘Predictive policing
is aptly named, it is predicting policing, not predicting crime.’ So if the initial
model was perfect, whatever the hell that even means, but like it's somehow sent police
to exactly the best places to find crime, based on the probability of crimes actually
being in place, I guess there's no problem. Right? But as soon as there's any amount of
bias. Right? So for example in the US, there's a lot more arrests of black people than of
white people, even for crimes where black people and white people are known to do them
in the same amount. So in the presence of this bias, or any kind of bias, you're kind
of like setting off this domino chain of ‘feedback loops’, where that bias will be exploded
over time. So, you know, one thing I like to think about is to think like well: ‘What
would happen if this, if this model was just really really really good?’. Like: ‘Who
would be impacted?’ You know: ‘What would this extreme result look like? How would you
know what was really happening?’ This incredibly predictive algorithm that was like changing
the behavior of yours, of your police officers or whatever, you know. ‘What would that
look like? What would actually happen?’ And then like, think about like: ‘Okay,
what could go wrong?’ And then: ‘What kind of rollout plan? What kind of monitoring
systems? What kind of oversight could provide the circuit breaker?’ Because that's what
we really need here. Right? Is, we need like, nothing's going to be perfect, you can't be
sure that there's no ‘feedback loops’, but what you can do is try to be sure that
you see when the behavior of your system is behaving in a way that's not what you want.
Did you have anything to add to that Rachel? I would add to that is that you're at risk
of potentially having a ‘feedback loop’ anytime that your model is kind of controlling
what your next round of data looks like. And I think that's true for pretty much all products,
and that can be a hard jump from people, people coming from kind of a science background,
where you may be thinking of data as: ‘I have just observed some sort of experiment.’
Where is kind of, whenever you're, you know, building something that interacts with the
real world you are now also controlling what your future data looks like based on, kind
of, behavior of your algorithm for the current, current round of data. Right? So… So given
that you probably can't avoid ‘feedback loops’ the, you know, the, the thing you
need to then really invest in is the human in the loop. And so a lot of people like to
focus on automating things which I find weird, you know, if you can decrease the amount of
human involvement by like 90 percent you've got almost all of the economic upside of automating
it completely but you still have the room to put human circuit breakers in place. You
need these appeals processes, you need the monitoring, you need, you know, humans involved
to kind of go: ‘Hey that's, that's weird. I don't think that's what we want.’
Okay, yes Rachel. And just one more note about that. Those humans though do need to be integrated
well with kind of product and engineering, and so one issue that comes up is that in
many companies I think that ends up kind of being underneath trust and safety handles
a lot of sort of issues with, how things can go wrong, or how your platform can be abused,
and often trust and safety is pretty siloed away from product and eng, which actually
kind of has the, the control over, you know, these decisions that really end up influencing
them. And so having... That. They. The engineers probably consider them to be pretty, pretty
annoying a lot of the time, how they get in the way, and get in the way of them getting
software out the door. Yeah, but like the kind of, the more integration you can have
between those I think it's helpful for the kind of the people building the product to
see what is going wrong, and what can go wrong. Right. If the engineers are actually on top
of that, they're actually seeing these, these things happening, that it's not some kind
of abstract problem anymore. So, you know, at this point now that we've got to the end
of chapter 2, you actually know a lot more than most people about, about, deep learning,
and actually about some pretty important foundations of machine learning, more generally, and of
data products more generally. So now’s a great time to think about writing. So, sometimes
we have formatted text that doesn't quite format correctly. In Jupyter notebook by the
way it only formats correctly in, in the book book. So, that's what it means when you see
this kind of pre-formatted text. So... The... The idea here is to think about starting writing,
at this point, before you go too much further. Rachel. There's a question. Oh, okay let's
hear the question. Question is: ‘I am, I assume there are fast AI type ways of keeping
a nightly updated transfer learning setup. Well could there be one of the fast AI version
4 notebooks, have an example of the nightly transfer learning training, like the previous
person asked? I would be interested in knowing how to do that most effectively with fast
AI.’ Sure. So I guess my view is there's nothing fast AI specific about that at all.
So I actually suggest you read Emmanuel’s book. That book I showed you to understand
the kind of the ideas, and if people are interested in this I can also point you with some academic
research about this as well, and there's not as much as that there should be, but there
is some, there is some good work in this area. Okay. So, the reason we mention writing at
this point in our journey is because, you know, things are going to start to get more
and more heavy, more and more complicated, and a really good way to make sure that you're
on top of it is to try to write down what you've learned. So sorry, I wasn’t sharing
the right part of the screen before, but this is what I was describing in terms of the pre-formatted
text, which doesn't look correct. So... When... So, Rachel actually has this great article
that you should check out which is ‘Why you should blog’, and I will say it's sort
of her saying cuz I have it in front of me and she doesn't. Weird as it is. So Rachel
says that: ‘The top advice she would give her younger self is to start blogging sooner.’
So Rachel has a math PhD, and this kind of idea of, like, blogging was not exactly something,
I think, they had a lot of in the PhD program, but actually it's like, it's a really great
way of finding jobs. In fact, most of my students who have got the best jobs are students that
have good blog posts. The thing I really love is that it helps you learn by, by writing
down, it kind of synthesizes your ideas, and yeah, you know, there's lots of reasons to
blog. So there's actually something really cool I want to show you. Yeah. I was also
just gonna note I have a second post called ‘Advice for Better Blog Posts’, that's
a little bit more advanced, which I'll post a link to as well, and that, talks about some
common pitfalls that I've seen in many, in many blog posts, and kind of the importance
of putting, putting the time in to do it well, and and some things to think about. So I'll
share that post as well. Thanks Rachel.
Um, so one reason that sometimes people don't blog is because it's kind of annoying to figure
out how to. Particularly, because I think the thing that a lot of you will want to blog
about is cool stuff that you're building in Jupyter notebooks. So, we've actually teamed
up with a guy called Hamel Husain, and, and with GitHub to create this free product. As
usual with fast AI, no ads, no anything, called ‘fastpages’, where you can actually blog
with Jupyter notebooks. And so you can go to ‘fastpages’ and see for yourself how
to do it, but the basic idea is that, like, you literally click one button, it sets up
a plug for you, and then you dump your notebooks into a folder called underscore notebooks,
and they get turned into blog posts. It's... It's basically like magic, and Hamel's done
this amazing job of this, and so... This means that you can create blog posts where you've
got charts, and tables, and images, you know, where they're all actually the output of,
of Jupyter notebook, along with all the, the markdown formatted text, headings, and so
forth, and hyperlinks, and the whole thing. So this is a great way to start writing about
what you're learning about here. So something that Rachel and I both feel strongly about
when it comes to blogging is this, which is, don't try to think about the absolute most
advanced thing you know and try to write a blog post that would impress Geoff Hinton.
Right? Because most people are not Geoff Hinton. So like, (a) you probably won't do a good
job, because you're trying to, like, blog for somebody who's more, got more expertise
than you, and (b) you've got a small audience now. Right? Actually there's far more people
that are not very familiar with deep learning, than people who are. So try to think, you
know, and, and you really understand what it's like, what it was like six months ago
to be you, because you were there six months ago. So try and write something, which the
six months ago version of you would have been, like, super interesting, full of little tidbits
you would have loved, you know, that you would, that would have delighted you, that six months
ago version of you. Okay. So once again, don't move on until you've had a go at the questionnaire,
to make sure that you, you know, understand the key things we think that you need to understand,
and, yeah, have a think about these further research questions as well, because they might
help you to engage more closely with material. So let's have a break, and we'll come back
in five minutes time. So welcome back everybody. This is an interesting moment in the course,
because we're kind of jumping from a part of the course, which is, you know, very heavily
around kind of the, kind of the, the structure of like what are we trying to do with machine
learning, and what are the kind of the pieces, and what do we need to know to make everything
kind of work together. There was a bit of code, but not masses. There was basically
no math, and we kind of want to put that at the start for everybody who's not, you know,
who's kind of wanting to, an understanding of, of these issues, without necessarily wanting
to, kind of, dive deep into the code, in the math themselves. And now we're getting into
the diving deeper part. If, if you're not interested in that diving deep yourself, you
might want to skip to the next lesson about ethics, where we, you know, is kind of, that
rounds out the kind of, you know, slightly less technical material. So what we're going
to look at here is, we're going to look at what we think of as kind of a toy problem,
but just a few years ago is considered a pretty challenging problem. The problem is recognizing
handwritten digits, and we're going to try and do it from scratch. Right? And we're gonna
try and look at a number of different ways to do it. So, we're going to have a look at
a dataset called MNIST, and so, if you've done any machine learning before you may well
have come across MNIST. It contains handwritten digits and it was collided into a machine
learning data set by a guy called Yann LeCun and some colleagues, and they used that to
demonstrate one of the, you know, probably the first computer system to provide really
practically useful scalable recognition of handwritten digits. LeNet-5 was the system,
was actually used to automatically process like 10% of the checks in the, in the US.
So, one of the things that really helps, I think, when building a new model is to, kind
of, start with something simple, and gradually scale it up. So, we've created an even simpler
version of MNIST, which we call MNIST_SAMPLE, which only has threes and sevens. Okay, so
this is a good starting point to make sure that we can, kind of, do something easy. I
picked threes and sevens for MNIST_SAMPLE, because they're very different. So I feel
like, if we can't do this, we're going to have trouble recognizing every digit. [coughs]
So step one is to call untar_data, untar_data is the fast AI function which takes a URL,
checks whether you've already downloaded it, if you haven't it downloads it, checks whether
you've already uncompressed it, if you haven't, it uncompress is it, and then it finally returns
the path of where that ended up. So you can see here URLs.MNIST_SAMPLE. So you could just
hit tab to get autocomplete. Is just some, some location. Right? Doesn't really matter
where it is, and so then when we... All that, I've already downloaded it, and already uncompressed
it, because I've already run this once before, so it happens straight away, and so path shows
me where it is. Now in this case path is dot, and the reason path is dot is, because I've
used this special base path attribute to path, to tell it kind of like where's my, where's
my starting point, you know, and, and that's used to print so when I go here ls, which
prints a list of files, these are all relative to where I actually untarred this to. So it
just makes it a lot easier not to have to see the whole set of parent path folders.
Um ls is actually... So, so path is a... Let's see what kind of type it is. So, it's a pathlib
path object. Um, pathlib is part of the Python standard library. It's a really very, very,
very nice library, but it doesn't actually have ls. Where there are libraries that we
find super helpful, but they don't have exactly the things we want, we liberally add the things
we want to them. So we add ls. Right? So if you want to find out what ls is, you know,
there's, as we've mentioned it's a few ways you can do it you can pop a question mark
there, and that will show you where it comes from. So there's actually a library called
fastcore, which is a lot of the foundational stuff in fast AI that is not dependent on
PyTorch, or pandas, or any of these big heavy libraries. So, this is part of fastcore and
if you want to see exactly what it does, you, of course remember, you can put in a second
question mark, to get the source code, and as you can see there's not much source code
to it. And, you know, maybe most importantly, please, don't forget about doc, because really
importantly that gives you this ‘Show in docs’ link, which you can click on to get
to the documentation to see examples, pictures, if relevant, tutorials, tests ,and so forth.
So what's, so when you're looking at a new data set, you kind of just used, I always
start with just ls, see what's in it, and I can see here there's a train folder, and
there's a valid folder, that's pretty normal. So let's look at ls on the train folder, and
it's got a folder called 7 and a folder called 3, and so this is looking quite a lot like
our bear classifier dataset. We downloaded each set of images into a folder based on
what its label was. This is doing it at another level though. The first level of the folder
hierarchy is, is it training or valid, and the second level is, what's the label. And
this is the most common way for image datasets to be distributed. So let's have a look. Let's
just create something called 3s, that contains all of the contents of the three directory.
Training. And let's just sort them, so that this is consistent. Do the same for sevens,
and let's look at the 3s and you can see there's just, they’re just numbered. All right.
So let's grab one of those, open it, and take a look. Okay. So, there's the picture of a
3. And so what is that really?
But not 3, im3. So PIL is the Python Imaging Library. It's the most popular library by
far for working with images on Python and it's a PNG, not surprisingly. So Jupyter notebook
knows how to display many different types and you can actually tell if you create a
new type you can tell it how to display your type. And so PIL comes with something that
will automatically display the image, like so. What I want to do here though is to look
at like how we're going to treat this as numbers, right. And so one easy way to treat things
as numbers is to turn it into an array. The array is part of numpy, which is the most
popular array programming library for Python. And so if we pass our PIL image object to
array, it just converts the image into a bunch of numbers. And the truth is, it was a bunch
of numbers the whole time. It was actually stored as a bunch of numbers on disk. It's
just that there's this magic thing in Jupyter that knows how to display those numbers on
the screen. Now let me say, array(), turning it back into a numpy array. We're kind of
removing this ability for Jupyter notebook to know how to display it like a picture.
So once I do this, we can then index into that array and (create everything from the)
grab everything, all the rows from 4 up to but not including 10, and all the columns
from 4 up to and not including 10. And here are some numbers and they are 8-bit unsigned
integers, so they are between 0 and 255. So an image, just like everything on a computer,
is just a bunch of numbers. And therefore, we can compute with it. We could do the same
thing, but instead of saying array(), we could say tensor(). Now our tensor is basically
the PyTorch version of a numpy array. And so you can see it looks, it's exactly the
same code as above, but I've just replaced array() with tensor(). And the output looks
almost exactly the same, except it replaces array with tensor and so you'll see this - that
basically a PyTorch tensor and an numpy array behave nearly identically, much if not most
of the time. But the key thing is that a PyTorch tensor can also be computed on a GPU, not
just a CPU. So in in our work, and in the book, and in the notebooks, in our code, we
tend to use tensors, PyTorch tensors, much more often than numpy arrays because they
kind of have nearly all the benefits of numpy arrays, plus all the benefits of GPU computation.
And they've got a whole lot of extra functionality as well. A lot of people who have used Python
for a long time, always jump into numpy because that's what they used to. If that's you, you
might want to start considering jumping into tensor. Like wherever you used to write array,
just start writing tensor and just see what happens. Because you might be surprised at
how many things you can speed up or do it more easily. So let's grab that that 3 image,
turn it into a tensor and so that's going to be a 3 image tensor - that's why I've got
a im3_t here. And let's grab a bit of it, okay, and turn it into a panda's data frame.
And the only reason I'm turning it into a panda's data frame is that pandas has a very
convenient thing called background_gradient() that turns a background into a gradient, as
you can see. So here is the top bit of the 3. You can see that the 0s are the whites
and the numbers near 255 are the blacks. Okay, and there’s some whatsit bits in the middle
which, which are grey. So here we have, we can see what's going on when our images, which
are numbers, actually get displayed on the screen. It's just it's just doing this, okay,
and so I'm just showing a subset here the actual phone number and MNIST is a 28 by 28
pixels square. So that's 768 pixels. So that's super tiny, right. Well my mobile phone, I
don't know how many megapixels it is, but it's millions of pixels. So it's nice to start
with something simple and small, okay. So, here's our goal - create a model, but by model,
I just mean some kind of computer program learnt from data that can recognize 3s versus
7s. You can think of it as a 3 detector. Is it a 3, because if it's not a 3, it's a 7.
So have a stop here, pause the video and have a think. How would you do it? How would you,
like you don't need to know anything about neural networks, or anything else. How might
you, just with common sense, build a 3 detector, okay?
So I hope you grabbed a piece of paper, a pen, jotted it some notes down. I’ll tell
you the first idea that came into my head was … what if we grab every single 3 in
the data set and take the average of the pixels? So what's the average of this pixel, the average
of this pixel, the average of this pixel, the average of this pixel, right. And so there'll
be a 28 by 28 picture which is the average of all of the 3s, and that would be like the
ideal 3. And then we'll do the same for 7s. And then so when we then grab something from
the validation set to classify, we’ll say, “Like, oh, is this image closer to the ideal
3s, the ideal 3, the mean of the 3s, or the ideal 7? This is my idea and so I'm going
to call this the pixel similarity approach. I'm describing this as a baseline. A baseline
is like a super simple model that should be pretty easy to program from scratch with very
little magic. You know, maybe it's just a bunch of kind of simple averages, simple arithmetic,
which you're super confident is going to be better than, better than a random model, right.
And one of the biggest mistakes I see, in even experienced practitioners, is that they
fail to create a baseline. And so then they build some fancy Bayesian model or, or some
fancy, fancy Bayesian model or some fancy neural network and they go, “Wow, Jeremy
look at my amazingly great model!” And I'll say like, “How do you know it's amazingly
great?” and they’ll say, “oh, look, the accuracy is 80%.” And then I'll say,
“Okay, let's see what happens if we create a model where we always predict the mean.
Oh look, that's 85%.” And people get pretty disheartened when they discover this, right.
And so make sure you start with a reasonable baseline and then gradually build on top of
it. So we need to get the average of the pixels, so we're going to learn some nice Python programming
tricks to do this. So the first thing we need to do is we need a list of all of the 7s.
So remember we've got the 7s - maybe it is just a list of file names, right. And so for
each of those file names in the 7s, lets Image.open() that file just like we did before to get a
PIL object, and let's convert that into a tensor. So this thing here is called a list
comprehension. So if you haven't seen this before, this is one of the most powerful and
useful tools in Python. If you've done something with C#, it's a little bit like link - it's
not as powerful as link, but it's a similar idea. If you've done some functional programming
in in JavaScript, it's a bit like some of the things you can do with that, too. But
basically, we're just going to go through this collection, each item will become called
“o”, and then it will be passed to this function, which opens it up and turns it into
a tensor. And then it will be collated all back into a list. And so this will be all
of the 7s as tensors. So Silva and I use lists and dictionary comprehensions every day. And
so you should definitely spend some time checking it out, if you haven't already. So now that
we've got a list of all of the 3s as tensors, let's just grab one of them and display it.
So remember, this is a tensor, not a PIL image object. Ao Jupyter doesn't know how to display
it. So we have to use something a command to display it - and show_image() is a fast.ai
command that displays a tensor. And so here is 3. So we need to get the average of all
of those 3s. So to get the average, the first thing we need to do is to (turn) change this
so it's not a list, but it's a tensor itself. Currently three_tensors[1] has a shape which
is 28 by 28. Oh this is this is the rows by columns, the size of this thing, right. But
three_tensors itself, it's just a list. But I can't really easily do mathematical computations
on that. So what we could do is we could stack all of these 28 by 28 images on top of each
other to create a, like a 3d cube of images. And that's still quite a tensor. So a tensor
can have as many of these axes or dimensions as you like. And to stack them up you use,
funnily enough, stack(). And so this is going to turn the list into a tensor. And as you
can see the shape of it is now 6131 by 28 by 28. So it's kind of like a cube of height
6131 by 28 by 28.
The other thing we want to do is, if we're going to take the mean we want to turn them
into floating-point values, because we don't want to kind of have integers rounding off.
The other thing to know is that it's just kind of a standard in computer vision that
when you are working with floats, that you expect them to be between 0 and 1. So we just
divide by 255, because they were between 0 and 255 before. So this is a pretty standard
way to kind of represent a bunch of images in PyTorch. So these three things here are
called the axes -- first axis, second axis, third axis, and overall we would say that
this is a rank 3 tensor, as it has three axes. So this one here was a rank two tensor -- just
has two axes. So you can get the rank from a tensor by just taking the length of its
shape: one, two, three. You can also get that from, so the word -- I've been using the word
axis -- you can also use the word dimension. I think numpy tends to call it axis; pytorch
tends to call it dimension. So the rank is also the number of dimensions: ndim. So you
need to make sure that you remember this word. Rank is the number of axes or dimensions in
a tensor, and the shape is a list containing the size of each axis in a tensor.
So we can now say stacked_threes.mean(). Now, if we just say stacked_threes.mean(), that
returns a single number -- that's the average pixel across that whole cube, that whole rank
three tensor. But if we say mean(0), that is: take the mean over this axis, so that's
the mean across the images, right? And so that's now 28 by 28 again, because we kind
of like reduced over this 6131 axis. We took the mean across that axis and so we can show
that image, and here is our ideal three. So here's the ideal seven using the same approach.
All right, so now let's just grab a three -- it's just any old three -- there it is.
And what I'm going to do is I'm going to say, “Well, is this three more similar to the
perfect three, or is it more similar to the perfect seven?” And whichever one it's more
similar to, I'm going to assume that that's the answer. So we can't just say look at each
pixel and say what's the difference between this pixel you know zero zero here, and zero
zero here, and then 0 1 here, and then 0 1 here, and take the average. And the reason
we can't just take the average is that there's positives and negatives, and they're going
to average out to nothing, right, so I actually need them all to be positive numbers.
So there's two ways to make them all positive numbers. I could take the absolute value,
which simply means remove the minus signs, okay? And then I could take the average of
those; that's called the mean absolute difference or L1 norm. Or I could take the square of
each difference and then take the mean of that, and then at the end I could take the
square root, kind of undoes the squaring, and that's called the root mean squared error,
or L2. So let's have a look. Let's take a three and subtract from it the mean of the
threes, and take the absolute value, and take the mean and call that the distance using
absolute value of the three to a_3. And there is the number, .1. And so this is the mean
absolute difference, or L1 norm. So when you see a word like L1 norm, if you haven't seen
it before it may sound pretty fancy, but all these math terms that we see, you know you
can turn them into a tiny bit of code, right? It's, you know, don't let the mathy bits fool
you. They're often -- like in code it's just very obvious what they mean, whereas with
math you just, you just have to learn it, or learn how to google it.
So here’s the same version for squaring: take the difference, square it, take the mean,
and then take the square root. So now we'll do the same thing for our three; this time
we'll compare it to the mean of the sevens. All right, so the distance from a_3 to the
mean of the threes in terms of absolute was .1, and the distance from a_3 to the mean
of the sevens was 0.15. So it's closer to the mean of the threes than it is to the mean
of the sevens, so we guess therefore that this is a three, based on the mean absolute
difference. Same thing with RMSE (root mean squared error) would be to compare this value
with this value, and again root mean squared error is closer to the mean3 than to the mean7.
So this is like a machine learning model (kind of); it’s a data-driven model which attempts
to recognize threes versus sevens, and so this is a good baseline. I mean, it's a reasonable
baseline, it's going to be better than random. We don't actually have to write out “- abs
mean” -- we can just actually use L1 loss. Now, L1 loss does exactly that; we don't have
to write “- squared” -- we can just write mse_loss, and that doesn't do the square root
by default so we have to pop that in. Okay? And as you can see, they're exactly the same
numbers. It's very important before we kind of go too
much further, to make sure we're very comfortable working with arrays and tensors. And you know,
they're so similar. So we could start with a list of lists, right, which is kind of a
matrix. We can convert it into an array, or into a tensor. We can display it, and they
look almost the same. You can index into a single row, you can index into a single column,
and so it's important to know -- this is very important -- colon means every row, because
I put it in the first spot. Right, so if it were in the second spot it would mean every
column and so therefore comma colon ( ,: ) is exactly the same as removing it. So it just
turns out you can always remove colons that are at the end, because they're kind of, they're
just implied, right? You never have to, and I often kind of put them in anyway, because
just kind of makes it a bit more obvious how these things kind of match up, or how they
differ. You can combine them together so give me the first row and everything from the first
up to but not including the third column -- back to this at 5, 6. You can add stuff to them;
you can check their type. Notice that this is different to the Python type, right, so
type is a function; this tells you it's a tensor. If you want to know what kind of tensor,
you have to use type as a method. So it's a long tensor. You can multiply them by a
float, turns it into a float. You know so have a fiddle around if you haven't done much
stuff with numpy or PyTorch before, this is a good opportunity to just go crazy -- try
things out. Try things that you think might not work and see if you actually get an error
message, you know. So we now want to find out how good is our
model? Our model that involves just comparing something to to the mean. So we should not
compare… you should not check how good our model is on the training set. As we've discussed,
we should check it on a validation set, and we already have a validation set: it's everything
inside the valid directory. So let's go ahead and like combine all those steps before. Let's
go through everything in the validation set 3.ls(). Open them, turn them into a tensor,
stack them all up, turn them into floats, divide by 255.
Okay, let's do the same for sevens. So we're just putting all the steps we did before into
a couple of lines.
I always try to print out shapes, like all the time, because if a shape is not what you
expected then you can, you know, get weird things going on. So the idea is we want some
function is_three that will return true if we think something is a three. So to do that
we have to decide whether our digit that we're testing on is closer to the ideal three or
the ideal seven. So let's create a little function that returns the difference between
two things, takes the absolute value and then takes the mean. So we're going to create this
function mnist_distance that takes the difference between two tensors, takes their absolute
value, and then takes the mean. And it takes the mean, and look at this, we got minus this
time, it takes the mean over the last -- over the second last and third last -- sorry the
last and second last dimensions. So this is going to take the mean across the kind of
x and y axes. And so here you can see it's returning a single number, which is the distance
of a three from the mean3. So that's the same as the value that we got earlier: .1114. So
we need to do this for every image in the validation set because we're trying to find
the overall metric. Remember: the metric is the thing we look at to say how good is our
model. So here's something crazy: we can call mnist_distance not just on a three, but on
the entire validation set, against the mean three. That's wild! Like, there's no normal
programming that we would do where we could somehow pass in either a matrix or a rank
3 tensor and somehow it works both times. And what actually happened here is that instead
of returning a single number it returned 1,010 numbers. And it did this because it used something
called broadcasting. And broadcasting is like the super special magic trick that lets you
make Python into a very very high-performance language, and in fact, if you do this broadcasting
on GPU tensors and PyTorch, it actually does this operation on the GPU even though you
wrote it in Python. Here's what happens. Look here this a - b. So we’re doing a-b on two
things. We've got first of all valid_3_tens, so valid three tensor is a thousand or so
images, right, and remember that mean3 is just our single ideal three. So what is something
of this shape minus something of this shape? Well, broadcasting means that if this shape
doesn't match this shape, like if they did match it would just subtract every corresponding
item, but because they don't match, it actually acts as if there's a thousand and ten versions
of this. So it's actually going to subtract this from every single one of these okay.
So broadcasting -- let's look at some examples. So broadcasting requires us to first of all
to understand the idea of element-wise operations. This is an element-wise operation. Here is
a rank 1 tensor of size 3 and another rank 1 tensor of size 3, so we would say these
sizes match (they're the same) and so when I add 1, 2, 3, to 1, 1, 1 I get back 2 3 4.
It just takes the corresponding items and adds them together. That's called element-wise
operations. So when I have different shapes, as we described before, what it ends up doing
is it basically copies this number a thousand and ten times, and it acts as if we had said
valid_3_tens minus 1,010 copies of mean3. As it says here it doesn't actually copy mean3
1,010 times; it just pretends that it did, right? It just acts as if it did, so basically
kind of loops back around to the start again and again and it does the whole thing in C
or in CUDA on the GPU. So then we see absolute value, right? So let's go back up here after
we do the minus, we go absolute value so what happens when we call absolute value on something
of size 1010 by 28 by 28? It just calls absolute value on each underlying thing right and then
finally we call mean. -1 is the last element always in Python, -2 is the second-last.
So this is taking the mean over the last two axes, and so then it's going to return just
the first axis. So we're going to end up with 1,010 means -- 1,010 distances, which is exactly
what we want: we want to know how far away is our each of our validation items from the
the ideal three. So then we can create our is_3 function, which is, “Hey, is the distance
between the number in question and the perfect three less than the distance between the number
in question and the perfect seven?” If it is, it's a three, right? So our three, that
was an actual three we had: is it a three? Yes. Okay, and then we can turn that into
a float, and “yes” becomes 1.0. Thanks to broadcasting, we can do it for that entire
set, right? So this is so cool! We basically get rid of loops. In this kind of programming,
you should have very few, very very few loops. Loops make things much harder to read, and
hundreds of thousands of times slower (on the GPU potentially tens of millions of times
slower). So we can just say is_3 on our whole valid_3_tens and then turn that into float,
and then take the mean; so that's going to be the accuracy of the threes on average.
And here's the accuracy of the sevens -- it's just one minus that -- and so the accuracy
across threes is about 91 and a bit percent. The accuracy on sevens is about 98%, and the
average of those two is about 95%. So here we have a model that's 95 percent accurate
at recognizing threes from sevens. It might surprise you that we can do that using nothing
but arithmetic, right, but so that's what I mean by getting a good baseline.
Now the thing is, it's not obvious how we kind of improve this, right? I mean the thing
is, it doesn't match Arthur Samuel’s description of machine learning. This is not something
where there's a function which has some parameters which we're testing against some kind of measure
of fitness, and then using that to like improve the parameters iteratively. We kind of, we
just did one step and that's that, okay? So we will try and do it in this way where
we arrange for some automatic means of testing the effectiveness of -- he called it a weight
assignment, we'd call it a parameter assignment -- in terms of performance, and a mechanism
for altering the weight assignment to maximize the performance. But we won’t do it that
way, right, because we know from Chapter 1, from Lesson 1, that if we do it that way,
we have this like magic box called machine learning that can do -- you know, particularly
combined with neural nets -- should be able to solve any problem, in theory, if you can
at least find the right set of weights. So we need something that we can get better and
better, to learn. So let's think about a function which has parameters. So instead of finding
an ideal image and seeing how far away something is from the ideal image, so instead of like
having something where we test how far away we are from an ideal image, what we could
instead do is come up with a set of weights for each pixel. So we're trying to find out
if something is the number three, and so we know that like in the places that you would
expect to find ‘3’ pixels, you could give those like high weights. So you can say,”Hey,
if there's a dot in those places, we give it like a high score and if there's dots in
other places we'll give it like a low score. So we can actually come up with a function
where the probability of something being, well in this case let's say an eight, is equal
to the pixels in the image multiplied by some sort of weights, and then we sum them up,
right, so then anywhere where our -- the image we're looking at, you know, has pixels where
there are high weights, it's going to end up with a high probability. So here x is the
image that we're interested in, and we're just going to represent it as a vector, so
let's just have all the rows stacked up, end to end into a single long line. So we're going
to use an approach where we're going to start with a vector W. So a vector is a Rank 1 tensor,
okay? We’re going to start with a vector W that's going to contain random weights,
random parameters, depending on whether you use the Arthur Samuel version of the terminology
or not.
And so, we'll then predict whether a number appears to be a three or a seven by using
this tiny little function. And then we will figure out how good the model is. Where we
will calculate like, how accurate it is or something like that. Yeah this is the loss,
and then the key step is we're then going to calculate the gradient. Now the gradient
is something that measures for each weight if I made it a little bit bigger will the
loss get better or worse. If I made it a little bit smaller will the loss get better or worse?
And so if we do that for every weight we can decide for every weight whether we should
make that weight a bit bigger or a bit smaller. That’s called the gradient. Right? So once
we have the gradient we then step, is the word we use is step. Change all the weights,
up a little bit for the ones where the gradient we should, said, we should make them a bit
higher, and down a little bit for all the ones where the gradient said they should be
a bit lower. So now it should be a tiny bit better and then we go back to step two and
calculate a new set of predictions, using this formula, calculate the gradient again,
step the weights, keep doing that. So this is basically the flow chart and then at some
point when we're sick of waiting or when the loss gets good enough we'll stop. So these
seven steps 1, 2, 3, 4, 5, 6, 7… These seven steps are the key to training all deep learning
models. This technique is called stochastic gradient descent. Well, it's called gradient
descent, we’ll see the stochastic bit very soon. And for each of these seven steps there's
lots of choices around exactly how to do it. Right? We've just kind of hand waved a lot,
like what kind of random initialization, and how do you calculate the gradient, and exactly
what step do you take based on the gradient, and how do you decide when to stop, blah blah
blah. Right? So in this... In this course we're going to be like learning about, you
know, these steps, you know, that's kind of part one, you know. I then the other big part
is like, well what's the actual function, neural network. So how do we train the thing
and what is the thing that we train. So, we initialize parameters with random values.
We need some function that's going to be the loss function that will return a number that's
small if the performance of the model is good. We need some way to figure out whether the
weight should be increased a bit or decreased a bit, and then we need to decide like when
to stop, which will just say let's just do a certain number of epochs. So, let's like,
go even simpler. Right? We're not even going to do MNIST. We're going to start with this
function x squared, okay? And in fast AI we've created a tiny little thing called plot function,
that plots a function. All right, so there’s our function f, and what we're going to do
is we're going to try to find this is our loss function. So we're going to try and find
the bottom point. Right? So we're going to try and figure out what is the x value, which
is at the bottom. So our seven step procedure requires us to start out by initializing,
so we need to pick some value. Right? So the value we pick, which is to say: ‘oh let's
just randomly pick minus one and a half.’ Great! So now we need to know, if I increase
x a bit, does my, but remember this is my loss does my loss get a bit better, remember
better is smaller, or a bit worse. So we can do that easily enough. We can just try a slightly
higher x and a slightly lower x and see what happens. Right? And you can see it's just
the slope. Right? The slope at this point tells you that if I increase x by a bit then
my loss will decrease, because that is the slope at this point. So, if we change our,
our weight, our parameter, just a little bit in the direction of the slope. Right? So here
is the direction of the slope and so here's the new value at that point, and then do it
again, and then do it again, eventually we'll get to the bottom of this curve. Right?
So this idea goes all the way back to Isaac Newton, at the very least, and this basic
idea is called Newton's method. So a key thing we need to be able to do is to calculate this
slope. And the bad news is to do that we need calculus. At least that’s bad news for me
because I've never been a fan of calculus. We have to calculate the derivative. Here's
the good news, though. Maybe you spent ages in school learning how to calculate derivatives
- you don't have to anymore, the computer does it for you, and the computer does it
fast. It uses all of those methods that you learned at school and a whole lot more - like
clever tricks for speeding them up, and it just does it all automatically. So, for example,
it knows (I don't know if you remember this from high school) that the derivative of x
squared is 2x. It’s just something it knows, it's part of its kind of bag of tricks, right.
So, so PyTorch knows that. PyTorch has an engine built in that can take derivatives
and find the gradient of functions. So to do that we start with a tensor, let's say,
and in this case we're going to modify this tensor with this special method called requires_grad.
And what this does is it tells PyTorch that any time I do a calculation with this xt,
it should remember what calculation it does so that I can take the derivative later. You
see the underscore at the end? An underscore at the end of a method in PyTorch means that
this is called an in-place operation it actually modifies this. So, requires_grad_ modifies
this tensor to tell PyTtorch that we want to be calculating gradients on it. So that
means it's just going to have to keep track of all of the computations we do so that it
can calculate the derivative later. Okay, so we've got the number 3 and let's say we
then call f on it (remember f is just squaring it, so 3 squared is 9. But the value is not
just 9, it's 9 accompanied with a grad function which is that it knows that a power operation
has been taken. So we can now call a special method, backward(). And backward(), which
refers to backpropagation, which we'll learn about, which basically means take the derivative.
And so once it does that we can now look inside xt, which we said requires grad, and find
out its gradient. And remember, the derivative of x squared is 2x. In this case that was
3, 2 times 3 is 6. All right, so we didn't have to figure out the derivative we just
call backward(), and then get the grad attribute to get the derivative. so that's how easy
it is to do calculus in PyTorch. So what you need to know about calculus is not how to
take a derivative, but what it means. And what it means is it's a slope at some point.
Now here's something interesting - let's not just take3, let's take a Rank 1 tensor also
known as a vector [3., 4., 10.] and let's add sum to our f function. So it's going to
go x squared .sum. and now we can take f of this vector, get back 125. And then we can
say backward() and grad and look - 2x 2x 2x. So we can calculate, this is, this is vector
calculus, right. We're getting the gradient for every element of a vector with the same
two lines of code. So that's kind of all you need to know about calculus, right. And if
this is, if this idea that, that a derivative or gradient is a slope is unfamiliar, check
out Khan Academy. They had some great introductory calculus. And don't forget you can skip all
the bits here they teach you how to calculate the gradients yourself. So now that we know
how to calculate the gradient, that is the slope of the function, that tells us if we
change our input a little bit, how will our output change correspondingly. That's what
a slope is, right. And so that tells us that for every one of our parameters, if we know
their gradients, then we know if we change that parameter up a bit or down a bit, how
will it change our loss. So therefore, we then know how to change our parameters. So
what we do is, let's say all of our weights are called “w”, we just subtract off them
the gradients multiplied by some small number and that small number is often a number between
about 0.001 and 0.1 and it's called the learning rate and this here is the essence of gradient
descent
So if you pick a learning rate that's very small, then you take the slope and you take
a really small step in that direction, and another small step, another small step, another
small step, and so on, it's going to take forever to get to the end. If you pick a learning
rate that's too big, you jump way too far each time and again, it's going to take forever.
And in fact in this case, sorry this case we're assuming we're starting here and it's
actually is so big it got worse and worse. Or here's one where we start here and it's
like it's not so big it gets worse and worse, but it just takes a long time to bounce in
and out right. So picking a good learning rate is really important, both to making sure
that it's even possible to solve the problem and that it's possible to solve it in a reasonable
amount of time. So we'll be learning about picking, how to pick learning rates in this
course. So let's try this, let's try using gradient
descent. I said SGD, that's not quite accurate, it's just going to be gradient descent to
solve an actual problem. So the problem we're going to solve is, let's imagine you were
watching a roller coaster go over the top of a hump, right. So as it comes out of the
previous hill it's going super fast and it's going up the hill and it's going slower and
slower and slower until it gets to the top of the hump, and then it goes down the other
side, it gets faster and faster and faster. So if you like how to stopwatch or whatever
or some kind of speedometer and you are measuring it just by hand at kind of equal time points
you might end up with something that looks a bit like this, right. And so the way I did
this was I just grabbed a range, just grabbed the numbers from naught up to, but not including
20, right. These are the time periods at which I'm taking my speed measurement. And then
I've just got some quadratic function here - I multiplied by 3 and then square it and
then add 1, whatever, right. And then I also, actually sorry. I take my time - 9.5 square
it, times .75, and add 1. And then I add a random number to that or add a random number
to every observation. So I end up with a quadratic function which is a bit bumpy. So this is
kind of like what it might look like in real life because my speedometer kind of testing
is not perfect. All right, so we want to create a function that estimates at any time what
is the speed of the roller-coaster. So we start by guessing what function it might be.
So we guess that it's a function - a times time squared, plus b times time, plus c - you
might remember from school is called a quadratic. So let's create a function, right. And so
let's create it using kind of the Alpha Samuels technique, the machine learning technique.
This function is going to take two things - it's going to take an input, which in this
case is a time, and it's going to take some parameters. And the parameters are a, b, and
c. So in Python you can split out a list or a collection into its components, like so.
And then here's that function. So we’re not just trying to find any function in the
world, we're just trying to find some function which is a quadratic by finding an a, and
a b, and a c. So the Arthur Samuel technique for doing this is to next up come up with
a loss function; come up with a measurement of how good we are. So if we've got some predictions
that come out of our function and the targets which are these, you know, actual values,
then we could just do the mean squared error. Okay, so here's that means squared error we
saw before - the difference squared, then take the mean. So now we need to go through
our seven step process, we want to come up with a set of three parameters a, b and c,
which are as good as possible. So step one is to initialize a, b, and c to random values.
So this is how you get random values, three of them in PyTorch. And remember we're going
to be adjusting them, so we have to tell PyTorch that we want the gradients. I'm just going
to save those away so I can check them later. And then I calculate the predictions using
that function, f, which was this. And then let's create a little function which just
plots how good at this point are our predictions. So here is a function that prints in red our
predictions, and in blue our targets. So that looks pretty terrible.
So let’s calculate the loss, using the mse function we wrote. Okay, so now we want to
improve this. So calculate the gradients using the two steps we saw, call backward and then
get grad. And this says that each of our parameters has a gradient that's negative. Let's pick
a learning rate of ten to the minus five, or we multiply that by ten to the minus five,
and step the weights, And remember step the weights means minus equals the learning rate
times the gradient. There’s a wonderful trick here, which I’ve called .data. The
reason I've called .data is that .data is a special attribute in PyTorch, which if you
use it, then the gradient is not calculated. And we certainly wouldn't want the gradient
to be calculated of the actual step we're doing. We only want the gradient to be calculated
of our function, f. All right, so when we step the weights we have to use this special
.data attribute. After we do that, delete the gradients that we already had and let's
see if loss improved. So the loss before was 25800, now it's 5,400. And the plot has gone
from something that goes down to -300 to something that looks much better. So let's do that a
few times. So I just grabbed those previous lines of code and pasted them all into a single
cell. Okay so preds, loss.backward, data grad = none. And then from time-to-time print the
loss out, and repeat that ten times. And look getting better and better. And so we can actually
look at it getting better and better. So this is pretty cool, right. We have a technique,
this is the Arthur Samuel technique for finding a set of parameters that continuously improves
by getting feedback from the result of measuring some loss function. So that was kind of the
key step, right. This, this is the gradient descent method. So you should make sure that
you kind of go back and feel super comfortable with what's happened. And you know, if you're
not feeling comfortable, that that's fine, right. If it's been a while, or if you've
never done this kind of gradient descent before, this might feel super unfamiliar. So kind
of try to find the first cell in this notebook where you don't fully understand what it's
doing, and then stop and figure it out. Look at everything that's going on, do some experiments,
do some reading until you understand that cell where you're stuck before you move forwards.
So let's now apply this to MNIST. So for MNIST we want to use this exact technique and there's
basically nothing extra we have to do. Except one thing - we need a loss function. And the
metric that we've been using is the error rate, or the accuracy. It's like how often
are we correct, right. And and that's the thing that we're actually trying to make good,
our metric. But we've got a very serious problem - which is, remember we need to calculate
the gradient to figure out how we should change our parameters. And the gradient is the slope
or the steepness, which you might remember from school is defined as rise over run. It's
(y_new - y_old) divided by (x_new - x_old). So the gradients actually defined when x_new
is is very very close to x_old, meaning their difference is very small. That, think about
it - accuracy. If I change a parameter by a tiny tiny tiny amount, the accuracy might
not change at all because there might not be any 3 that we now predict as a 7 or any
7 that we now predict as a 3, because we change the parameter by such a small amount. So it's
it's it's possible, in facr, it's certain, that the gradient is zero at many places and
that means that our parameters aren't going to change at all. Because learning rate times
gradient is still zero when the gradient’s zero for any learning rate. So this is why
the loss function and the metric are not always the same thing. We can't use a metric as our
loss if that metric has a gradient of zero. So we need something different. So, we want
to find something that kind of is pretty similar to the accuracy in that like as the accuracy
gets better
this ideal function we want gets better as well but it should not have a gradient of
zero. So let's think about that function. Suppose we had three images. Actually, you
know what? This is actually probably a good time to stop. Because actually, you know,
we've we've kind of, we've got to the point here where we understand gradient descent.
We kind of know how to do it with a simple loss function and I actually think before
we start looking at the MNIST loss function, we shouldn't move on. Because we've got so much so much
assignments to do for this week already. So, we've got built your web application, and
we've got both step-through-step through this notebook to make sure you fully understand
it. So I actually think we should probably stop right here before we make things too
crazy. So before I do, Rachel, are there any questions? Okay great, all right. Well thanks
everybody. Sorry for that last-minute change of tack there but I think this is going to
make sense. So I hope you have a lot of fun with your web applications. Try and think
of something that's really fun, really interesting. It doesn't have to be like, important. It
could just be some you know cute thing. We've had students before, a student that I think
he said he had 16 different cousins, and he created something that would classify a photo
based on which of his cousins... It was for, like his fiancee meeting his family. [laughs]
You know you can come up with anything you like, but you know, yeah, show off your application
and maybe have a look around at what ipywidgets can do, and try and come up with something
that you think is pretty cool. All right, thanks everybody. I will see you next week!
Welcome back and here is lesson 4 which is where we get deep into the weeds of exactly
what is going on when we are training a neural network and we started looking at this in
the previous lesson. We were looking at the stochastic gradient descent and so to remind
you, we were looking at what Arthur Samuel said. “Suppose we arrange for some automatic
means of testing the effectiveness of any current weight assignment ( or we would call
it parameter) in terms of actual performance and provide a mechanism for altering the weight
assignment so as to maximize that performance. So we could make that entirely automatic and
a machine so programmed would learn from its experience” and that was it. So our initial
attempt on the MNIST data set was not really based on that. We didn't really have any parameters.
So then , last week we tried to figure out how we could parameterize it, how we could
create a function that had parameters. And, what we thought we could do would be to have
something where that say the probability of being some particular number was expressed
in terms of the pixels of that number and some weights, and then we would just multiply
them together and add them up. So we looked at how stochastic gradient descent works last
week and the basic idea is that we start out by initializing the parameters randomly. We
use them to make a prediction using a function, such as this one. We then see how good that
prediction is, by measuring using a loss function, we then calculate the gradient which is how
much with the loss change if I change one parameter by a little bit, we then use that
to make a small step to change each of the parameters by a little bit, and by multiplying
the learning rate by the gradient to get a new set of predictions and so we went round
and round and round a few times until eventually we decided to stop and so these are the basic
seven steps. Then we went through and so we did that for simple quadratic equation, and
we had something which looked like this and so by the end we had this nice curve getting
closer and closer and closer. So I have a little summary at the start of this section
summarizing gradient descent that Silvyan and I have in the notebooks, in the book,
of what we just did, so you can review that, and make sure it makes sense to you. So now
let's use this to create our MNIST “3” vs. “7” model and so to create a model,
we're going to need to create something that we can pass into a function like, let's see
where it was, passing to a function like this one. So we need just some pixels that are
all lined up and some parameters that are all lined up, and then we're going to sum
them up. So our axis are going to be pixels and so in this case because we're just going
to multiply each pixel by a parameter and add them up, the effect that they're laid
out in a grid is not important so let's reshape those grids and turn them into vectors. The
way we reshape things in Pytorch is by using the “.view” method. The view method you
can pass to it how large you want each dimension to be and so in this case we want the number
of columns to be equal to the total number of pixels in each picture., which is 28 x
28. because they're 28 by 28 images. And then the number of rows will be however many rows
there are in the data, and so if you just use minus one when you call view, that means,
you know, as many as there are in the data, so this will create something of the same,
with the same total number of elements that we had before. So we can grab all our 3 we
can concatenate them with torch.cat with all of our 7 and then reshape that into a Matrix
where each row is one Image with all of the rows and columns of the image all lined up
in a single vector. Then we're going to need labels, so that's our X, so we're going to
need labels, our labels will be a 1 for each of the 3s and a 0 for each of the 7s so basically,
we're creating “is 3 model”. So that's going to create a vector, we actually need
it to be a matrix in Pytorch so .unsqueeze will add an additional unit dimension to wherever
I've asked for so here in position one, so in other words, this is going to turn up from
something which is a vector of 12396 long into a matrix with 12396 rows and one column.
That's just what Pytorch expects to see. So now we're going to turn our X, Y into a Dataset
and a Dataset is a very specific concept in Pytorch. It's something which we can index
into, using square brackets, and when we do so, it's expected to return a tuple. So here
if we look at how we're going to create this Dataset and when we index into it, it's going
to return a tuple containing our independent variable and a dependent variable, for each
particular row, and so to do that we can use the Python zip function, which takes one element
of the first thing and combines it with concatenates it with one element of the second thing and
then it does that again and again and again and so then if we create a list of those it
gives us a, It is as a Dataset!. It gives us a list which when we index into it, it's
going to contain one image and one label, and so here you can see why there's my label
and my image I won't print out the whole thing, but it's a 784 long vector. So that's a really
important concept, a Dataset is something that you can index into, and get back at Tuple.
And here I am, this is called destructuring the tuple: which means I'm taking the two
parts of the tuple and putting the first part in one variable in the second part in the
other variable, which is something we do a lot in Python, it's pretty handy, a lot of
other languages support that as well. Repeat the same three steps for a validation set.
So we've now got a training Dataset and a validation Dataset. Right! So now we need
to initialize our parameters and so to do that, as we've discussed, we just do it randomly.
So here's a function. They're given some size, some, some shape if you like. We'll randomly
initialize, using a normal random number distribution in PyTorch that's what .Randn does and we
can hit shift tab to see how that works, okay!? And it says here that it's going to have a
variance of one. So, I probably should NOT call this standard deviation, I probably should
have called this: variance actually. So multiply it by the variance - to change its variance
to whatever is requested, which will default to one. And then as we talked about, when
it comes to calculating our gradients we have to tell PyTorch which things we want gradients
for and the way we do that is requires_grad_ . Remember this underscore at the end is a
special magic symbol which tells PyTorch that we want this function to actually change the
thing that it's referring to. This will change this tensor, Such that it requires radiance.
So here's some weights, so our weights are going to need to be 28 by 28 by 1 shape 28
by 28 because every pixel is going to need a weight and then 1 because we're going to
need it again , so we're going to need to have that unit access to make it into a column.
So that's what PyTorch expects. So there's our weights. Now just weights by pixels actually
isn't going to be enough because weights by pixels were always equal zero. When the pixels
are equal to zero, it has a zero intercept. So we really want something where it's like
W * X + B, a line. So the B is we call the bias and So that's just going to be a single
number. So let's grab a single number for our bias. So, remember I told you, there's
a difference between the parameters and weights. So, actually speaking, so here the weights
are the W in this equation, the bias is B in this equation, and the weights and bias
together is the parameters of the function they're all the things that we're going to
change, they're all the things that have gradients that we're going to update. So there's an
important bit of jargon for you: the weights and biases of the model are the parameters.
We can... yes question! R: “What's the difference between gradient descent and stochastic gradient
descent? J: So far we've only done gradient descent and will be doing stochastic gradient
descent in a few minutes. We can now create and calculate predictions for one image so
we can take an image such as the first one and multiply by the weights only to transpose
them to make them line up in terms of the rows and columns and add it up, and add the
bias and there is a prediction. We want to do that for every image we could do that with
a for loop and that would be really really slow. It wouldn't run on the GPU and it wouldn't
run in optimize and see code. So we actually want to use always to do kind of like looping
over pixels, looping over images. You always need to try to make sure you're doing that
without a Python for loop, in this case doing this calculation for lots of rows and columns
is a mathematical operation called matrix multiply, so if you've forgotten your matrix
multiplication or maybe never quite got around to it at a high school. It would be a good
idea to have a look at Khan Academy or something to learn about what it is, but it's actually
I'll give you the quick answer. This is from Wikipedia, if these are two matrices A and
B then this element here, 1, 2 in the output is going to be equal to the first bit here
times the first bit here, plus the second bit here, times the second bit here. So it's
going to be A1,2 * A1,1 + B 2,2 * A 1,2 that's you can see the orange matches the orange.
Ditto for over here. This would be equal to B1,3 * A 3,1 + B2,3 * A 3,2 and so forth for
every. Here's a great picture of that in action if you look at matrixmultiplication.XYZ another
way to think of it is we can kind of flip the second bit over on top and then multiply
each bit together and add them up, multiplied each bit together and add them up and you
can see always the second one here and ends up in the second spot and the first one ends
up in the first spot. And that's what matrix multiplication is. So we can do our, multiply
and Add them up by using matrix multiplication and in Python and therefore PyTorch matrix
multiplication is the @ sign operator. So when you see @, that means matrix multiply
so here is our 20.2336 if I do a matrix multiply of our training set by our weights and then
we add the bias and here is our 20.2336 for the first one. And you can see through. It's
doing every single one, okay!?. So that's really important is that matrix multiplication
gives us an optimized way to do these simple linear functions, whereas we want as many
kinds of rows and columns as we want. So this is one of the two fundamental equations of
any neural network. Some rows, of data rows and columns of data much use multiply some
weights add some bias and the second one which was here in a moment is an activation function.
So that is some predictions from our randomly initialized model, so we can check how good
our model is and so to do that we can decide that anything greater than 0 we will call
a 3 and anything less than 0 we will call a 7. So preds greater than 0.0 tells us whether
or not something is predicted to be a 3 or not. Then turn that into a float, so rather
than true and false, make it one in zero because it's what our training set contains and then
check with our thresholded predictions are equal to our training set and this will return
true every time a row is correctly predicted and false otherwise. So if we take all those
trues and falses and turn them into floats, that'll be ones and zeroes and then take their
mean: It's 0.49, so not surprisingly our randomly initialized model is right about half the
time at predicting threes from sevens. I Added one more method here, which is .item() without
item This would return a tensor, It's a rank zero tensor. It has no rows. It has no columns
it just it's just a number on its own, but I actually wanted to unwrap it to create a
normal Python scalar mainly just because I wanted to see them easily see the full set
of decimal places and the reason for that is I want to show you how we're going to calculate
the derivative on the accuracy. By changing a parameter a tiny bit, so let's take one
parameter which will be weights[0] and Multiply it by 1.0001 and so that's going to make it
a little bit bigger and then if I calculate how the the accuracy changes based on the
change in that weight that will be the gradient of the accuracy with respect to that parameter
so I can do that by calculating my new set of predictions and then I can threshold them
and then I can check whether they're equal to the training set and then take the meanAnd
I get back: exactly the same number so remember that Gradient is equal to Rise over run if
you remember back to your calculus or if you'd forgotten your calculus. Hopefully you've
reviewed it on Khan Academy so The change in the Y so y_new - y_old which is 0.4912
etc minus 0.4912 etc, which is 0 divided by This change will give us 0 so at this point
we have a problem our derivative is 0 so we have 0 gradients: Which means our step will
be zero which means our prediction will be unchanged.Okay So we have a problem and our
problem is that our gradient is zero and with a gradient ofZero we can't take a step and
we can't get better predictions. And so Intuitively speaking the reason that our gradient is zero
is because when we change a single pixel by a tiny bit we might not ever in any way change
an actual prediction to change from a three predicting a three to a seven or Vice versa
because we have this we have this threshold. And so in other words our Our accuracy Loss
function here is very bumpy. It's like a flat step flat step flat step. So it's got this
Zero gradient all over the place. So what we need to do is use something other than
accuracy as our loss function So, let's try and create a new function and what this new
function is going to do is it's going to Give us a better value. Kind of in much the same
way that Accuracy gives a better value. So this is the loss member of small losses better
so to give us a lower loss when the accuracy is better, but it won't have a zero gradient.
It means that a slightly better prediction needs to have a slightly better lossSo, let's
have a look at an example. Let's say our Targets, our labels of like that are three. Oh There's
just three rows three images here one zero one, okayAnd we've made some predictions from
a neural net and those predictions gave us a point. [0.9, 0.4, 0.2] so now consider:
This loss function a loss function: we're going to use torch.where() which is basically
the same as This list comprehension it’s basically an if statement so it's going to
say for for where target equals one We're going to return 1 minus predictions. So here
target is one so it'll be 1 minus 0.9 and Where target is not 1 it'll just be predictions
so well these Examples here. The first one target equals 1 will be 1 - 0.9 = 0.1.The
next one is target equals 0 so to speak the prediction is just 0.4 And then for the third
one, it's a 1 for target So it'll be 1 - prediction, which is 0.8. And so you can see here when
the Prediction is correct. Correct. In other words, it's a number, you know It's a high
number when the target is 1 and a low number when the target is 0, these numbers are going
to be smaller. So the worst one is when we predicted 0.2. So we're pretty we really thought
that was actually a zero But it's actually a 1 so we ended up with a 0.8 here because
this is 1 minus prediction 1 - 0.2 = 0.8, So we can then take the mean of all of these
to calculate a loss. So if you think about it this loss will be the smallest if The predictions
are exactly right. So if we did predictions is actually identical to the targets this
will be [0., 0., 0.] okay, where else if they were exactly wrong say they were one then
it's [1., 1., 1.]. So it's going to be the loss will be better ie smaller when the predictions
are closer to the targets. And so here we can now take the mean and when we do we get
here 0.433. Let's say we change this last bad one in accurate prediction from 0.2 to
0.8 and the loss gets better from 0.43 to 0.23 but this is just this function is torch.where().
So this is actually pretty good. This is actually a loss function which pretty closely tracks
accuracy was the accuracies better, The loss will be smaller But also it doesn't have these
zero gradients because every time we change the prediction the loss changes Because the
prediction is literally harder the loss that's pretty neat, isn't it? One problem is this
is only going to work. Well, as long as the predictions are between 0 & 1 Otherwise, this
one - prediction thing is going to look a bit funny. We should try and find a way to
ensure that the predictions are always between zero and one and that's also going to just
make a lot more intuitive sense because you know we like to be able to kind of think of
these as if they're like probabilities or at least nicely scaled numbers so we need
some function that can take our numbers: Have a look, something which can take these big
numbers and turn them all into numbers between zero and one and it so happens that we have
exactly the right function. It's called the sigmoid functions of the sigmoid function
looks like this if you pass in a really small number you get a number very close to zeroIf
you pass in a big number you get a number very close to 1 it never gets past one and
it never goes smaller than zero and then it's kind of like this smooth curve between and
in the middle It looks a lot like the y = x line. This is the definition of the sigmoid
function It's 1 over 1 + e to minus x What is exp? exp is just e to the power of something
so if we look at e: It's just a number like pi so simply, it's just a number that has
a particular value right? So if we go e Squared and we look at It's going to be a tensor,
use PyTorch, make it a float:There we go and you can see that these are the same number
so that's what torch.exp means. Okay, so you know for me when I see these kinds of interesting
functions, I don't worry too much about The definition what I care about is the shape
alright So you can have a play around with graphing calculators or whatever to kind of
see Why it is that you end up with this shape from this particular equation but for me,
I just never think about that it never Really matters to me what's important is this sigmoid
shape, which is what we want. It's something that squashes every number to be between naught
and 1 So we can change em nest loss to be exactly the same as it was before but first
we can make everything into sigmoid First and then use torch,where() so that is a loss
function that has all the properties we want. It'sAt something which is going to be have
not have any of those nasty Zero gradients and we've ensured that the input to the where()
between naught and one SoThe reason we did this is because our our accuracy Was Kind
of what we really care about is a good accuracy. We can't use it to get our gradients. Just
just to create our steps to improve our parameters So we can change Our our accuracy to another
function that is similar in terms of it It's better when the accuracy is better, but it
also does not have these zero gradients And so you can see now where why we have a metric
and a loss the metric is the thing we actually care about the loss is the thing that's similar
to what we care about but has a nicely behaved gradient. Sometimes the thing you care about
your metric does have a nicely defined gradient and you can use it directly as a lossFor example,
we often use means squared error but for classification unfortunately notSo we need to now use this
toTo update the parameters And so there's a couple of ways we could do this one would
be to loop through every image, Calculate a prediction for that image and then calculate
a loss and then do a step andThen step the other parameters and then do that again for
the next image in the next image in the next image. That's going to be really slow. Because
we're we're doing a single step for a single image. So that would mean an epoch would take
quite a while. We could go much faster, By doing every single image in the data set so
a big matrix multiplication It can all be paralyzed on the GPU and then so then we can
We could then do a step based on the gradients looking at the entire dataset but now that's
going to be like a lot of work to just update the weights once remember sometimes our datasets
have Millions or tens of millions of items. So that's probably a bad idea too. So why
not compromise? Let's grab a few data items at a timeTo calculate our loss and our step
now if we grab a few data items at a time those two data items are called a mini batch
and a mini batch just means a few pieces of dataAnd so the size of your mini batch is
called not surprisingly the batch size, right? so the bigger the batch size the closer you
get to the full size of your data set the longer it's going to take to Calculate a singleSet
of losses a single step But the more accurate it's going to be, it's going to be like, the
gradients are going to be much closer to the true data set gradients. And then the smaller
the batch size the faster each step we'll be able to do, but those steps will represent
a smaller number of items and so they won't be such an accurate approximation of the real
gradient of the whole dataset. Is there a reason the mean of the loss is calculated
over, say, doing a median, since the median is less prone to getting influenced by outliers?
In the example you gave, if the third point which was wrongly predicted as an outlier,
then the derivative would push the function away while doing SGD, and a median could be
better in that case. Honestly, I've never tried using a median. The problem with a median
is, it ends up really only caring about one number, which is the number in the middle.
So it could end up really pretty much ignoring all of the things at each end, and all it
really cares about is the order of things. So my guess is that you would end up with
something that is only good at predicting one thing in the middle. But I haven't tried
it. It’d be interesting to see. Well, I guess the other thing that would happen with
a median is, you would have a lot of zero gradients, I think. Because it's picking the
thing in the middle, and you could, you know change your values, and the thing in the middle,
well wouldn't be zero gradients, but bumpy gradients. I think in the middle would suddenly
jump to being a different item. So it might not behave very well. That's my guess. You
should try it. Okay, so how do we ask for a few items at a time? It turns out that Pytorch
and Fastai provide something to do that for you. You can pass in any dataset to this class
called DataLoader and it will grab a few items from that dataset at a time. You can ask for
how many by asking for a batch size, and then, as you can see, it will grab a few items at
a time until it's grabbed all of them. So here, I'm saying let's create a collection
that just contains all the numbers from nought to 14. Let's pass that into a DataLoader with
a batch size of 5, And then, that's going to be something called an iterator, in Python.
It's something that you can ask for one more thing from an iterator. If you pass an iterator
to list in Python, it returns all of the things from the iterator. So here are my three mini-batches,
and you'll see here all the numbers from nought to 15 appear. They appear in a random order,
and they appear five at a time. They appear in random order, because shuffle = True. So
normally in the training set we ask for things to be shuffled, so it gives us a little bit
more randomization. More randomization is good, because it makes it harder for it to
learn what the dataset looks like. So that's how our DataLoader is created. Now, remember
though, that our datasets actually return tuples, and here I've just got single ints.
So let's actually create a tuple. So if we enumerate all the letters of English, then
that means that returns (0, ‘a’), (1, ‘b’), (2, ’c’) etc. Let's make that
our dataset. So, if we pass that, to a DataLoader with a batch size of 6, and as you can see,
it returns tuples containing 6 of the first things, and the associated 6 of the second
things. So this is like our independent variable and this is like our dependent variable. Okay,
and so at the end, you know that with the batch size weren't necessarily exactly divided
nicely into the full size of the Dataset, you might end up with a smaller batch. So
basically then, we already have a Dataset remember, and so we could pass it to a DataLoader
and then we can basically say this. An iterator in Python is something that you can actually
loop through. So when we say for in DataLoader, it's going to return a tuple. We can de-structure
it into the first bit and the second bit, and so that's going to be our x and y. We
can calculate our predictions, we can calculate our loss from the predictions and the targets,
we can ask it to calculate our gradients and then we can update our parameters just like
we did in our toy SGD example for the quadratic equation. So that's re-initialize our weights
and bias with the same two lines of code before, let's create the data loader this time from
our actual MNIST dataset and create a nice big batch size, so we do plenty of work each
time, and just to take a look. Let's just grab the first thing from the ‘DataLoader’.
‘first’ is a fast AI function, which just grabs the first thing from an iterator. It's
just, it’s useful to look at, you know, kind of an arbitrary mini batch. So here is
the shape we're going to have. The first mini batch is 256 rows of 784 long, that's 28 by
28. So 256 flattened out images, and 256 labels that are 1. Well, because there's just the
number 0 or the number 1, depending on whether as a 3 or a 7. We do the same for the validation
set. So here's a validation DataLoader… And so let's grab a batch here, testing, pass
it into, well, why do we do that? We should… What… Look… Yeah, I guess, yeah, actually
for our testing, I'm going to just manually grab the first four things just so that we
can make sure everything lines up. So... So let's grab just the first four things. We'll
call that a batch. Pass it into that linear function, we created earlier. Remember linear,
was just, x batch at weights matrix multiply, plus bias. And so that's going to give us
four results. That's a prediction of each of those four images. And so then we can calculate
the loss, using that loss function we just used, and let's just grab the first four items
of the training set, and there's the loss. Okay. And so now we can calculate the gradients,
and so the gradients are 784 by 1, so in other words it's a column where every weight as
a gradient. It's what's the change in loss for a small change in that parameter, and
then the bias as a gradient it’s a single number, because the bias is just a single
number. So we can take those three steps and put it in a function. If you pass... If you...
This is ‘calculate gradient’. You pass it an X batch, a Y batch, and some model,
then it's going to calculate the predictions, calculate the loss, and do the backward step.
And here we see ‘calculate gradient’, and so we can get the, just to take a look,
the mean of the weights gradient, and the bias gradient. And there it is. If I call
it a second time, and look. Notice I have not done any step here. This is exactly the
same parameters. I get a different value. That’s a concern. You would expect to get
the same gradient every time you called it with the same data. Why have the gradients
changed? That's because, ‘loss.backward’ does not just calculate the gradients. It
calculates the gradients, and adds them to the existing gradients. The things in the
‘.grad’ attribute. The reasons for that we'll come to later, but for now the thing
to know is just it does that. So actually what we need to do is to call ‘grad.dot.zero_’.
So ‘dot.zero’ returns a tensor containing zeros, and remember ‘_’ does it in place
so that updates the ‘weights.grad’ attribute, which is a tensor, to contain zeros. So now
if I do that, and call it again, I will get exactly the same number. So here is how you
train one epoch with SGD. Loop through the DataLoader, grabbing the X batch, and the
Y batch, calculate the gradient, prediction, loss backward. Go through each of the parameters.
We're going to be passing those in. So there's going to be the 768 weights, and the one bias,
and then, for each of those, update the parameter, to go minus equals gradient times learning
rate. That's our Gradient Descent step. And then zero it out for the next time around
the loop. I'm not just saying p minus equals. I'm saying ‘p.data’ minus equals, and
the reason for that is that, remember, PyTorch keeps track of all of the calculations we
do, so that it can calculate the gradient. Well, I don't want to calculate the gradient
of my Gradient Descent step. That's like not part of the model, right? So dot data is a
special attribute in Pytorch, where if you write to it, it tells Pytorch not to update
the gradients using that calculation. So this is your most basic, standard SGD, stochastic
gradient descent loop. So now we can answer that earlier question. The difference between
stochastic gradient descent and gradient descent, is that gradient descent does not have this
here that loops through each mini-batch. For gradient descent it does it on the whole dataset
each time around. So train epoch or gradient descent, would simply not have the for loop
at all, but instead it would calculate the gradient for the whole dataset and update
the parameters based on the whole dataset, which we never really do in practice. We always
use mini-batches of various sizes. Okay, so we can take the function we had before where
we compare the predictions to whether that, well that, we used to be comparing the predictions
to whether they were greater or less than zero, right? But now that we're doing the
sigmoid, remember the sigmoid will squish everything between naught and one. So now
we should compare the predictions to whether they're greater than 0.5 or not. If they're
greater than 0.5 ,just to look back at our sigmoid function. So zero, what used to be
zero, is now, on the sigmoid is 0.5. Okay, so we need to, just to make that slight change
to our measure of accuracy. So to calculate the accuracy for some X-batch and some Y-batch,
oh this is actually assumed this is actually the predictionsThen we take the sigmoid of
the predictions. We compare them to 0.5 to tell us whether it's a 3 or not, we check
what the actual target was, to see which ones are correct, and then we take the mean of
those, after converting the booleans to floats. So we can check that accuracy. Let's take
our batch, put it through our simple linear model, compare it to the four items of the
training set, and there's the accuracy. So if we do that for every batch in the validation
set, then we can loop through with a list comprehension, every batch in the validation
set, get the accuracy based on some model, stack those all up together, so that this
is a list, right? So, if we want to turn that list into a tensor, where the items of the
list, of the tensor, are the items of the list. That's what stack does. So we can stack
up all those, take the mean, convert it to a standard Python scalar, we're calling that
item, round it to four decimal places just for display, and so here is our validation
set accuracy. As you would expect, it's about 50% because it's random. So we can now train
for one epoch. So we can say, remember “train_epoch” needed the parameters? So, our parameters
in this case are the weights tensor and the bias tensor. So train one epoch using the
“linear1” model with the learner, with a learning rate of one, with these two parameters,
and then validate, and look at that! Our accuracy is now 68.8%. So we've trained an epoch. So
let's just repeat that many times. Train and validate, and you can see the accuracy goes
up and up and up and up and up to about 97%. So that’s coole! Ae've built an SGD optimizer
of a simple linear function that is getting about 97% on our simplified MNIST where there's
just the threes in the sevens. So a lot of steps there, let's simplify this through some
refactoring. So the kind of simple refactoring we're going to do, we're going to do a couple,
but the basic idea is, we're going to create something called an optimizer class. The first
thing we'll do is, we'll get rid of the “linear1” function. But remember the “linear1” function
does ‘x’ ‘@’ ‘w’ plus ‘b’. There's actually a class in Pytorch that does
that equation for us, so we may as well use it. It's called nn.linear, and nn.linear does
two things, it does that function for us, and it also initializes the parameters for
us, so we don't have to do weights and bias init_params anymore. We just create an nn.linear
class and that's going to create a matrix of size (28, 28, 1) and a bias of size 1.
It will set requires_grad=True for us. It's all going to be encapsulated in this class,
and then when I call that as a function, it's going to do my X hat W + B. So to see the
parameters in it, we would expect it to contain 784 weights and 1 bias, we can just call that
parameters and we can destructure it to w, b and see, yep! It is 784 and 1 for the weights
and bias. So that's cool. So this is just, you could, you know, it could be an interesting
exercise for you to create this class yourself, from scratch. You should be able to, at this
point. So that you can confirm that you can recreate something that behaves exactly like
nn.linear. So, now that we've got this object which contains our parameters in a parameters
method, we could now create an optimizer. So if your optimizer we're going to pass it
the parameters to optimize and a learning rate, we’ll store them away and we'll have
something called step which goes through each parameter and does that thing we just saw:
p.data -= p.grad times learning rate. And it's also going to have something called zero
grad, which goes through each parameter and zeroes it out, or we could even just set it
to None. So that's the thing we're going to call Basic Optimizer. So those are exactly
the same lines of code we've already seen wrapped up into a class. So we can now create
an optimizer, passing in the parameters of the linear model, these, and our learning
rate, and so now our training loop is: look through each mini batch in the data loader,
calculate the gradient, opt.step, opt.zero_grad, that's it! Validation function doesn't have
to change, and so let’s put our training loop into a function, that's going to loop
through a bunch of epochs, call an epoch, print validate_epoch and then run it. And
it's the same! We're getting a slightly different result here, but it’s much the same idea.
Okay, so that's cool, right, we've now refactoring using, you know, create our own optimizer
and using PyTorch built-in nn.linear class. And you know, by the way, we don't actually
need to use our own BasicOptim. Not surprisingly, PyTorch comes with something which does exactly
this, and not surprisingly it's called SGD. So, and actually this SGD is provided by fastai:
fastai and PyTorch provide some overlapping functionality. Then it works much the same
way, so you can pass to SGD your parameters and your learning rate, just like BasicOptim,
okay? And train it, and get the same result. So, as you can see, these classes that are
in fastai and PyTorch, are not mysterious, they're just pretty, you know, thin wrappers
around functionality that we've now written ourselves. So there's quite a few steps there,
and if you haven't done gradient descent before, then there's a lot of unpacking. So, this
lesson is kind of the key lesson. It's the one where, you know, like we should, you know,
really take us, stop and a deep breath at this point, and make sure you're comfortable.
What's the data set? What's the data loader? What's nn.linear? What's SGD? And if, you
know, if one, any or all of those don't make sense, go back to where we defined it from
scratch using Python code. Well the data loader we didn't define from scratch, but it, you
know, the functionality is not particularly interesting. You can certainly create your
own from scratch if you wanted to--that would be another pretty good exercise! Let's refactor
some more. fastai has a ‘dataloaders’ class, which as we've mentioned before is
a tiny class, that just you pass it a bunch of dataloaders and it just stores them away
as a .train and a .valid. Even though it's a tiny class, it's super handy, because with
that we now have a single object that knows all the data we have: and so it can make sure
that your training dataloader is shuffled and your validation loader isn't shuffled,
you know, make sure everything works properly. So that's what the dataloaders class is: you
can pass in the training and valid dataloader. And then the next thing we have in fastai
is the learner class. And the learner class is something where we're going to pass in
our dataloaders, we're going to pass in our model, we're going to pass in our optimization
function, we're going to pass in our loss function, we're going to pass in our metrics.
So all the stuff we've just done manually--that's all learner does! It's just going to do that
for us. So it's just going to call this train_model and this train_epoch. It's just you know,
it's inside learner. So now if we go learn.fit(), you can see again, it's doing the same thing,
getting the same result. And it's got some nice functionality. It's printing it out into
a pretty table for us, and it's showing us the losses and the accuracy and how long it
takes. But there's nothing magic, right? You've been able to do exactly the same thing by
hand using Python and PyTorch. Okay, so these abstractions are here to let you write less
code and to save some time and to save some cognitive overhead, but they're not doing
anything you can't do yourself. And that's important, right? Because if they're doing
things you can't do yourself, then you can't customize them, you can't debug them, you
know, you can't profile them. So we want to make sure that the stuff we're using is stuff
that we understand what it's doing. So this is just a linear function, it’s not great:
we want a neural network. So, how do we turn this into a neural network? Remember this
is a linear function x@w+B. To turn it into a neural network, we have two linear functions,
exactly the same but with different weights and different biases and in between, this
magic line of code, which takes the result of our first linear function and then does
a max() between that and 0. So a max() of res and 0 is going to take any negative numbers
and turn them into 0’s. So we're going to do a linear function, we're going to replace
the negatives with 0 and then we're going to take that and put it through another linear
function. That (believe it or not) is a neural net! So, w1 and w2 were weight tensors b1
and b2 are bias tensors (just like before) so we can initialize them (just like before)
and we can now call exactly the same training code that we did before to roll these. So
res.max(0) is called a rectified linear unit. Which you will always see referred to as ReLU.
In PyTorch it already has this function--it's called f.relu(). And so if we plot it you
can see it's as you'd expect, it's 0 for all negative numbers and then it's y=x for positive
numbers. Here's some jargon “rectified linear unit” sounds scary, sounds complicated,
but it's actually this incredibly tiny line of code, this incredibly simple function.
And this happens a lot in deep learning. Things that sound complicated and sophisticated and
impressive turnout to be normally super simple, frankly... At least once, you know what it
is... So: Why do we do: Linear layer ReLu Linear Layer: Well if we got rid of the middle
If we got rid of the middle ReLu and just went linear layer linear layer then you could
rewrite that as a single linear layer when your multiply things and add and then multiply
things and add andYou can just change the coefficients and make it into a single multiply
and then addSo no matter how many linear layers we stack on top of each other we can never
make anything moreAnd of effective than a simple linear modelBut if you put a non-linearity
between the linear layersThen actually you have the opposite. This is now where something
called the universal approximation theorem holds which is that if the size of the weight
and bias matrices are big enoughThis can actually approximate any arbitrary function including
the function of how do I recognize threes from sevens or Or whateverSo that's kind of
amazing, right this tiny thing is actually a universal function approximator as long
as you have W1 B1 W2 and B2Have the right numbers and we know how to make them the right
numbers we use SGDCould take a very long time. It could take a lot of memoryBut the basic
idea is that there is some solution to any computable problem andThis is one of the biggest
challengesA lot of beginners have to deep learning is that there's nothing else to it
like that? There's often this likeOkay, how do I make a neural net?Oh, that is the neural
net. Well, how do I, do deep learning training with SGDthere's things to like Make a train
a bit faster. There's you know things to mean you need a few less parameters but everything
from here is justPerformance tweaks honestly, rightSo this is you know, this is the key
understanding of of training a neural networkOkay, we can simplify things a bit more We already
know that we can use nn.linear to replaceTheir weight and bias, so let's do that for both
of the linear layers, and then since we're simply takingThe result of one function and
passing it into the nextThe result of that function passive to the next and so forth
and then returned the end this is called function composition function composition is when you
justTake the result of one function pass it to a new one take a result of one function.
Pass it to a new one and so every pretty much neural network is just doing function composition
of linear layers and these are called activation functions or nonlinearities So PyTorch provides
something to do function composition for us and it's called nn.sequential so it's gonna
do a linear layer pass the result to a ReLu you pass the result to a linear layerYou'll
see here. I'm not using F.ReLU. I'm using nn.ReLU This is identical returns exactly
the same thing, but this is a classRather than a function. Yes, Rachel “By using the
non-linearity Won't using a function that makes all negative output zero make many of
the gradients in the network zero and stop the learning process dueto many zero gradients?”
Well, that's a fantastic question and the answer is yes, it doesBut they won't be zero
for every image and remember the mini-batches a shuffled soEven if it's zero for every image
in one mini batch, it won't be for the next mini batchAnd it won't be the next time around
we go for another epoch. So Yes, it can create zeros and ifThe neural net ends up with a
set of parametersThat's that lots and lots of inputs end up as zeros. You can end up
with whole mini batches that are zero andYou can end up in a situation where some of the
neurons remain In active inactive means their zero and they're basically dead units And
this is a huge problemIt basically means you're wasting computationSo there's a few tricks
to avoid that which we'll be learning about a lot one. Simple trick is toNot make this
thing flat here, but just make it a less steepMine that's called a leakyReLU. Well, you leaky
rectified linear unit andIt that they helped a bitAs well learn though even better is to
make sure that we just kind of initialize to sensible initial values that are not too
big and not too small and step by sensible initial values that are particularly not too
big and generally if we do that we can keep things in the zone where they're positive
most of the time but we are going to learn about how to actually analyze inside a network
and find out how many dead units we have how many of these zeros we have becauseAs is as
you point out they are they are bad news. They don't do any work and they will Continue
to not do any work if enough of the inputs end up being zeroOkay, so now that we've got
a neural netWe can use exactly the same learner we had before but this time we pass in the
simple netInstead of the linear one. Everything else is the same and we can call fit just
like before andGenerally as your models get deeper. So here we've gone from one layer-twoAnd
I'm only counting the parameterised layers as layers. You could say it's three. I was
going to call it two. There's twoTrainable layers. So I've gone from one layer to I've
checked dropped my learning rate from 1 to 0.1because the deeper models all tend to be
kind of bumpier less nicely behaved so often you need to use lower learning ratesAnd so
we trained it for awhile okay, andCan actually find out what that training looks like by
looking inside our learner and there's an attribute we create for recorder andThat's
going to recordWell everything that appears in this tableBasically, well these three things
the training loss the validation loss and the accuracy or any metricsso recorded values
contains that kind of table of results and so item number two ofEach row will be the
accuracy and so the capital L class, which I'm using here as a nice little method called
itemgot that will will getThe second item from every row and then I can plot thatHow
the training went and I can get the final accuracyBy grabbing the last row of the table
and grabbing the second It's indexed to 0 1 2 then my final accuracy. Not bad98.3%So
this is pretty amazing, we now have a function that can solve any problemTo any level of
accuracy if we can find the right parameters and we have a way to findHopefully the best
or at least a very good that our parameters for any functionSo this is kind of the magic
yes, RachelHow could we use what we're learning here to get an idea of what the network is
learning along the wayLike Zieler and Fergus did more or lessWe will look at that laterNot
in the full detail of their paper but basically you can look in the dot parameters to see
the values of those parametersand at this point. Well, I mean, why don't you try it
yourself? Right? You've actually got nowThe parameters, so if you want to grab the model
you can actually see the learned.modelSo we can we can look inside learn.model to see
the actual model that we just trained and you can see it's got the three things in it.
They're linear then ReLU than linear, and you know, what I kind of like to do is to
put that into a variable, make it a Bit easy to work with, you can grab one layer by indexing
in parameters and that just gives me something called a generator. It's something that will
give me a list of the parameters when I ask for them. I could just go weight comma bias
equals to de-structure them and so the weight Here's 30 by 784: because that's what I asked
for. So one of the things to note here is that to create a Neural Net so something that's
more than one layer. I actually have 30 outputs not just one right so I'm kind of generating
lot so if you can think of as generating lots of featuresSo it's kind of like 30 different
linear of linear models here and then I combined those 30 back into one. So you could look
at one of those by having a look at here, so there's the numbers in the first row, we
could reshape that into the the original shape of the images
and we could even have a look and there it is right? So you can see this is something
So this cool right we can actually see here we've got something which isWhich is kind
of learning to find things at the top and the bottom and the middle And so we could
look at the second one. Okay, no idea what that's showing and so some of them are kind
of you know, I probably got far more than I need which is why they're not that obvious.
But you can see yeah, here's another thing that's looking pretty similar kind of looking
for this little bit in the middle, so yeah, this is the basic idea to understand the features
that are not the first layer but later layers, you have to be a bit more sophisticated but
yeah to see the first layer ones you can you can just plot themOkay, so then, you know
just to compare we could use the full fastai toolkit so grab our data loaders by using
data loaders from folder as we've done before and create a cnn_ learner and a ResNet and
fit it for a single epoch and, WOAH, 99.7! All right, so we did 40 epochs and got 98.3
as I said using all the tricks you can really speed things up and make things a lot better
and so by the end of this course or at least both parts of this course, you'll be able
to from scratch at this 99.7 in a single epoch, all right, so Jargon! So jargon: just remind
us ReLU function that returns zero for negatives mini-batch a few inputs and labels, which
optionally are randomly selected the forward pass is the bit where we calculate the predictions
the loss is the function that we're going to take the derivative of and then the gradient
is the derivative of the loss with respect to each parameter the backward pass is when
we calculate those gradients gradient descent is that full thing of taking a step in the
direction opposite to the gradients by capital after calculating the loss andThen the learning
rate is the size of the step that we take Other things to know, perhaps the two most
important pieces of jargon are all of the numbers that are in a neural network the numbers
that we're learning are called parameters and then the numbers that we're calculating
so every value that's calculated every matrix multiplication element that's calculated:
They're called activations so activations and parameters are all of the numbers in the
neural net and so be very careful when I say from here on in in these lessons activations
or parameters. You've got to make sure you know what those mean because that's that's
the entire basically almost the entire set of numbers that exist inside a neural net
so activations are calculated, Parameters are learned. We're doing this stuff with Tensors
and Tensors are just regularly shaped to arrays, rank zero tensors, we call scalars, rank 1
tensor:. we call vectors rank two tensors we call matrices and we continue on to rank
3 tensors rank 4 tensors and so forth and rank five tensors are very common in deep
learning. So don't be scared of going up to higher numbers of dimensions. Okay, so let's
have a break oh we got a question, okay R: “Is there a rule of thumb for what non-linearity
to choose given that there are many?” Yeah, there are many non-linearities to choose from
and it doesn't generally matter very much which you choose so just use ReLU or Leaky
ReLU or yeah, whatever any anyone should work fine later on we'll we'll look at the minor
differences between between them but it's not so much something that you pick on a per
problem it's more like some take a little bit longer and a little bit more accurate
and some over it faster and a little bit less accurate. That's a good question, okay. So
before you move on it's really important that you finish the questionnaire for this chapter
because there's a whole lot of concepts that we've just done so, you know try to go through
the questionnaire go back and relook at the notebook and please run the code through the
cat experiments and make sure it makes senseAll right. Let's have a seven minute break see
you back here in seven minutes time. Okay, welcome back, so now that we know how to create
and train a Neural Net. Let's cycle back and look deeper at some applications. And so we're
going to try to kind of interpolate in from one end we've done they're kind of from scratch
version at the other end we've done the kind of four lines of code version and we're going
to gradually nibble at each end until we find ourselves in the middle and we've we've we've
touched on all of itso let's go back up to the kind of the four lines of code version
and and delve a little deeper. So, let's go back to PETs and let's think though about
like: How do you actually, start with a new dataset and figure out how to use it so, you
know the the data sets we provide it's easy enough to untar them you to say untar that
will download it and untar it. If it's a data set that you're getting you can just use the
terminal or [?]a Python or whatever, so, let's assume we have a path that's pointing at something
so initially you don't you don't know what that something is, so we can start by doing
LS to have a look and see what's inside there. So the PETs data set that we saw in Lesson
one contains three things annotations images and models and you'll see we have this little
trick here where we say path.BASE_ path equals and then the path to our data and that just
does a little simple thin:. Where when we print it out, it just doesn't show us. It
just shows us relative to this path, which is a bit convenient. So, go and have a look
at the readme for the original PETs dataset, it tells you what these images and annotations
folders are and not surprisingly the images path, so if we go path slash images, that's
how we use PathLib to grab the sub directory and then LS we can see here are the names
that the paths through the images. As it mentions here most functions and methods in fastai
which returned a collection don't return a Python list that they returned a capital L
and a capital L as we briefly mentioned is basically an enhanced list. One of the enhancements
is the way it prints the representation of it starts by showing you. How many items there
are in the list in the collection: so there's 7349 images and, It it if there's more than
ten things it truncates it and just says dot dot to avoid filling up your screen, so there's
a couple of little conveniences there, and so we can see from this output that a file
name as we mentioned in lesson 1 if the first letter is a capital it means it's a Cat and
if the first letter is lowercase it means it's a dog, but this time we've got to do
something a bit more complex a lot more complex which is figure out what breed it is and so
you can see the breed is kind of everything up to after the in the file name: It's everything
up to the the last underscore and before this number is the breed. So we want to label everything
with its breed, so we're going to take advantage of this structure, so the way I would do this
is to use a regular expression. A regular expression is something that looks at a string
and basically lets you kind of pull it apart into its pieces in very flexible way. It is
kind of simple little language for doing that. Um, if you haven't used regular expressions
before um, please Google regular expression tutorial now and look it's going to be like
one of the most useful tools you'll come across in your life. I use them almost every day
.I will go to details about how to use them since there's so many great tutorials. And
there's also a lot of great like exercises, you know, there's regex regex is short for
regular expression. There's regex crosswords, There's reges Q&A all kinds of core regex
things a lot of people like me love this tool in order to, there's also a regex lesson in
the fastAI NLP course, maybe even to regex lessons. Oh, yeah, I'm sorry for forgetting
about the first day. I know because, what an excellent resource that is! So, RegularExpressions
are how to get right the first time. So the best thing to do is to get a sample string.
So good - good way to do that would be to just grab one of the file names. So let's
pop it in Fname and then you can experiment with reg expressions. So re is the regular
expression module in Python and find all will just grab all the parts of a regular expression
that have parentheses around them. So this regular expression and R is a special kind
of string in Python which basically says don't treat backslash as special because normally
in Python like backslash n means a newline. So here's us a string, which I'm going to
capture. Any letter one or more times followed by an underscore followed by a digit one or
more times, followed by anything I probably don’t have to use backslash t for this but
that’s fine followed by the letters jpg followed by the end of the string and so if
I call that regular expression against my file names name, Oh! Looks good, right so
we kind of check it out! So, now that seems to work we can create a data block where the
independent variables are images the dependent variables are categories just like before
get items is going to be get image files we're going to spit it randomly as per usual and
then we're going to get the label by calling regex labeler, which is a just a handy little
fastai class which labels things with a regular expression. We can't call the regular expression
this particular regular expression directly on the path lib path object we actually want
to call it on the name attribute and fast AI has a nice little function called using
attr using attribute which takes this function and changes it to a function which will be
passed this attribute that's going to be using regex labeler on the name attribute and then
from that data block we can create the data loaders as usual there's two interesting lines
here resize and aug_transforms() aug_transforms() we have seen before in notebook 2, in the
section core data augmentation and so aug_transforms() was the thing which can zoom in and zoom out,
and warp, and rotate and change contrast and change brightness and so forth and flip, to
kind of give us almost, It's like giving us more data being generated synthetically from
the data. We already have and we also learned about random resize crop: which is a kind
of a really cool way of getting, ensuring you get square images at the same time that
you're augmenting the data here, we have a resize to a really large image but you know
by deep learning standards 460x460 is a really large image and then we're using aug_transforms()
with a size. So that's actually going to use random resize crop to a smaller size Why are
we doing that? This particular combination of two steps does something which I think
is unique to Fastai which we call pre-sizing. And the best way is, I will show you this
beautiful example of Powerpoint wizardry that I'm so excited about, to show how pre-sizing
works. What pre-sizing does, is that the first step where we say resize to 460 by 460 is,
it grabs a square, and it grabs it randomly. If it's a kind of landscape orientation photo,
it'll grab it randomly. So it'll take the whole height and randomly grab somewhere from
along the side. If it's a portrait orientation, then it will grab it, you know, take the full
width and grab a random bit from top to bottom. So then we take this area here, and here it
is, right? And so that's what the first resize does. And then the second aug_transforms bit,
will grab a random warped crop, possibly rotated, and will turn that into a square. So there's
two steps, it’s first of all resize to a square that's big, and then the second step,
is to a kind of rotation and warping and zooming stage to something smaller, in this case 224
by 224. Because this first step creates something that's square, and always is the same size,
the second step can happen on the GPU. Normally, things like rotating and image warping are
actually pretty slow. Also, normally doing a zoom and rotate and a warp actually is really
destructive to the image because each one of those things requires an interpolation
step. Which it's not just slow, it actually makes the image really low quality. So we
do it in a very special way in Fastai. I think it's unique, where we do all of the all of
these kind of coordinate transforms like rotations and warps and zooms and so forth, not on the
actual pixels, but instead we kind of keep track of the changing coordinate values in
a in a non-lossy way, so the full floating-point value, and then once at the very end, we then
do the interpolation. The results are quite striking. Here is what the difference looks
like. Hopefully you can see this on the video. On the left is our pre-sizing approach, and
on the right is the standard approach that other libraries use. And you can see that
the one on the right is a lot less nicely focused, and it also has weird things like
this should be grass here, but it's actually got its bum sticking way out. This has a little
bit of weird distortions, this has got loads of weird distortions. So you can see the pre-sized
version really ends up way way better and I think we have a question, Rachel. Are the
blocks in the DataBlock an ordered list? Do they specify the input and output structures
respectively? Are there always two blocks or can there be more than two? For example,
if you wanted a segmentation model, would the second block be something about segmentation?
So, yeah, this is an ordered list. So the first item says I want to create an image,
and then the second item says I want to create a category. So that's my independent and dependent
variable. You can have one thing here, you can have three things here, you can have any
amount of things here you want. Obviously the vast majority of the time it'll be two
only: there's an independent variable and a dependent variable. We'll be seeing this
in more detail later, although if you go back to the earlier lesson when we introduced DataBlocks,
I do have a picture, kind of, showing how these pieces get together. So, after you've
put together DataBlock, created your DataLoaders, you want to make sure it's working correctly.
So the obvious thing to do for a computer vision DataBlock is show_batch and show_batch
will show you the items, and you can kind of just make sure they look sensible, that
looks like the labels are reasonable. If you add a unique=True, then it's going to show
you the same image with all the different augmentations. This is a good way to make
sure your augmentations work. If you make a mistake in your DataBlock, in this example,
there's no resize, so different images are going to be different sizes or be impossible
to collate them into a batch. So if you call ‘.summary’, this is a really neat thing,
which will go through and tell you everything that's happening. So I… Collecting the items.
How many did I find? What happened when I split them? What are the different variables,
independent, dependent variables I’m creating. Let's try and create one of these. Here’s
each step. Create my image. Create categorize. Here’s what the first thing gave me. An
American Bulldog. Here’s the final sample. Is this image, this size, this category. And
then eventually it says oh, oh, it's not possible to collate your items. I tried to collate
these zero index members of your tuples. So in other words, that's the independent variable
and I got, this was size 500 by 375, this was 375 by 500. Oh, I can't collate these
into a tensor because they're different sizes. So this is a super great debugging tool for
debugging your DataBlocks. We have a question. How does the item transforms presize work
if the resize is smaller than the image? Is a whole width or height still taken, or is
it just a random crop with the revised value? So if you remember back to Lesson 2, we looked
at the different ways of creating these things you can use squish, you can use pad, or you
can use crop. So if your image is smaller than the precise value, then squish will really
be zoom, so it will just small stretch. It'll stretch it, and then pattern crop will do
much the same thing. And so you'll just end up with a, you know, the same. This looks
like these, but it'll be a, kind of, lower, more pixelated, lower resolution because it's
having to zoom in a little bit. Okay, so a lot of people say that you should do a hell
of a lot of data cleaning before you model. We don't. We say model as soon as you can,
because remember what we found in, in Notebook 2. Your, your model can teach you about the
problems in your data. So as soon as I've got to a point where I have a DataBlock, that's
working, and I have DataLoaders, I'm going to build a model. And so here I'm, you know,
it also tells me how I'm going. So, I'm getting 7% error. Wow, that's actually really good
for a pets model. And so at this point now that I have a model I can do that stuff we
learned about earlier, in 02, the Notebook 02, where we trained our model, and used it
to clean the data. So we can look at the classification, a confusion matrix, top losses, the image
cleaner widget, you know, so forth. Okay, now one thing interesting here is in Notebook
4 we included a loss function, when we created a Learner, and here we don't pass in our loss
function. Why is that? That's because fastAI will try to automatically pick a somewhat
sensible loss function for you. And so for a image classification task it knows what
loss function is the normal one to pick, and it's done it for you, but let's have a look
and see what it actually did pick. So we could have a look at ‘learn.loss_func’ and we
will see it is cross-entropy loss. What on earth is cross-entropy loss. I'm glad you
asked. Let's find out. Cross entropy loss is really much the same as the MNIST loss
we created with that, with that, sigmoid and the one minus predictions and predictions,
but it's, it's a, kind of, extended version of that. And the extended version of that,
is that, that ‘torch.where’ that we looked at in Notebook 4, only works when you have
a binary outcome. In that case it was: ‘Is it a three or not?’ But in this case we've
got which of the thirty-seven pet breeds is it? So, we want to, kind of, create something
just like that sigmoid and ‘torch.where’, that which also works nicely for more than
two categories. So, let's see how we can do that, so first of all, let's grab a batch.
There is a… Yes? There is a question. Why do we want to build a model before cleaning
the data? I would think a clean dataset would help in training. Yeah, absolutely a current
clean dataset helps in training, but remember as we saw in notebook 02, an initial model
helps you clean the dataset. So remember how ‘plot_top_losses’ helped us identify mislabeled
images, and the confusion matrix helps us recognize which things we were getting confused,
and might need, you know, fixing and the ‘ImageClassifierCleaner’ actually let us find things like, an image
that contained two bears, rather than one bear, and cleaned it up. So a model is just
a fantastic way to help you zoom in on the data that matters, which things seem to have
the problems, which things are most important. Stuff like that. So you would go through and
you clean it, with the model helping you, and then you go back and train it again, with
the clean data. Thanks for the great question. Okay, so in order to understand cross-entropy
loss let's grab a batch of data, which we can use ‘dls.one_batch’, and that's going
to grab a batch from the training set. We could also go first(dls.train) and that's
going to do exactly the same thing. And so then we can destructure that into the independent,
dependent variable, and so the dependent variable shows us we've got a batch size of 64. So
it shows us the 64 categories. And remember those numbers simply refer to the index of,
into the vocab. So for example 16 is a boxer. And so that all happens for you automatically,
when we say ‘show_batch’, it shows us those strings. So here’s a first mini-batch,
and so now we can view the predictions, that is the activations of the final layer of the
network, by calling ‘get_prieds’. And you can pass in a DataLoader, and a DataLoader
can really be anything that's going to return a sequence of mini batches. So we can just
pass in a list, containing our mini batch, as a DataLoader, and so that's going to get
the predictions for one mini batch. So here's some predictions. Okay, so the actual predictions,
if we go ‘preds[0].sum’, to grab the predictions for the first image, and add them all up,
they add up to one. And there are 37 of them. So that makes sense. Right? It's like the
very first thing is, what is the probability that that is a ‘dls.vocab’, the first
thing is what's the probability it's an Abyssinian cat. It's ten to the negative six. You see?
And so forth. So it's basically like it's not this, it's not this, it's not this, and
you can look through and, oh here this one here, you know, obviously what it thinks it
is. So how did it? You know, so we... We obviously want the probabilities to sum to one, because
it would be pretty weird if, if they didn't. It would say, you know, that the, the probability
of being one of these things is more than 1 or less than 1, which would be extremely
odd. So how do we go about creating these predictions, where each one is between zero
and one, and they all add up to 1. To do that we use something called softmax. Softmax is
basically an extension of sigmoid, to handle more than two levels, two categories. So remember
the sigmoid function looked like this. We used that for our 3s vs. 7s model. So what
if we want 37 categories, rather than two categories. We need one activation for every
category. So actually the threes and sevens model, rather than thinking of that as an
‘is-3’ model, we could actually say: ‘Oh that has two categories, so let's actually
create two activations. One representing how three like something is, and one representing
how seven like something is.’ So let's say, you know, let's just say that we have 6 MNIST
digits and these were the... Can I do this? And this first column were the activations
of my model for, for one activation, and the second column was for a second activation.
So my final layer actually has two activations now. So this is like how much like a 3 is
it? And this is how much like a 7 is it? But this one is not at all like a 3, and it's
slightly not like a seven. This is very much like a three, and not much like a seven, and
so forth. So we can take that model, and rather having, rather than having one activation
for like, is three, we can have two activations for how much like a three, how much like a
seven. So if we take the sigmoid of that, we get two numbers between naught and one,
but they don't add up to one. So that doesn't make any sense. It can't be 0.66 chances of
three , and 0.56 chances of seven, because every digit in that data set is only one,
or the other. So that's not going to work, but what we could do is we could take the
difference between this value, and this value and, say that's how likely it is to be a three.
So in other words this one here, with a high number here, and a low number here, is very
likely to be a three. So we could basically say in the binary case, these activations,
that what really matters is their relative confidence of being a three versus a seven.
So we could calculate the difference between column one and column two, or column index
zero and column index one, right? And here's the difference between the two columns, there's
that big difference, and we could take the sigmoid of that. Right? And so this is now
giving us a single number between naught and one, and so then, since we wanted two columns,
we could make column index zero, the sigmoid, and column index one, could be one minus that,
and now look these all add up to one. So here's probability of three, probability of seven,
but the second one, probably three, probability of seven, and so forth. So like that's a way
that we could go from having two activations for every image, to creating two probabilities,
each of which is between naught and one, and each pair of which adds to one. Great. How
do we extend that to more than two columns? To extend it to more than two columns we use
this function, which is called softmax. Softmax is equal to e to the x, divided by the sum
of e to the x. Just to show you if I go softmax on my activations, I get 0.6025, 0.3975, 0.6025,
0.3975, I get exactly the same thing. Right? So softmax in the binary case, is identical
to the sigmoid that we just looked at. But in the multi category case, we basically end
up with something like this. Let's say we were doing the teddy bear, grizzly bear, brown
bear, and for that, remember, our neural net is going to have the final layer, will have
three activations. So let's say it was 0.02, -2.49, 1.25. So to calculate softmax I first
go e to the power of each of these three things, so here's e to the power of .02, e to the
power of -2.49, e to the power of 3.4, e to the power of 1.25. Ok, then I add them up
so there's the sum of the exps and then softmax will simply be 1.02 divided by 4.6 and then
this one will be 0.08 divided by 4.6. And this one will be 3.49 divided by 4.6 so since
each one of these represents each number divided by the sum, that means that the total is one.
Okay, and because all of these are positive and each one is an item divided by the sum
it means all of these must be between naught and one. So this shows you that softmax always
gives you numbers between naught and 1 and they always add up to 1. That in practice
you can just call torch dot softmax. And it will give you this result of this, this function.
So you should experiment with this in your own time, you know, write this out by hand
and try putting in these numbers, right, and, and see how that you get back the numbers
I claim you're going to get back and make sure this makes sense to you. So one of the
interesting points about softmax is, remember I told you that exp is e to the power of something,
and now what that means is that e to the power of something grows very very fast. Right?
So like exp of 4 is 54, exp of 8 is 29, 2980, right. It grows super fast and what that means
is that if you have one activation that's just a bit bigger than the others, its softmax
will be a lot bigger than the others. So intuitively the softmax function really wants to pick
one class among the others. Which is generally what you want, right, when you're trying to
train a classifier to say which breed is it. You kind of want it to to pick one and kind
of go for it, right? And so that's what softmax does. That's not what you always want. So
sometimes at inference time you want it to be a bit cautious. And so you kind of got
to remember that softmax isn't always the perfect approach but it's the default. It's
what we use most of the time and it works well on a lot of situations. So that is softmax.
Now in the binary case for the MNIST three versus sevens, this was how we calculated
the MNIST loss, we took the sigmoid and then we did either one minus that or that as our
loss function. Which is fine as you saw it, it worked, right? And so we could do this
exactly the same thing. We can't use torch dot where anymore because targets aren't just
zero or one, targets could be any number from naught to 36. So we could do that by replacing
the torch dot where with indexing. So here's an example for the binary case. Let's say
these are our targets 0 1 0 1 1 0 and these are our softmax activations which we calculated
before, they’re just from some random numbers, just for a toy example. So one way to do instead
of doing torch dot where, we could instead, have a look at this, I could say I could grab
all the numbers from naught to 5 and if I index into here With all the numbers from
0 to 5 and then my targets, 0 1 0 1 0 1 1 0 then what that's going to do is it's going
to pick a row 0 it'll pick 0.6. And then for row 1 it'll pick 1, a 0.49. for row 2, it'll
pick 0, a .13, for row 4 it'll pick 1, a .003 and so forth. So this is a super nifty indexing
expression which you should definitely play with, right, and it's basically this trick
of passing multiple things to the pytorch indexer. The first thing says, which rows
should you return; and the second thing says, for each of those rows, which column should
you return? So this is returning all the rows and these columns, for each one and so this
is actually identical to torch dot where. Or isn't that tricky? And so the nice thing
is we can now use that for more than just two values. And so here's, here's the fully
worked out thing, so I've got my threes column, I've got my sevens column, here's that target,
here’s the indexes from naught one, two, three, four five. And so here's 0, 0, .6;
1, 1, .49; 0, 2, .13, and so forth. So yeah this works just as well with more than two
columns. So we can add, you know, for doing a full MNIST, you know, so all the digits
from naught to nine. We could have ten columns and we would just be indexing into the ten.
So this thing we're doing where we're going minus our activations matrix, all of the numbers
from naught to N and then our targets, is exactly the same as something that already
exists in pytorch called F dot nll_loss as you can see. Exactly the same. And so again,
we're kind of seeing that these things inside pytorch and fastAI are just little shortcuts
for stuff we can write ourselves. Nll_loss stands for negative log likelihood, again,
it sounds complex, but actually it's just this indexing expression. Rather confusingly,
there's no log in it. We'll see why in a moment. So let's talk about logs. So this loss, this
loss function works quite well as we saw in the notebook 04. It's basically this, it is
exactly the same as we learned in notebook 04, just a different way of expressing it,
but we can actually make it better. Because remember the probabilities we're looking at
are between naught and one so they can't be smaller than zero. They can't be greater than
one, which means that if our model is trying to decide whether to predict .990 or .999
, it's going to think that those numbers are very very close together, but won't really
care. But actually if you think about the error, you know if there's like a hundred
things, a thousand things, then this would like be ten things are wrong and this would
be like one thing is wrong. But this is really like ten times better than this so really,
what we'd like to do is to transform the numbers between zero and one to instead between, be
between negative infinity and infinity. And there's a function that does exactly that
which is called logarithm. Okay, so, as the, so the numbers we could have can be between
zero and one and as we get closer and closer to zero it goes down to infinity and then
at one, it's going to be zero and we can't go above zero because our loss function we
want to be negative. So, this logarithm, in case you forgot, is, hopefully you vaguely
remember what logarithm is from high school, but that basically the definition is this:
if you have some number that is y that is b to the power of a Then logarithm is defined
such that a equals the logarithm of y comma b. In other words it tells you b to the power
of what equals y. Which is not that interesting of itself but one of the really interesting
things about logarithms is this very cool relationship, which is that log of a times
b equals log of a plus log of b. And we use that all the time in deep learning and machine
learning because this number here a times b can get very very big or very very small.
If you multiply things, a lot of small things together, you'll get a tiny number, if you
multiply a lot of big things together, you'll get a huge number. It can get so big or so
small that the kind of the precision in your computer's floating point gets really bad,
whereas this thing here adding is not going to get out of control. So we really love using
logarithms like particularly in a deep neural net where there's lots of layers, we're kind
of multiplying and adding many times, though, this kind of tends to come out quite nicely.
So when we take the probabilities that we saw before, the things that came out of this
function, and we take their logs and we take the mean, that is called negative log likelihood,
and so this ends up being kind of a really nicely behaved number because of this property
of the log that we described. So if you take the softmax and then take the log, then pass
that to an nll_loss because remember that we didn't actually take the log at all despite
the name, that gives you cross entropy loss. So that leaves an obvious question of why
doesn't nll_loss actually take the log and the reason for that is that it's more convenient
computationally to actually take the log back at the softmax step. So pytorch has a function
called log_softmax so since it's actually easier to do the log at the softmax stage,
it's just faster and more accurate. Pytorch assumes that you use soft log max and then
pass that to nll_loss. so nll_loss does not do the log. It assumes that you've done the
log beforehand. So log_softmax followed by nll_loss is the definition of cross-entropy
loss in pytorch. So that's our loss function and so you can pass that some activations
and some targets and get back a number and pretty much everything in pytorch every every
one of these kinds of functions, you can either use the NN version as a class like this and
then call that object as if it's a function, or you can just use F dot with the camelcase
name as a function directly and as you can see, they're exactly the same number. People
normally use the class version in the documentation in pytorch, you'll see it normally uses a
class version so we tend to use the class version as well. You'll see that it's returning
a single number and that's because it takes the mean because a loss needs to be as we've
discussed the mean but if you want to see the underlying numbers before taking the mean
you can just pass in reduction=none and that shows you the individual cross-entropy losses
before taking the mean. Okay, great, so this is a good place to stop with our discussion
of loss functions and such things. Rach, were there any questions about this? Why does the
loss function need to be negative? Well, okay, I mean I guess it doesn't but it's we want
something that the lower it is, the better, and we kind of need it to cut off somewhere.
I have to think about this more during the week because I'm, it's a bit tough, I’m
a bit tired. Yeah, so let me let me refresh my memory when I'm awake Okay. Now next week
… well, nope not for the video. Next week actually happened last week so it's the thing
I'm about to say is actually your. So next week we're going to be talking about data
ethics, and I wanted to kind of segue into that by talking about how my week’s gone,
because a week or two ago I did as part of a lesson I actually talked about the efficacy
of masks. I mean specifically wearing masks in public and I pointed out that the efficacy
of masks seemed like it could be really high and maybe everybody should be wearing them.
And somehow I found myself as the face of a global advocacy campaign. And so if you
go to masks4all.co, you’ll find a website talking about masks. And I've been on, you
know, TV shows in South Africa and the US and England and Australia and on radio and
blah blah blah talking about masks. Why is this? Well, it's because as a data scientist,
you know, I noticed that the data around masks seemed to be getting misunderstood and it
seemed that that misunderstanding was costing possibly hundreds of thousands of lives. You
know, literally in the places that were using masks it seemed to be associated with orders
of magnitude fewer deaths and one of the things to talk about next week is like, you know,
what's your role as a data scientist. And, and you know, I strongly believe that it's
to understand the data and then do something about it. And so nobody was talking about
this So I ended up writing an article that appeared in The Washington Post that basically
called on people to really consider wearing masks (which is this article). And, you know
I was, I was lucky, I managed to kind of get a huge team of brilliant (not, not a huge,
but a pretty decent-sized team of brilliant) volunteers who helped, you know, kind of build
this website and kind of some PR folks and stuff like that. But what became clear was,
and I was talking to politicians, you know, senators, and staffers, and what was becoming
clear is that people weren't convinced by the science, which is fair enough because
it's, it's hard to. You know when the WHO and the CDC is saying you don't need to wear
a mask and some random data scientist is saying but doesn't seem to be what the data is showing.
You know, you've got half a brain you would pick the WHO and the CDC not the random data
scientist. So I really felt like I, if I was going to be an effective advocate, I needed
to sort the science out. And you, know credentialism is strong. And so it wouldn't be enough for
me to say it. I needed to find other people to say it. So I put together a team of 19
scientists, Including you know a professor of sociology, a professor of aerosol dynamics,
the founder of an African movement that's that kind of studied preventive methods for
methods for tuberculosis, a Stanford professor who studies mask disposal and cleaning methods,
a bunch of Chinese scientists who study epidemiology modeling A UCLA professor, who is one of the
top Infectious disease epidemiologist experts, and so forth. So like this kind of all-star
team of people from all around the world, and I had never met any of these people before
so (well, no not quite true, I knew Austin a little bit and I knew Zeynep a little bit,
and Lex a little bit). But on the whole you know (and well Reshama, we all know she's
awesome. So it's great to actually have a fast.ai community person there too. And, so,
but yeah, I kind of tried to pull together people from you know, as many geographies
as possible and as many areas of expertise as possible. And you know the kind of the
global community helped me find papers about, about everything. About, you know, how different
materials work, about how droplets form, about epidemiology, about case studies of people
infecting with and without masks, blah blah blah. And we ended up in the last week; basically
we wrote this paper. It contains 84 citations. And you know, we basically worked around the
clock on it as a team, and it's out. And it's been sent to a number of, some of the earlier
versions 3 or 4 days ago we sent to some governments. So one of the things is in this team. I try
to look for people who were working closely with government leaders, not just that they're
scientists. And so this, this went out to a number of government ministers. And in the
last few days, I've heard that it was a very significant part of decisions by governments
to change their, to change their guidelines around masks. And you know the fight’s not
over by any means, and in particular the UK is a bit of a holdout. But I'm going to be
on ITV tomorrow and then BBC the next day. You know, it's it's kind of required stepping
out to be a lot more than just a data scientist. So I've had to pull together, you know politicians
and staffers. I've had to, you know, you know , hassle with the media to try and get you
know coverage. And you know today I'm now starting to do a lot of work with unions to
try to get unions to understand this You know, it's really a case of like saying - okay as
a data scientist, and in conjunction with real scientists, we've built this really strong
understanding that masks, you know this simple, but incredibly powerful tool. That doesn't
do anything unless I can effectively communicate this to decision-makers. So today I was you
know on the phone to, you know, one of the top union leaders in the country, explaining
what this means. Basically it turns out that in buses in America, the kind of the air conditioning
is set up so that it blows from the back to the front. And there's actually case studies
in the medical literature of how people that are seated downwind of an air conditioning
unit in a restaurant ended up all getting sick with Covid 19. And so we can see why
like bus drivers are dying. Because they're like, they're right in the wrong spot here
and their passengers aren't wearing masks. So I tried to unexplained this science to
union leaders so that they understand that to keep the workers safe it's not enough just
for the driver to wear a mask, but all the people on the bus needed to be wearing masks
as well. So, you know all this is basically to say ,,, you know as data scientists, I
think we have a responsibility to study the data and then do something about it. It's
not just a research exercise, it's not just a computation exercise, you know. What, what's
the point of doing things if it doesn't lead to anything? So, yeah, so, next week. We'll
be talking about this a lot more, but I think you know - this is a really to me kind of
interesting example of how digging into the data can lead to really amazing things happening.
And, and in this case, I strongly believe, and a lot of people are telling me they strongly
believe that this kind of advocacy work that's come out of this data analysis is, is already
saving lives. And so I hope this might help inspire you to, to take your data analysis
and to take it to places that it really makes a difference. So thank you very much, and
I'll see you next week.
Welcome to lesson five and we'll be talking about ethics for data science and this corresponds
to chapter 3 of the book.
I've also just taught a six-week version of this course, I'm currently teaching an eight-week
version of this course and will release some combination or subset of that as a Fast AI
and USF ethics for data science class.
If you want more detail, coming in July.
I am Rachel Thomas I am the founding director of the Center for Applied Data Ethics at the
University of San Francisco and also co-founder of fastai together with Jeremy Howard.
My background, I have a PhD in math and worked as a data scientist and software engineer
in the tech industry and then have been working at USF and on fastai for the past four years
now.
So ethics issues are in the news.
These articles I think are all from this fall, kind of showing up at this intersection of
how technology is impacting our world in many kind of increasingly powerful ways.
Many of which really raised concerns and I want to start by talking about three cases
that I hope everyone working in technology knows about and is on the lookout for.
So even if you only watch five minutes of this video, these are kind of the three cases
I want you to see and one is feedback loops.
And so feedback loops can occur whenever your model is controlling the next round of data
you get.
So the data that's returned quickly becomes flawed by the software itself.
And this can show up in many places.
One example is with recommendation systems.
And so recommendation systems are essentially about predicting what content the user will
like but they're also determining what content the user is even exposed to and helping determine
what has a chance of becoming popular.
And so YouTube has gotten a lot of attention about this for kind of highly recommending
many conspiracy theories, many kind of very damaging conspiracy theories.
There is also they've kind of put together recommendations of paedophilia picked out
of what were kind of in home movies but when are kind of strung together, ones that happen
to have young girls in bathing suits or in their pajamas.
So there's some really really concerning results and this is not something that anybody intended
and we'll talk about this more later.
Um, I think particularly for many of us coming from a science background we are often used
to thinking of like oh you know like we observe the data but really whenever you're building
products that interact with the real world you're also kind of controlling what the data
looks like.
Second case study I want everyone to know about it comes from software that's used to
determine poor people's health benefits.
It's used in over half of the 50 states and the Verge did an investigation on what happened
when it was rolled out in Arkansas and what happened is there was a bug and the software
implementation that incorrectly cut coverage for people with cerebral palsy or diabetes,
including Tammy Dobbs who's pictured here and was interviewed in the article.
And so these are people that really needed this health care and it was erroneously cut
due to this bug and so they were really and they couldn't get any sort of explanation
and there was no appeals or recourse process in place.
And eventually, this all came out through a lengthy court case.
But it's something where it caused a lot of suffering in the meantime.
And so it's really important to implement systems with a way to identify and address
mistakes and to do that quickly in a way that hopefully minimizes damage because we all
know software can have bugs.
Our code can behave in unexpected ways and we need to be prepared for that.
I wrote more about this idea in a post two years ago “What HBR gets wrong about algorithms
and bias” . And then the third case study that everyone should know about, so this is
Latanya Sweeney who's director of the Data Privacy lab at Harvard, has a PhD in Computer
Science.
And She noticed several years ago that when you google her name you would get these ads
saying “Latanya, Sweeney, Arrested?”
implying that she has a criminal record.
She's the only Latanya Sweeney and has never been arrested.
She paid $50 to the background check company and confirmed that she's never been arrested.
She tried googling some other names and she noticed, for example, Kristen Lindquist got
much more neutral ads that just say we found Kristen Lindquist even though Kristen Lindquist
has been arrested three times.
And so being a computer scientist, Dr Sweeney to study this very systematically she looked
at over 2,000 names and found that this pattern held in which disproportionately African American
names were getting these ads suggesting that the person had a criminal record regardless
of whether they did and Traditionally European American or white names
were getting more neutral ads.
And this problem of, kind of, bias in advertising shows up a ton.
Advertising is, kind of, the profit model for most of the major tech platforms, and
it kind of continues to pop up in high-impact ways.
Just last year there was research showing how Facebook's ad system discriminates even
when the person placing the ad is not trying to do so.
So for instance the same housing ad, exact same text, if you change the photo between
a white family and a black family, it served two very different audiences.
So this is something that can really impact people when they're looking for housing, when
they're applying for jobs, and is a, is a definite area of concern.
So now I want to, kind of, step back and ask why, why does this matter.
And, so a very kind of extreme, extreme example, it's just that data collection has played
a pivotal role in several genocides, including, including the Holocaust.
And so this is a photo of Adolf Hitler meeting with the CEO of IBM at the time.
I think this photo was taken in 1937, and IBM continued to partner with the Nazis, kind
of, long past when many other companies broke their ties.
They produced computers that were used in concentration camps to code whether people
were Jewish, how they were executed, and this is also different from now, where you might
sell somebody a computer, and they never hear from them again.
These machines require a lot of maintenance, some kind of ongoing relationship with vendors
to, kind of, upkeep and repair them.
And it's something that a Swiss judge ruled: It does not seem unreasonable to deduce that
IBM's technical assistance facilitated the task of the Nazis in the commission of their
crimes against humanity.
Acts also involving accountancy and classification by IBM machines and utilized in the concentration
camps themselves.’
I'm told that they haven't gotten around to apologizing yet.
Oh, that’s...
I guess they've been busy.
…terrible too, yeah.
Okay.
Yeah, and so this is a very, kind of, very sobering example, but I think it's important
to keep in mind, kind of, what can go wrong, and how technology can be used for, for harm.
For very, very terrible harm.
And so this just kind of raises a question, questions that we all need to grapple with
of: ‘How would you feel if you discovered that you had been part of a system that ended
up hurting society?’
‘Would you, would you even know?
‘Would you be open to finding out, kind of, how, how things you had built may have
been harmful?’ and ‘How can you help make sure this doesn't happen?’
And so I think these are questions that we all, all need to grapple with.
It's also important to think about unintended consequences on how your tech could be used,
or misused whether that's by harassors, by authoritarian governments, for propaganda
or disinformation.
And then on a, kind of a more concrete level you could even end up in jail.
And so there was a Volkswagen engineer who got prison time for his role in the diesel
cheating case.
So if you remember this is where Volkswagen was cheating on emissions test, and one of
the, kind of, programmers that was a part of that.
And that person was just following orders from what their boss told them to do, but
that is not, not a good excuse for, for doing something that's unethical.
And so something to be aware of.
So ethics is, the, the discipline dealing with what's good and bad.
It's a set of moral principles.
It's not a set of answers, but it's kind of learning what sort of, what sort of questions
to ask, and even how to weigh these decisions.
And I'll say some more about, kind of, ethical foundations, and different ethical philosophies
later, later on in this lesson.
But first I'm going to, kind of, start with some, some use cases.
Ethics is not the same as religion, laws, social norms, or feelings.
Although it does have overlap with all these things.
It's not a fixed set of rules.
It's well founded standards of right and wrong, and this is something where clearly not everybody
agrees on the ethical action in, in, every case, but that doesn't mean that, kind of,
anything goes, or that all actions are considered equally ethical.
There are many things that are widely agreed upon, and there are, kind of, a philosophical,
philosophical underpinnings for, kind of, making these decisions.
And ethics is also the ongoing study and development of our ethical standards.
It's a kind of, never-ending process of learning to, kind of, practice our ethical wisdom.
And I'm gonna refer it, several times too...
So here I'm referring to a few articles from the Markkula Center for tech ethics at Santa
Clara University.
In particular the work of Shannon Vallor, Brian Green and Irina Raicu, who is fantastic
and they have a lot of resources, some of which I'll circle back to later, later in
this talk.
I spent years of my life studying ethics.
It was my major at university and I spent much time on the question of what is ethics?
I think I'll a away from that is that studying the philosophy ethics was not particularly
helpful in learning about ethics.
Yes, and I will try to keep this kind of very, very applied and very practical.
Also, very kind of tech industry-specific, of what, what do you need in terms of applied
ethics?
Markoulis said it's great - they somehow they take stuff that I thought was super dry and
turn it into useful checklists and things.
I did want to note this was really neat so Casey Fiesler’s a professor at University
of Colorado that I really admire and she created a crowd-sourced spreadsheet of tech ethics
syllabi.
This was maybe two years ago and got over 200 syllabi entered into this this crowd-sourced
spreadsheet and then she did a meta-analysis on them, of kind of looking at all sorts of
aspects of the syllabi, and what's being taught and how it's being taught and ... And published
a paper on it.
What do we teach when we teach tech ethics, and a few interesting things about it?
Is it raises, there a lot of ongoing discussions and lack of agreement on how to, how to best
teach tech ethics.
Should it be a standalone course versus worked into every course in the curriculum?
Who should teach it - a computer scientist, a philosopher, or a sociologist?
And she analyzed for the syllabi what was the course home, and the instructor home?
And you can see that the the instructors came from a range of courses, including computer
science.
A range of disciplines, computer science, information science, philosophy, science and
tech studies, engineering, law, math, business.
What topics to cover -- a huge range of topics that can be covered, including a law and policy
privacy and surveillance in equality justice and human rights, environmental impact, AI
and robots, and professional ethics, work in labor, cybersecurity.
The list goes on and on, and so this is clearly more than can be covered in any even a full
semester length course and certainly not in a kind of a single, single lecture.
What learning outcomes?
This is an area where there's a little bit more agreement, where kind of the number one
skill that courses were trying to teach was critique, followed by spotting issues, making
arguments.
And so a lot of this is just even learning to spot what the issues are and how to critically
evaluate, kind of a piece of technology or a design proposal to see what could go wrong,
what the the risks could be.
All right.
So we're gonna go through kind of a few different core topics.
And as I suggested this is gonna be a kind of extreme subset of what could be covered.
I was trying to pick things that we think are very important and high-impact.
So one is recourse and accountability.
So I already shared this example earlier of, you know, the system that was determining
poor people's healthcare benefits having a bug.
And something that was kind of terrible about this was nobody took responsibility, even
once the bug was found.
So the creator of the algorithm was interviewed and asked, they asked him you know should
people be able to get an explanation for why their benefits have been cut? and he gave
this very callous answer of, ‘You know, yeah, they probably should, but I should probably
dust under my bed, you know, like who's gonna do that,” which is very callous.
And then he ended up blaming the policymakers for how they had rolled out the algorithm.
The policymakers, you know, could blame the software engineers that implemented it, and
so there was a lot of passing the buck here.
Dana Boyd has said that, you know, it's always been a challenge for bureaucracy to assign
responsibility, or bureaucracy is used to evade responsibility, and today's algorithmic
systems are often extending bureaucracy.
A couple of questions and comments about cultural context of any notes that there didn't seem
to be any mention of cultural contexts for ethics as part of those syllabi, and somebody
else was asking, ‘How do you deal, you know, is this culturally dependent?
And how do you deal with that?”
It is culturally dependent.
I will mention this briefly later on, so I'm gonna share three different ethical philosophies
that are kind of from the West, and we'll talk just briefly of one slide on.
For instance right now, there are a number of indigenous data sovereignty
movements.
And I know the Maori data sovereignty movement has been particularly active, but different,
you know, different cultures do have different views on ethics, and I think that the cultural
context is incredibly important.
And we will not get into it tonight, but there's also kind of a growing field of algorithmic
colonialism, and, kind of, studying what are some of the issues when you have technologies
built in one, you know, particular country, and culture, being implemented, you know,
halfway across the world in very different cultural context, often with little to no
input from people, people living in that culture.
And although I do want to say that there are things that are widely, although not universally,
agreed on, and so, for instance the Universal Declaration on Human Rights, despite the name
it is not universally accepted but many, many different countries have accepted that as
a human rights framework, and as those being fundamental rights, and so there are, kind
of, principles that are often held cross culturally, although, yeah, it's rare for something, probably,
to be truly, truly universal.
So returning to this topic of, kind of, accountability and recourse.
Something to keep in mind is the data contains errors.
And so there was a dank database used in California.
It's tracking supposedly gang members, and an auditor found that there were 42 babies
under the age of 1, who had been entered into this database.
And something concerning about the database is that it's basically never updated, I mean
people are added, but they're not removed and so once you're in there, you're in there.
And 28 of those babies were marked as having admitted to being gang members.
And so keep in mind that this is just a really obvious example of the error, but how many
other kind of totally wrong entries are there.
Another example of data containing errors involves the, the three credit bureaus in
the United States.
The FTC's large-scale study of credit reports found that 26% had at least one mistake in
their files, and 5% had errors that could be devastating.
I'm gonna, this is the headline of an article that was written by a public radio reporter
who went to get a an apartment and the landlord called him back afterwards and said, you know,
your background check showed up that you had firearms convictions, and this person did
not have any firearms convictions, and it's something where in most cases the landlord
would probably not even tell, tell you and let you know, that's why you weren't getting
the apartment.
And so, and this guy looked into it.
I should note that this guy was white, which I'm sure helped him in getting the benefit
of the doubt and found this error, and he made dozens of calls, and could not get it
fixed until he told them that he was a reporter, and that he was going to be writing about
it, which is something that most of us would not be able to do.
But it was...
Even once he had pinpointed the error, and he had to, you know, talk to the, you know,
like, County Clerk, in the place he used to live.
It was still a very difficult process to get it updated, and this can have a huge, huge
impact on people's lives .There's also the issue of when technology is used in ways that
the creators may not have intended.
So for instance with facial recognition it is pretty much entirely being developed for
adults, yet NYPD is putting the photos of children as young as age 11 into, into, databases.
And we know the error rates are higher.
This is not how it was developed.
So this is, this is a, a, serious, serious concern.
And there are a number of, kind of, misuses.
The Georgetown Center for Privacy and Technology, which is fantastic, you should definitely
be following them, did a report, ‘Garbage In, Garbage Out [...]’, looking at how police
were using facial recognition in practice and they found some really concerning examples,
for instance, in one case NYPD had a photo of a suspect, and they…
It wasn't returning any matches, and they said: ‘Well this person kind of looks like
Woody Harrelson,’ so then they googled the actor Woody Harrelson, and put his face into
the facial recognition and used that to generate leads.
And this is clearly not the correct use at all, but it's, it's a way that it's being,
it's being used.
And so there's, kind of, total lack of accountability here.
And then another, kind of, study of cases in all 50 states of police officers, kind
of, abusing confidential databases to look up ex-romantic partners, or to look up activists
and so, you know, here this is not necessarily error in the data, although that can be present
as well, but kind of keeping in mind how it can be misused by the users.
All right.
So the next topic is feedback loops and metrics.
And so I talked a bit about feedback loops in the beginning as kind of one of one of
the three key use cases.
And so this is a topic, I wrote a blog post about this fall “The problem with metrics
is a big problem for AI”.
And then together with David Uminsky who is director of The Data Institute expanded this
into a paper “Reliance on metrics is a fundamental challenge for AI”.
And this was accepted to the “Ethics and Data Science” conference.
But over emphasizing metrics can lead to a number of problems including manipulation,
gaming, myopic focus on short-term goals because it's easier to track short-term quantities,
unexpected negative consequences.
And much of AI and machine learning centers on optimizing a metric.
This is kind of both, you know, the strength of machine learning is it's gotten really
really good at optimizing metrics.
But I think this is also kind of inherently a weakness or a limitation.
I'm going to give a few examples.
And this can happen even not just in machine learning kind of but in analog examples as
well.
So this is from a study of when English is, England's public health system implemented
a lot more targets around numbers in the early 2000s.
And the study was called “What's measured is what matters”.
And so they found so one of the targets was around reducing ER wait times, which seems
like a good goal.
However this led to cancelling scheduled operations to draft extra staff into the ER.
So if they felt like there were too many people in the ER they would just start canceling
operations so they could get more doctors requiring patients to wait in queues of ambulances
because time waiting an ambulance didn't count towards your your er wait time.
Turning stretchers into beds by putting them in hallways, I mean there are so big discrepancies
in the numbers reported by hospitals versus by patients.
And so if you ask the hospital on average how long are people waiting you get a very
different answer than when you're asking the patients how long did you have to wait?.
Another, another example is of essay grading software.
And so this essay grading software I believe is being used in 22 states now in the United
States.
Yes, 20 states and it tends to focus on metrics like sentence length, vocabulary, spelling,
subject verb agreement.
Because these are the things that we, we know how to measure and how to measure with a computer.
But it can't evaluate things like creativity or novelty.
However gibberish essays with lots of sophisticated words score well.
And there are even examples of people creating computer programs to generate these kind of
gibberish sophisticated essays.
And then there you know graded by this other computer program and highly rated.
And there's also bias in this.
Essays by african-american students received lower grades from the computer than from expert
human graders.
And essays by students from mainland China received higher scores from the computer than
from expert human graders.
And authors of the study thought that they, this, this results suggest they may be using
chunks of pre memorized text that score well.
And this is, these are just kind of two examples, I have a bunch more in the blog post and even
more in the paper of ways that metrics can invite manipulation and gaming whenever they're
they're given a lot of emphasis.
And this is a good hearts laws of kind of a law that a lot of people talk about.
And it's this idea that the more you rely on a metric the kind of the less reliable
it becomes.
So returning to this example of feedback loops and recommendation systems, Guillaume Chaslot
is a former google, youtube engineer.
YouTube is owned by Google and he wrote a really great post.
And he's done a ton to raise awareness about this issue and founded the nonprofit ‘AlgoTransparency’
which kind of externally tries to monitor YouTube's recommendations.
He's partnered with the Guardian and the Wall Street Journal to do investigations.
But he wrote a post around how kind of in the earlier days the recommendation system
was designed to maximize watch time.
And so and this is, this is something else that's often going on with metrics is that
any metric is just a proxy for what you truly care about.
And so here, you know the team at Google was saying well, you know, if you're watching
more YouTube it signals to, to us that they're happier.
However this also ends up incentivizing content that tells you the rest of the media is lying
because kind of believing that everybody else is lying will encourage you to spend more
time on a particular platform.
So Guillaume wrote a great post about this kind of mechanism that's at play and you know,
this is not just YouTube.
This is any recommendation system could, I think be susceptible to this and there have
been a lot of talk about kind of issues with many recommendation systems across platforms.
But it is, it is something to be mindful of, and something that the, kind of, creators
of this did not anticipate.
And last year, Gillaume, kind of, gathered this data on...
So here the x-axis is the number of channels, number of YouTube channels recommending a
video, and the y-axis is the log of the views, and we see this extreme outlier, which was
Russia's Today take, Russia Today's take on the Mueller Report, and this is something
that Guillaume observed, and then was picked up by the the Washington Post.
But this, this strongly suggests that Russia Today has perhaps gamed the the Recommendation
Algorithm, which is, which is not surprising, and it's something that I think many content
creators are conscious of, and trying to, you know, experiment and see what, what gets
more heavily recommended, and thus more views.
So it's, it’s also important to note that our online environments are designed to be
addictive, and so when, kind of, what we click on is often used as a proxy of, of what we
enjoy, or what we like, that's not necessarily though for of our, kind of, like our best
selves, or our higher selves.
It's, you know, it's what we're clicking on, in this, kind of, highly addictive environment
that's often appealing to some of our, kind of, lower instincts.
Zeynep Tufekci uses the analogy of a, of a cafeteria, that's, kind of, shoving salty,
sugary, fatty foods in our faces, and then learning that ‘Hey people really like, salty,
sugary, fatty foods!’, which I think most of us do in a, kind of, very primal way, but
we often, you know, kind of, our higher self is like ‘Oh I don't want to be eating junk
food all the time,’ and online we often, kind of, don't have a great mechanisms to
say, you know, like ‘Oh I really want to read like more long-form articles that took
months to research, and are gonna take a long time to digest.’
While we may want to do that our online environments are not, not always conducive to it.
Yes?
Sylvain made the comment about the false sense of security argument, which is very relevant
to masks and things.
Don’t you have anything to say about its false sense of security argument?
Can you say more?
There's a common feedback at the moment that people shouldn’t wear masks, because they
might have a false sense of security.
That, kind of, makes sense to you from an ethical point of view, to be telling people
that?
No, I don't think that's a good argument at all.
In general there's so many, other people including Jeremy have pointed this out...
There's so many actions we take to make our lives safer, whether that's wearing seatbelts,
or wearing helmets when biking, practicing safe sex, like all sorts of things where we
really want to maximize our safety, and so I think, and Zeynep Tufekci had a great thread
on this today of...
It's not that there can never be any sort of impact in which people have a false sense
of security, but it is something that you would really want to be gathering data on,
and build a strong case around, and not just assume it's gonna happen, and that…
In most cases people can think of, even if that is a small second-order effect, the effect
of doing something that increases safety tends to have a much larger impact on actually increasing
safety.
You have anything to add to that or...
Yes I mentioned before a lot of our incentives are focused on short term metrics.
Long term things are much harder to measure, and often involve kind of complex relationships.
And then the, the, fundamental business model of, of, most of the tech companies is around
manipulating people's behavior, and monopolizing their time, and these things.
I don't think an advertising is inherently bad, but they...
I think it can be negative when, when taken to an extreme.
There's a great essay by James Grimmelmann ‘The Platform is the Message’, and he
points out: ‘These platforms are structurally at war with themselves.’
‘The same characteristics that make outrageous & offensive content unacceptable are what
make it go viral in the first place.’
And so there's this, kind of, real tension here in which often things, yeah, that, kind
of, can make content really offensive or unacceptable to us, are also what are, kind of, fuelling
their popularity, and being promoted in many cases.
I mean this is it, this is an interesting essay, because he, he, does this, like, really
in-depth dive on the ‘Tide Pod Challenge’, which was this meme around eating Tide Pods,
which are poisonous, do not eat them, and he really analyzes it though.
It's a great look at meme culture, which is very common and how, kind of, argues there's
probably no example of someone talking about the ‘Tide Pod Challenge’, that isn't partially
ironic, which is common in memes, that even, kind of, whatever you're saying they're, kind
of, layers of irony, and different groups are interpreting them differently, and that
even when you try to counteract them, you're still promoting them.
So with the ‘Tide Pod Challenge’ a lot of, like, celebrities we're telling people
‘Don't eat Tide Pods’, but that was also then, kind of, perpetuating, the, the popularity
of this meme.
So it's...
This is an essay I would recommend, that I think it's pretty insightful.
And so this is a...
We'll get to disinformation shortly, but the the major tech platforms often incentivize
and promote disinformation, and this is unintentional, but it's, it is somewhat built into their
design and architecture, their recommendation systems, and ultimately their business models.
And then on the...
On the topic of metrics.
I'm...
I just wonder, bring up, so there's this idea of blitzscaling, and the premise is that if
a company grows big enough, and fast enough, profits will eventually follow.
It prioritizes speed over efficiency, and risks potentially disastrous defeat, and Tim
O'Reilly wrote a really great article last year talking about many of the problems with
this approach, which, I would say, is incredibly widespread, and is, I would say, the fundam...
Kind of fundamental model underlying a lot of venture capital.
And in it, though, investors kind of end up anointing winners, as opposed to, to market
forces.
It tends to lend itself towards creating monopolies and duopolies.
It can...
It's bad for founders, and people end up kind of spreading themselves too thin.
So there are a number, a number of significant downsides to this.
Why am I bringing this up in an ethics lesson?
When we were talking about metrics.
But hockey, hockey stick growth requires automation, and a reliance on metrics.
Also prioritizing speed above all else doesn't leave time to reflect on ethics, and that
is something that's hard that I think it, you do often have to kind of pause to, to
think about ethics, and that following this model, when you do have a problem, it's often
going to show up on a huge scale, if you've, if you've scaled very quickly.
So, I think, this is something to at least, at least be aware of.
So one person asks about: ‘Is there a dichotomy between AI ethics, which seems like a very
First World problem, and wars, poverty, environmental exploitation, has been, kind of, a different
level of a problem, I guess?’
And there's an answer here, which somebody else, maybe you can comment on whether you
agree, or I have anything to add which is that, ‘AI ethics…’, they're saying,
‘...is very important also for other parts of the world particularly in areas with high
cell phone usage.
For example, many countries in Africa have high cell penetration, people get their news
from Facebook, and WhatsApp, and YouTube, and though it's useful, it's been the source
of many problems’.
Did you have any comments on, on kind of...?
Yeah so...
I think the first question…
So AI ethics, as I noted earlier, and I'm using the, the, phrase data ethics here, but
it's this very broad, and it refers to a lot of things.
I think if people are talking about the, you know, ‘In the future can computers achieve
sentience and what are the ethics around that?’ and that is not my focus at all.
I'm very much focused on, and this is our mission with the Center for Applied Data Ethics
at the University of San Francisco, is, kind of, how are people being harmed now?
What are the most immediate harms?
And so in that sense, I don't think that data ethics has to be a First World or, kind of,
futuristic issue.
It's…
It’s what's happening now, and yeah, and as, as the person said in a few examples,
well one example, I'll get to later is definitely the, the genocide in Myanmar in which the
Muslim minority, the Rohingya are experiencing genocide.
The UN has ruled that Facebook played a determining role in that, which is really intense, and
terrible.
And so, I think, that's an example of technology, yeah, leading to very real harm now.
They're also WhatsApp,, which is owned by, owned by Facebook.
There have been issues with people spreading disinformation, and rumors, and it's led to
several lynching, dozens of lynchings in India.
Of people kind of spreading these false rumors of ‘Oh there's a kidnapper coming around’,
and in these kind of small, remote villages, and then a visitor, or, or stranger shows
up and gets killed.
WhatsApp also played a very important role, or bad role, in the election of Bolsonaro
in Brazil, election of Duterte in the Philippines.
So, I think, technology is having a kind of very immediate impact on, on people.
And that...
Those are the types of ethical questions I'm really interested in, and that I hope, I hope
you are interested in as well.
Do you have anything else to say about that or..?
And I will, I will talk about disinformation.
I realize those were, kind of, some disinformation focused, and I'm gonna talk about bias first.
I think it's bias, then disinformation.
Yes?
Question.
Mhm.
‘When we talk about ethics, how much of this is intentional unethical behavior?
I see a lot of the examples as more of incompetent behavior, or bad modeling, where the product,
or models are rushed without sufficient testing, or thought around bias thereforth, but not
necessarily malintent.
Yeah, no, I agree with that.
I think that most of this is unintentional.
I do think there's a, often, no...
Well...
We'll get into some cases.
I think that, I think in many cases the profit incentives are misaligned, and I do think
that, when people are earning a lot of money it is very hard to consider actions that would
reduce their profits, even if they would prevent harm and increase, kind of, ethics.
And so I think that, you know, there's at some point where valuing profit over how people
are being harmed is, you know, when does, when does that become intentional is, you
know, a question to debate, but I, you know, I don't, I don't think people are setting
out to say like ‘I want to cause a genocide’, or ‘I want to help authoritarian leader
get elected’.
Most people are not are not starting with that, but I think sometimes it's a carelessness,
and a thoughtlessness, but that I, I do think we are responsible for that, and we're responsible
to kind of be more careful, and more thoughtful in how we approach things.
Alright So bias...
So bias, I think, is a, an issue that's probably gotten a lot of attention, which is great
and, I want to get a little bit more in-depth, because sometimes discussions on bias stay
a bit superficial.
There was a great paper by Harini Suresh and John Guttag, last year, that looked at, kind
of, came with this taxonomy of different types of bias, and how they had, kind of, different
sources in the machine learning, kind of, pipeline.
And it was really helpful, because, you know, different sources have different causes, and
they also require different, different approaches for addressing then.
The...
Harini wrote a blog post version of the paper as well, which I love when researchers do
that.
I hope more of you, if you're writing an academic paper, also write the blogpost version.
I'm just going to go through a few of these types.
So one is, representation bias, and so I would imagine many of you have heard of Joy Buolamwini’s
work, which has rightly received a lot of publicity.
In ‘Gender Shades, she and Timnit Gebru investigated commercial computer vision products
from Microsoft, IBM and Face++, and then Joy Buolamwini and Deb Raji, did a follow-up study
that looked at Amazon and Kairos and several other companies, and the typical results they
kind of found basically everywhere was that these products performed significantly, significantly
worse on dark skinned women.
So they were, kind of, doing worse on people with darker skin, compared to lighter skin.
Worse on women than on men, and then the kind of intersection of that, dark skinned women
had these very high error rates.
And so one example is IBM, their product was 99.7% accurate on light skinned men, and only
65% percent accurate on dark skinned women.
And again, this is a commercial computer vision product that was released.
Question?
It's a question from the TWIML study group.
‘At the Volkswagen example, in many cases its management that drives and rewards unethical
behavior.
What can an individual engineer do in a case like this?
Especially in a place like Silicon Valley where people move companies so often?’
Yeah, so I think, I think that's a great point.
Yeah, and that is an example where I would have, I would have much rather seen people
that were higher ranking doing jail time about this, because, I think, that they were, they
were driving that, and, I think, that, yeah, it's great to remember that.
I know many people in the world don't have this option, but I think for many of us working
in tech, particularly in Silicon Valley, we tend to have a lot of options, and often more
options than we realize like.
Okay?
I talk to people frequently that feel trapped in their jobs, even though, you know, they're
a software engineer in Silicon Valley, and, and so many companies are hiring.
And so I think it is important to use that leverage.
I think a lot of the, kind of, employee organizing movements are very promising, and that can
be useful, but really trying to, kind of, vet the, the ethics of the company you're
joining, and also being willing to walk away if you, if, if you're able to do so.
That's a great, great question.
So what this, this example of representation bias here, the, kind of, way to address this
is to build a more representative data set.
It's very important to keep consent in mind of the, the, people, of, if you're using pictures
of people, but Joy Buolamwini and Timnit Gebru did this, as part of, as part of ‘Gender
Shades’.
However, this is a...
The fact that this was a problem not just for one company, but basically kind of every
company they looked at, was due to this underlying problem, which is that in machine learning
bench, benchmark datasets spur on a lot of research, however, kind of, several years
ago all the, kind of, popular facial datasets were primarily of light skinned men.
For instance.
IJB-A, a kind of popular face dataset several years ago, only 4% of the images were of dark-skinned
women.
Yes?
Question: ‘I've been worried about COVID-19 contact tracing, and the erosion of privacy
location tracking, private surveillance Companies, etc.
What can we do to protect our digital rights post COVID.
Can we look to any examples in history of what to expect?’
That is…
That is a huge question, and something I have been thinking about as well.
I am, I'm gonna put that off till later to talk about, and that is something where in
the course I teach, I have an entire unit on privacy and surveillance, which I do not
in tonight's lecture, but I can share some materials, although I am already really, even
just like, rethinking how I'm gonna teach privacy and surveillance in the age of COVID-19
compared to two months ago, when I taught it the first time, but that is something I
think about a lot, and I will talk about later if we have time or, or on the forums, if we,
if we don't.
That's a great question.
A very important question...
On the topic.
And, and I will say, and I have not had the time to look into them yet, I do know that
there are groups that are working on what are kind of more privacy protecting approaches
for, for tracking and they're also groups putting out, like if we are going to use some
sort of tracking, what are the safeguards that need to be in place to do it responsibly.
Yes?
I've been looking at that too.
It does seem like this is a solvable problem with, with technology.
Not all of these problems are, but you can certainly store tracking history on somebody's
cell phone, and then you could have something where you say when you've been infected, and
at that point you could tell people that they've been infected by sharing the location, in
a privacy preserving way.
I think some people are trying to work on that.
I'm not sure it's actually technically a problem.
So I think there are, sometimes, there are ways to provide the minimum kind of level,
you know, kind of application with, with, you know, whilst keeping privacy.
Yeah, and then I think it's very important to also have things of, you know, clear like
expiration date, like we, you know, like looking back at 9/11 in the United States that, kind
of, ushered in all these laws, that were now, kind of stuck with, that have really eroded
privacy.
Of anything we do around COVID-19, being very clear, we are just doing this for COVID-19,
and then there's a time limit, and expires, and it's kind of for this clear purpose.
And they're also issues though of, you know, I mentioned earlier about data containing
errors, you know, this has already been an issue, and some other countries that we're
doing, kind of, more surveillance focused approaches of, you know, what about like when
it's wrong and people are getting kind of quarantined, and they don't even know why,
and for no reason, and so to be mindful of those.
But yeah, we’ll, we'll talk more about this, kind of, later on.
Back to...
Back to bias...
Yeah, we had kind of the, the benchmarks.
So, when the benchmark that's, you know, widely used, has bias, then that is really, kind
of, replicated at scale, and we're seeing this with ImageNet as well, which is, you
know, probably the most widely studied computer vision dataset out there.
Two-thirds of the ImageNet images are from the West.
So this pie chart shows that the 45% of the images in ImageNet are from the United States,
7% from Great Britain, 6% from Italy, 3% from Canada, 3% from Australia, you know, and we're
covering a lot of, a lot of this pie without having, having gotten to outside the West,
and so then this has shown up in concrete ways of classifiers trained on ImageNet.
So one of the categories is bridegroom, a man getting married, there are a lot of, you
know, cultural components to that, and so they have, you know, much higher error rates
on, on bridegrooms from, from the Middle East, or from the Global South.
And there are, there are people now, kind of, working to diversify these datasets, but
it is quite dangerous, that they can really be, kind of, widely built on its scale, or
have been widely built on its scale before these biases were recognized.
Another key study is the COMPAS Recidivism Algorithm, which is used in determining who,
who has to pay bail, so in the U.S a very large number of people are in prison, who
have not even had a trial yet just, because they're too poor to afford bail, as well as
sentencing decisions and parole decisions.
And ProPublica, I did a famous investigation in 2016 that I imagine many of you have heard
of, in which they found that the false positive rate for black defendants was nearly twice
as high as for white defendants.
So black defendants who were…
A study from Dartmouth found that it was, the software is no more accurate than Amazon
Mechanical Turk workers.
So random people on the Internet.
It's also the software is, you know, this proprietary black box using over 130 inputs,
and it's no more accurate than a linear classifier on three variables.
Yet it's still in use, and it's in use in many states, Wisconsin as one place where
it was challenged, yet the Wisconsin Supreme Court upheld its use.
If you're interested in the, kind of, topic of how you define fairness, because there
is a lot of intricacy here, and I mean, I don't know anybody working on this who thinks
that what COMPAS is doing is, is right, but they're using this different, different definition
of fairness.
Arvind Narayanan has a fantastic tutorial ‘...21 fairness definitions and their politics’,
that I, that I, highly recommend.
And so, going back to kind of this taxonomy of types of bias, this is an example of historical
bias, and historical bias is a fundamental structural issue, with the first step of the
data generation process, and it can exist even given perfect sampling and feature selection.
So, kind of, with the, the, image classifier that was something where we could, you know,
go gather a more representative set of images, and that would help address it, that is not
the case here.
So gathering kind of more data on the U.S. criminal justice system it's all going to
be biased, because that's really, kind of, baked into, baked into, our history and our
current state.
And so this is I think good, good to recognize.
One thing that can be done to, try to at least, mitigate this is to, to really talk to domain
experts, and by the people impacted, and so a really positive example of this is a tutorial
from the Fairness, Accountability, and Transparency Conference that Kristian Lum who's the Lead
Statistician for the Human Rights Data Analysis Group, and now professor UPenn, organized
together with a former Public Defender, Elizabeth Bender, who's the staff attorney for New York's
Legal Aid Society, and Terrence Wilkerson, an innocent man who was arrested, and cannot
afford bail.
And Elizabeth and Terrence, were able to provide a lot of insight to how the criminal justice
system works in practice which is often, kind of, very different from the, you know, more,
kind of, clean, logical abstractions that computer scientists deal with, but it's really
important to understand those kind of intricacies of how this is going to be implemented, and
used in these, you know, messy, complicated real world systems.
Question?
‘Aren’t the AI biases transferred from real life biases.
For instance, aren’t people being treated differently isn't everyday phenomenon too.’
That's correct, yes.
So this is often, yeah, coming from, from real-world biases, and I'll come to this in
a moment, but algorithmic systems can amplify those biases, so they can make them even worse,
but yeah, they are often being learned from, from existing data.
I…
I asked it because, I guess, I often see this being raised as it's, kind of, a reason not
to worry about AI.
So it’s not AI.
Well, I'm gonna get to that in a moment.
Actually, think in two slides.
So hold on to that question.
I just wanna talk about one other type of bias first.
Measurement bias.
So this was an interesting paper by Sendhil Mullainathan and Ziad Obermeyer, where they
looked at historic electronic health record data to try to determine what factors are
most predictive of stroke and they said, you know, this could be useful like prioritizing
patients at the ER.
And so they found that the number one most predictive factor was prior stroke, which,
that makes sense.
Second was, cardiovascular disease, that's also, that seems reasonable, and then third
most, kind of, still very predictive factor was accidental injury, followed by having
a benign breast lump, a colonoscopy, or sinusitis.
And so, I'm not a medical doctor, but I can tell something weird is going on with factors
three through six here.
Like, why would these things be predictive of, of stroke.
Does anyone want to think about, about why this might be?
Any guesses you want to read?
Oh someone's yeah.
Okay, the first answer was they test for it any time someone has stroke?
confirmation bias?
overfitting? is because they happen to be in hospital already?
Biased data?
EHR records these events?
Because the data was taken before certain advances in medical science?
These are, these are all good guesses.
Not, not quite what I was looking for, but good good thinking.
That's such a nice way of saying no.
So what, what the researchers say here is that this was about their patients, they are
people that utilize health care a lot and people that don't and they call it, kind of,
High Utility versus Low Utility Of Healthcare and there are a lot of factors that go into
this, I'm sure just who has health insurance and who can afford their co-pays, there may
be cultural factors, there may be racial and gender bias, there is racial and gender bias
on how people are treated.
So a lot of factors, and basically people that utilize health care a lot they will go
to a doctor when they have sinusitis and they will also go in when they're having a stroke
and people that do not utilize health care much are probably not going to go in possibly
for either and so, so what the authors write is that we haven't measured stroke which is,
you know, a region of the brain being denied, kind of, new blood and new oxygen; what we've
measured is: who had symptoms, who went to the doctor, received tests and then got this
diagnosis of stroke and, you know, that seems like it might be a reasonable proxy for, for
who had a stroke but a proxy is you know never exactly what you wanted and in many cases
that, that gap ends up being significant and so this is just one form that, that measurement
bias can take but I think it's something to really, kind of, be on the lookout for because
it can be quite subtle.
And so now starting to return to a point that was brought up earlier, aren't, aren't people
biased?
Yes.
Yes, we are and so there have been dozens and dozens, if not hundreds of studies on
this, but I'm just going to quote a few, all of which are linked to in this Sendhil Mullainathan
New York Times article if you want to find, find the studies, so this all comes from,
you know, peer-reviewed research.
But when doctors were shown identical files they were much less likely to recommend a
helpful cardiac procedure to black patients compared to white patients, and so that was
you know, same file, but just changing the race of the patient.
When bargaining for a used car, black people were offered initial prices $700 higher and
received fewer concessions.
Responding to apartment rental ads on Craigslist with a black name elicited fewer responses
than with a white name.
An all-white jury was 16 points more likely to convict a black defendant than a white
one, but when a jury had just one black member it convicted both at the same rate.
And so I share these to show that kind of no matter what type of data you're looking,
working on, whether that is Medical data or sales data or housing data or criminal justice
data that it's very likely that there's, there's bias in it.
There's a question: No, I was gonna say I find that last one really interesting, like
this kind of idea that a single black member of the jury, I guess it has some kind of like
anchoring impact, like it kind of suggests that, I'm sure you're going to talk about
diversity later, but I just want to keep this in mind that maybe even a tiny bit of diversity
here just reminds people that there's a, you know, a range of different types of people
and perspectives.
No, that's it, that's a great point yeah.
And so the question that was, kind of, asked earlier is so why does algorithmic bias matter?
Like, I have just shown you that humans are really biased too - so why are, why are we
talking about algorithmic bias?
And people have brought this up, kind of, like what's what's the fuss about it?
And there, I think algorithmic bias is a very significant, worth talking about and I'm going
to share four reasons for that.
One is the machine learning can amplify bias.
So it's not just encoding existing biases but in some cases it's making them worse and
there have been a few studies on this, one I like is from Maria De-Arteaga of CMU and
here they were, they took people's, I think job descriptions from LinkedIn, and what they
found is that Imbalances ended up being compounded and so in the group of surgeons, only 14%
were women however in the true positives, so they were trying to predict the, the job
title from the summary, women were only 11% in the true positives.
So this kind of imbalance has gotten worse.
And basically there was, kind of, this asymmetry where the, you know, the algorithm has learned
it's safer for, for women to, kind of, not, not guess surgeon.
Another, so this is one reason, another reason that algorithmic bias is a concern is that
algorithms are used very differently than human decision-makers in practice and so people
sometimes talk about them as though they are plug-and-play and are interchangeable of,
you know, with humans this bias and the algorithm is you know, this bias, why don't we just
substitute it in?
However, the, the whole system around it ends up, kind of, being different in practice.
One...
One, kind of, aspect of this is people are more likely to assume algorithms are objective
or error-free, even if they're given the option of a human override, and so if you give a
person, you know, even if you just say hey, I'm just giving the judge this recommendation,
they don't have to follow it, if it's coming from a computer many people are gonna take
that as objective.
In some cases also, there may be, you know, pressure from their boss to, you know, not
disagree with the computer more times, you know, nobody's gonna get fired by going with
the computer recommendation.
Algorithms are more likely to be implemented with no appeals process in place, and so we
saw that earlier when we were talking about recourse.
Algorithms are often used at scale.
They can be replicating an identical bias at scale.
And algorithmic systems are cheap.
And all of these, I think, are interconnected.
So in many cases, I think that algorithmic systems are being implemented, not because
they produce better outcomes for everyone, but because they're, kind of, a cheaper way
to do things at scale, you know, offering a recourse process is more expensive.
Being on the lookout for errors is more expensive.
So this is kind of cost-cutting measures, and Cathy O'Neil talks about many of these
themes in her book, ‘Weapons of Mass Destruction’, kind of, under the idea that the, ‘The privileged
are processed by people; the poor are processed by algorithms.’
There's a question?
Two questions.
Hmm.
‘This seems like an intensely deep topic, needing specialized expertise to avoid getting
it wrong.
If you were building an ML product, would you approach an academic institution for consultation
on this?
Do you see a data, product, development triad becoming a quartet, involving an ethics or
data privacy expert?’
Yes.
So I think interdisciplinary work is very important.
I would...
I would definitely focus on trying to find, kind of, domain experts on whatever your particular
domain is, who understand the intricacies of that domain, is important.
And, I think, with the, kind of, with the academic it depends.
You do want to make sure you get someone who is, kind of, applied enough to, kind of, understand
how, how, things are happening in, in, in industry.
But yeah, I think involving more people, and people from more fields is, is a good, a good
approach on the whole.
‘Someone invents and publishes a better ML technique, like attention or transformers,
and then next a graduate student demonstrates, using it to improve facial recognition by
5%, and then a small start-up publishes an app that does better facial recognition, and
then a government uses the app to study downtown walking patterns in endangered species, and
after these successes, for court-ordered monitoring, and then a repressive government then takes
that method to identify ethnicities, and then you get a genocide.
No one's made a huge ethical error at any incremental step, yet the result is horrific.
I have no doubt that Amazon will soon serve up a personally customized price for each
item that maximizes their profits.
How can such ethical creep be addressed, where the effect is remote for many small causes?’
This all...
Yeah, so that, that's a, kind of a, great summary of how, yeah, these things can happen
somewhat incrementally.
I'll talk about some tools to implement, kind of, towards the end of this lesson, that hopefully
can help us.
So some of it is, I think, we do need to get better at, kind of, trying to think a few
more steps ahead, than we have been.
You know, in particular we've seen examples of people, you know, there was this study
of how do I identify protesters in a crowd, even when they had scarves, or sunglasses,
or hats on.
You know, and when the, the, researchers on that were questioned, they were like, ‘Oh
it never even occurred to us that bad guys would use this, you know, we just thought
it would be for finding bad people’.
And so I do think, kind of, everyone should be building their ability to think a few more
steps ahead, and part of this is like it's great to do this in teams, preferably in diverse
teams, can help with that, that process.
I mean, on this question of computer vision there has been, you know, just in the last
few months, is it Joe Redmon, creator of YOLO, who has said that he's no longer working on
computer vision just because he thinks the, the misuses so far outweigh the the positives,
and Timnit Gebru said she's, she's considering that as well.
So, I think, there are, there are times where you have to consider...
And then, I think, also really actively thinking about how to, what safeguards do we need to
put in place to, kind of, address the, the misuses that are happening.
Yes?
I just wanted to say somebody really liked the Cathy O'Neil quote: ‘Privileged are
processed by people; the poor processed by algorithms’ and they're looking forward
to learning more, reading more from Kathy O'Neal.
Is there a book that you would recommend?
Yes.
Yeah.
And in…
And Kathy O'Neal also writes in the…
And Kathy O'Neill's a fellow, fellow math PhD, but she also has written a number of
good articles.
And it...
The book, kind of, goes through a number of those case studies of how algorithms are being
used in different places.
So, kind of in...
In summary of ‘Humans are biased, why do, why are we making a fuss about algorithmic
bias?’
So, one is we saw earlier.
Machine learning can create feedback loops.
So it's, you know, it's not just, kind of, observing what's happening in the world, but
it's also determining outcomes, and it's, kind of, determining what future data is.
Machine learning can amplify bias.
Algorithms and humans are used very differently in practice, and then I’ll also say technology
is power, and with that comes responsibility, and I think for, for all of us to, to have
access to deep learning, we're still in a, kind of, very fortunate and small percentage
of the world, that is able to use this technology right now, and I hope, I hope we will all
use it responsibly, and really take our power seriously.
And I just, I just noticed the time, and I think we're about to start next section on,
on analyzing or, kind of, steps, steps we can take, so this would be a good, a good,
place to take a break.
So let's meet back in seven minutes, at 7:45.
All right, let's start back up, and actually I was at a slightly different place than I
thought, but just a few questions that, that, you can ask about projects you're working
on, and I, I hope you will ask about projects you're working on.
The first is, should we, ‘should we even be doing this?’, and considering that maybe
there's some work that we shouldn't do.
There's a paper ‘When the Implication Is Not to Design (Technology)’.
As engineers we often tend to respond to problems with, you know, ‘What can I make or build
to address this?’, but sometimes the answer is to not make or build anything.
One example of research that I think has a huge amount of downside, and really no upside
I see was, kind of, to identify the ethnicity, particularly for people of ethnic minorities.
And so there was work done identifying the Chinese Uyghurs, which is the Muslim minority
in Western China, which has since, you know, over a million people have been placed in
internment camps.
And I think this is a very, very harmful, harmful line of research.
I think that the, you know, there have been at least two attempts of, building, building
a classifier to try to identify someone's sexuality, which is, it's probably, just picking
up on kind of stylistic differences, but this is something that a, could also be quite,
quite dangerous, as in many countries it's, it's illegal to be gay.
Yes.
So this is a question for me, which I don't know the answer to.
Yeah.
As that title says a Stanford scientist says he built the gaydar using ‘the lamest’
AI possible to prove a point.
And my understanding is, that point was to say, you know, I guess it's something like
‘Hey, you could use fast AI Lesson 1.
After an hour or two you can build this thing.
Anybody can do it.
You know, how do you feel about this idea that there's a role to demonstrate what's
readily available with the technology we have?
Yeah, I mean, that's something that I think...
So I appreciate that, and I'll talk about this a little bit later, OpenAI with GPT-2,
I think, was trying to raise a, raise a, debate around, around dual use and what is responsible
release of, of dual use technology, and what's a, kind of, responsible way to raise, raise
awareness of what is possible.
In the, in the cases of researchers that have done this on the sexuality question, to me
it hasn't seemed like they've put adequate thought into, how they're conducting that,
and who they're collaborating with, to ensure that it is something that is leading to, kind
of, helping address the problem, but I think you're right that, I think, there is probably
some place for letting people, yeah, know what is probably widely available now.
It reminds me a bit of my pen testing in infosec…
Yeah.
...where, where it's, kind of, considered...
Well, there's an ethical way that you can go about pointing out that it's trivially
easy to break into a company’s system.
Yes.
Yeah.
Yes.
Yeah, I would, I would agree with that, that there, there is an ethical way, but I think
that's something that we as a community, still have more work to do in even determining what
that is.
Other questions to consider are what bias is in the data and something I should highlight
is, people often ask me, you know, how can I debias my data, or ensure that its bias
free, and that's not possible.
All data contains bias, and the, kind of, most, most important thing is just to understand,
kind of, how your data set was created, and what its limitations are, so that you're not
blindsided by that bias, but you're never going to fully remove it.
And some of the, I think, most promising approaches in this area are work like, Timnit Gebru's
‘Datasheets for Datasets’, which is, kind of, going through and asking, kind of, a bunch
of questions about how your dataset was created, and for what purposes, and how it's being
maintained, and you know what are the risks in that.
Just to really kind of be aware of, of the context of your data.
Can the code and data be audited?
I think, particularly in the United States, we have a lot of issues with when private
companies are creating software that's really impacting people through the criminal justice
system, or hiring and when these things are, you know, kind of their proprietary black
boxes that are protected in court.
That, this creates a lot of kind of of issues of you know, what are what are our rights
around that?
Looking at error rates for different subgroups is really important and that's what so kind
of so powerful about Joy Buolamwini’s work.
If she had just looked at light-skinned versus dark skin and men versus women, she wouldn't
have identified just how poorly the algorithms were doing on dark-skinned women.
What is the accuracy of a simple rule-based alternative?
And this is something I think Jeremy talked about last week, which is just kind of good,
good machine learning practice, to have a baseline.
But particularly in cases like the COMPAS Recidivism, where this 130-variable black
box is not doing much better than a linear classifier on three variables.
That raises kind of a question of why are, why are we using this?
And then what processes are in place to handle appeals or mistakes, because there will be
errors in the data.
There may be bugs and the implementation, and we need to have a process for recourse.
Yes.
Can you explain this for me now?
Sorry, I’m asking my own questions, nobody voted them up at all.
What's the thinking behind this idea that a simpler model, is it that you gottta say
that a simpler model, all other things being the same, you should pick the simpler one?
Is that what this baseline’s for?
And if so, what's the kind of thinking behind that?
Well, with the COMPAS Recidivism Algorithm ... Some of this for me is linked to the proprietary
black box nature, and so you're right.
Maybe if we had a way to introspect and what were our rights around appealing something.
But I would say, yeah, like why use the more complex thing if the, the simpler one works
the same.
And then how diverse is the team that built it and I'll talk more about team diversity
later, later in this lesson.
Okay, it was Jeremy at the start, but I'm not the teacher.
So it actually is, “Jeremy, Do you think transfer learning makes this tougher, auditing
the data that led to the initial model?”
I assume they mean “Jeremy, please ask Rachel.”
No, they were, they were asking you.
That's, that's a good question.
Again, I think it's important.
I would, I would say I think it's important to have information probably on both datasets,
what the initial data set used was and what the the data set you used to fine-tune it.
Do you have thoughts on that?
What she said.
And then I'll say so while bias and fairness as well as accountability and transparency
are important, they aren't everything.
And so there's this great paper, “A Mulching Proposal” by Os Keyes, et al.
And here they talk about a system for turning the elderly into high nutrient slurry, so
this is something that it's clearly unethical but they proposed a way to do it that is fair
and accountable and transparent and meets these qualifications.
And so that kind of shows that some of the limitations of this framework, as well as
kind of being a good, a good technique for kind of inspecting whatever framework you
are using, of trying to find something that's clearly unethical that code that can meet,
meet the standards you've put forth.
That, that technique, I really like it.
It's like, it's my favorite technique from philosophy.
It's this idea that you, you say, okay, given this premise, here's what it implies.
And then you try and find an implied result which intuitively it is clearly saying.
And it's a really, it's, it's yeah, it's the number one philosophical thinking tool I got
out of university.
And sometimes we can have a lot of fun with it, like this time, too.
Thank you.
All right, so the next kind of big case study, your topic I want to discuss is disinformation.
So in 2016, in Houston a group called Heart of Texas posted about protests outside an
Islamic Center.
And they told people to come armed.
Another Facebook group posted about a counter-protest to show up supporting freedom of religion
and inclusivity.
And so there were kind of a lot of people present
at this, more people on the the side supporting freedom of religion.
And a reporter, though, for the Houston Chronicle noticed something odd -- which he was not
able to get in touch with the organizers for either side.
And It came out many months later that both sides had been organized by Russian trolls.
And so this is something where you had the people protesting were, you know, genuine
Americans kind of protesting their beliefs, but they were doing it in this way that had
been kind of completely framed very disingenuously by, by Russian operatives.
And, so when thinking about disinformation, it is not, people often think about so-called
fake news, you know and inspecting like a single post -- is this, you know, Is this
true or false?
But really disinformation is often about orchestrated campaigns of manipulation and that it involves,
kind of, all the seeds of truth, kind of the best propaganda always involves kernels of
truth, at least.
It also involves kind of misleading context and, and can, can involve very kind of sincere,
sincere people that get, get swept in it.
A report came out this fall, an investigation from Stanford's Internet Observatory, where
Renee DiResta and Alex Stamos work, of Russia's kind of most recent disinformation, or most
recently identified disinformation campaign.
And it was operating in six different countries in Africa.
It often purported to be local news sources.
It was multi-platform.
They were encouraging people to join their whatsapp and telegram groups and they were
hiring local people as reporters and a lot of, a lot of the content was not, not necessarily
disinformation.
It was stuff on culture and sports and local weather.
I mean there was a lot of kind of very pro-Russia coverage.
But then it covered a range of topics and so this is kind of a very sophisticated phase
of disinformation.
And in many cases it was hiring, hiring locals kind of as reporters to work for these sites.
And I should say well I've just given two examples of Russia.
Russia - certainly does not have a monopoly on disinformation.
There are plenty of, plenty of people involved and producing it.
Kind of on a topical topical Issue, there's been a lot of disinformation around around
Corona virus and Covid 19.
I, in terms of kind of a personal level, if you're looking for advice on spotting disinformation
or to share with loved ones about this, Mike Caulfield is a great person to follow.
… and he's even.
So he tweets @holden and then he has started an infodemic blog specifically about the about
Covid 19, but he, he talks about his approach, and how people have been trained in schools
for 12 years - here's a text, read it, use your critical thinking skills to figure out
what you think about it.
But professional fact checkers do the opposite -- they get to a page and they immediately
get off of it and look for kind of higher, higher quality sources to see if they can
find confirmation.
And Caulfield also really promotes the idea of a lot of critical thinking techniques that
have been taught take a long time, and you know, we're not going to spend 30 minutes
evaluating each tweet that we see in our Twitter stream.
It's better to give people an approach that they can do in 30 seconds that, you know,
It's not gonna be fail proof If you're just doing something for 30 seconds.
But it's better to to check, than to have something that takes 30 minutes that you're
just not going to do at all.
So I wanted to kind of put this out there as a resource.
I mean as a whole kind of set of lessons at lessons.checkplease.cc.
And he's a, he's a professor.
And I, in the data ethics course I'm teaching right now.
I made my first lesson the first half of which is kind of specifically about Corona virus
disinformation.
I've made that available on YouTube.
I've already shared it.
And so I'll add a link on the forums, If you want if you want a lot more detail on, on
disinformation than just kind of this the short bit here.
But so going back to kind of like what is disinformation?
It's important to think of it as an ecosystem again, it's not just a single post or a single
news story that, you know, it's misleading or has false elements in it.
But it's this really this broader ecosystem Claire Wartell first draft news, who is a
Leading expert on this and does a lot around kind of training journalists and how journalists
can report responsibly, talks about the trumpet of amplification and this is where rumors
or Memes or things can start on 4chan and 8chan and then move to closed messaging groups
such as WhatsApp, Telegram, Facebook Messenger.
From there to conspiracy communities on Reddit or YouTube, then to kind of more mainstream
social media and then picked up by the professional media and politicians.
And so this can make it very hard to address.
that it is this kind of multi-platform in many cases campaigns may be utilizing kind
of the differing rules or loopholes between the different platforms.
And I think we certainly are seeing more and more examples where it doesn't have to go
through all these steps But can can jump jump forward- And online discussion is very, very
significant because it helps us form our opinions, and then this is tough because I think most
of us think of ourselves as pretty independent-minded, but discussion really does you know we evolved
as kind of social beings and to be influenced by by people in our in-group and in opposition
to people in our out-group and so online discussion impacts us.
People discuss all sorts of things online: here's a Reddit discussion about whether the
US should cut defense spending, and you have comments, you're wrong and the defense budget
is a good example of how badly the U.S. spends money on the military, and someone else says
yeah, but that's already happening, here's a huge increase in the military budget, the
Pentagon budgets already increasing.
I didn't mean to sound like, stop paying for the military, I'm not saying that we cannot
pay the bills, but I think it would make sense to cut defense spending.
Does anyone want to guess what subreddit this is from?
unpopularopinion, news, changemyview, netneutrality.
These are good guesses but they're wrong.
I love the way you say, though.
This is all from what it is.
It's from the sub simulatorgpt2 oh, so these comments are all written by GPT-2, and this
is in good fun.
It was clearly labeled on the subreddit, that it's coming in GPT-2 is a language model from
OpenAI.
That was kind of in a trajectory of research that many, many groups were on, and so it
was released I guess about a year ago and, should I read the unicorn story Jeremy?
Okay.
So many of you have probably have probably seen: this this was cherry-picked, but this
is still very, very impressive.
So human written prompt was given to the Language Model: “in a shocking finding, scientists
discovered a herd of unicorns living in a remote previously unexplored valley in the
Andes mountains.
Even more surprising to the researchers was the fact that the unicorn spoke perfect English”.
And then the next part is all generated by the language model.
So this is a deep learning model that produced this, and the computer model generated “dr.
Jorge Perez found what appeared to be a natural fountain surrounded by two peaks of rock and
silver snow.
Perez and the others then ventured further into the valley.
By the time we reached the top of one peak, the water looked blue, with some crystals
on tops.
Perez and his friends were astonished to see the Unicorn herd.
These creatures could be seen from the air without having to move too much to see them.
They were so close, they could touch their horns.
While examining these bizarre creatures, the scientists discovered that the creatures also
spoke some fairly regular English.
Perez stated we can see for example that they have a common language, something like a dialect
or dialectic.”
And so I think this is really compelling prose, to have been generated by a computer in this
form.
So we've also seen advances in computers generating pictures as specifically GANs.
So Katie Jones was listed on LinkedIn as a Russia and Eurasia fellow, she was connected
to several people from mainstream Washington think-tanks, and The Associated Press discovered
that she is not a real person.
This photo was generated by a GAN.
And so this, I think it's kind of scary, when we start thinking about how compelling the
text that's being generated is, and combining that with pictures, these photos are all from
thispersondoesnotexist.com, generated by GANs.
And there's a very, very real and imminent risk that online discussion will be swamped
with fake manipulative agents, to an even greater extent than it already has.
And this can be used to influence public opinion.
So.
oh, actually this is...well, I’ll keep going.
So, going back in time to 2017, the FCC was considering repealing net neutrality
and so they opened up for comments to see, “How do Americans feel about net neutrality?”
and this is a sample of many of the comments that were opposed to net neutrality.
They wanted to repeal it, and included...
I'll just read a few clips.
“Americans as opposed to Washington bureaucrats deserve to enjoy the services they desire.”
“Individual citizens as opposed to Washington bureaucrats should be able to select whichever
services they desire.”
“People like me as opposed to so-called experts should be free to buy whatever products
they choose.”
And these have been helpfully color-coded so you can see a pattern: that this was a
bit of a Mad Libs, where you had a few choices (for green) for the first noun, and then in
orange or red--I guess--it's “as opposed to” or “rather than.”
Orange: we've got either “Washington bureaucrats,” “so-called experts,” “the FCC,” and
so on.
This analysis was done by Jeff Kao who's now a computational journalist at ProPublica doing
great work, and he did this analysis discovering this campaign, in which these comments were
designed to look unique but had been created through some mail-merge-style, kind of putting
together, Mad Libs.
Yes?
So this was great work by Jeff.
He found that..
So while the FCC received over 22 million comments, less than 4% of them were truly
unique.
This is not all malicious activity, there are many ways where you get a template to
contact your legislator about something.
But in the example shown previously, these were designed to look like they were unique
when they weren't.
More than 99% of the truly unique comments wanted to keep net neutrality; however, that
was not not the case if you looked at the full 22 million comments.
However, this was in 2017, which may not sound that long ago, but in in the field of natural
language processing we've had an entire revolution since then--there's just been so much progress
made.
And this would be (I think) virtually impossible to catch today, if someone was using a sophisticated
language model to generate comments.
So Jess asks a question, which I'm gonna treat as a two-part question I think it's not necessarily.
What happens when there's so much AI trolling that most of what gets straight from the web
is AI generated text?
And then the second part: And then what happens when you use that to generate more AI generated
text?
Yes, for the first part...
Yeah, this is a real risk, or not “risk,” but kind of challenge we're facing of real
humans can get drowned out when so much text is gonna be AI trolling.
We're already seeing, and I (in the interest of time I can talk about disinformation for
hours and I had to cut a lot of stuff out) but many people have talked about how the
new form of censorship is about drowning people out.
So it's not necessarily forbidding someone from saying something but just totally, totally
just drowning them out with the massive quantity of text and information and comments.
And AI can really facilitate that, and so I do not have a good solution to that.
In terms of AI learning from AI text, I mean, I think you're gonna get systems that are
potentially less and less relevant to humans, and may have harmful effects if they're being
used to create software that is interacting with or impacting humans, so that's a concern.
I mean one of the things I find fascinating about this is: we could get to a point where
99.99% of tweets and fastai forum posts, and whatever, are auto-generated.
Particularly on more like political-type places where a lot of it's pretty low content, pretty
basic.
The thing is, like if it was actually good: you wouldn't even know!
So what if I told you that 75% of the people you're talking to on the forum right now are
actually bots?
How can you tell which ones they are?
How would you prove whether I'm right or wrong?
Yeah, I think this is a real issue on Twitter.
Particularly people you don't know of, wondering like is this an actual person or a bot?
I think it's a common question people wonder about and can be hard to tell.
But, I think it has significance for- has a lot of significance for- kind of how human
government works, you know.
I think there's something about humans being in society and having norms and rules and
mechanisms that this can really undermine and make difficult.
So, when GPT2 came out, Jeremy Howard, co-founder of fastai, was quoted in the Verge article
on it, “I've been trying to warn people about this for a while.
We have the technology to totally fill twitter, email, and the web up with reasonable sounding,
context appropriate prose, which would drown out all other speech and be impossible to
filter.”
So, one kind of step towards addressing this is the need for digital signatures.
Oren Etzioni, the head of the Allen Institute on AI, wrote about this in HBR.
He wrote, “Recent developments in AI point to an age where forgery of documents, pictures,
audio recordings, videos, and online identities will occur with unprecedented ease.
AI is poised to make high fidelity forgery inexpensive and automated, leading to potentially
disastrous consequences for democracy, security, and society.”
and proposes kind of digital signatures as a means for authentication.
And, I will say here kind of one of the additional risks of kind of all this forgery and fakes
is that it also undermines people speaking the truth.
And, Zeynep Tufekci, who does a lot of research on protests around the world and in different
social movements, has said that she's often approached by kind of whistleblowers and dissidents
who in many cases will risk their lives to try to publicize like a wrongdoing or human
rights violation only to have kind of bad actors say, “oh, that picture was photoshopped-
that was faked” and that it's kind of now this big issue for for whistleblowers and
dissidents of how can they verify what they are saying and that kind of need for verification.
And then someone you should definitely be following on this topic is Renee DiResta and
she wrote a great article with Mike Godwin last year framing that we really need to think
disinformation as a cybersecurity problem, you know.
It sees kind of coordinated campaigns of manipulation and bad actors and there's, I think, some
important work happening at Stanford, as well on this.
Alright, questions on disinformation?
Okay, so our next step Ethical Foundations.
So, now the fastai approach we always like to kind of ground everything in what are the
real-world case studies before we get to kind of the theory underpinning it- and I'm not
going to go too deep on this at all.
And, so there is a fun article, “what would an Avenger do?”
And, hat tip to Casey Fiesler for suggesting this.
And, it goes through kind of three common ethical philosophies: Utilitarianism and gives
the example of Iron Man.
I'm trying to match good Deontological Ethics of Captain America being an example of this
adhering to the right.
And, then Virtue Ethics, Thor living by a code of honor and so I thought that was a
nice reading.
Rachel: Yes?
Question: Where do you stand on the argument that social media companies are just neutral
platforms and that problematic content is the entire responsibility of the users just
the same way that phone companies aren't held responsible when phones are used for scams
or car companies held responsible when vehicles are used for, say, terrorist attacks?
Rachel: So, I do not think that the platforms are neutral because they make a number of
design decisions and enforcement decisions around even kind of what their Terms of Service
are and how those are enforced.
And, keeping in mind harassment can drive many people off of platforms and so kind of
many of those decisions is not that “Oh, everybody gets to keep free speech when there's
no enforcement”.
It's just changing kind of who is silenced.
I do think that there are a lot of really difficult questions that are raised about
this because I also think that the platforms, you know, they're not publishers.
But, they are in this I think kind of a intermediate area where they are performing many of the
functions that publishers used to perform.
So, you know like a newspaper would be, which is curating which articles are in it, which
is not what platforms are doing, but they are getting closer closer to that.
I mean, something I come back to is that it is an uncomfortable amount of power for private
companies to have.
Yeah, and so it does raise a lot of difficult decisions
But I, I do not believe that they are they are neutral.
So, for for this part I mentioned the Markkula Center earlier, definitely check out their
site, Ethics in Technology Practice.
They have a lot of useful, useful resources.
And I'm gonna go through these relatively quickly as just kind of examples.
So they give some kind of deontological questions that technologists could ask.
And so deontological ethics or where you kind of have various kind of rights or duties that
you might want to respect.
And this can include principles like privacy or autonomy.
How might the dignity and autonomy of each stakeholder be impacted by this project?
What considerations of trust and of justice are relevant?
Does this project involve any conflicting moral duties to others?
In some cases, you know, there'll be a kind of conflict between different, different rights
or duties you're considering.
And so this is, this is kind of an example, and they have more, more in the reading of
the types of questions you could be asking, kind of when evaluating, of just, even how
do you evaluate, kind of whether, whether a project is ethical.
Consequentialist questions -- who will be directly affected, who will be indirectly
affected?
Will the (and consequentialist includes utilitarianism as well as common good), will the effects
in aggregate create more good than harm, and what types of good and harm?
Are you thinking about all the relevant types of harm and benefits including psychological,
political, environmental, moral, cognitive, emotional, institutional, cultural?
Also looking at long term, long term benefits and harms, and then who experiences them?
Is this something where the risk of the harm are going to fall disproportionately on the
least powerful?
Who's going to be the ones to accrue the benefits?
Have you considered dual use?
And so these are, these are again kind of questions you could use when trying to, trying
to evaluate a project.
And I think, and the recommendation of the Markkula Center is that this is a great activity
to kind of to be doing as a team and as a group.
Yes?
I was gonna say like I can't, I can't overstate how useful this tool is.
Like it, you know, I think, “Oh, it's just a list of questions.”
Yeah, you know, but like this is kind of to me, this is that this is the big gun tool
for for how you, how you handle this.
It’s like if somebody is helping you think about the right set of questions and then
you let go through them with a diverse group of people and discuss the questions.
I mean that’s, this is, this is gold.
Like don't, you know, go back and reread these.
And don't, don't just skip over them.
Take them to work.
Use them next time you're talking about a project.
They're a really great, great set of questions to use.
A great tool in your toolbox.
And go to the original reading, has even kind of more detail and more elaboration on the
questions.
And then they kind of give a summary of five potential ethical lenses.
The, the rights approach -- which option in best respects the rights of all who have a
stake?
The justice approach -- which option treats people equally or proportionally?
And so these two are both deontological.
The utilitarian approach -- which option will produce the most good and do the least harm?
The common good approach -- which option best serves the community as a whole, not just
some members?
And so here three and four are both a consequentialist.
And then -- virtue approach, which option leads me to act as the sort of person I want
to be and that can involve, you know particular virtues of you know, do you value trustworthiness
or truth or courage?
And so I mean a great activity if this is something that you're studying or talking
about at work with your teammates, the, the Markkula Center has a number of case studies
that you can talk through and will even ask you to kind of evaluate them, you know evaluate
them through these five lenses.
And how does that kind of impact your, your take on what the what the right thing to do
is.
It's kind of weird for a programmer or a computer programmer or data scientist in some way in
some ways to like think of these as tools like fast.ai or pandas, or whatever, but I
mean they absolutely are.
This is like, these are like software tools for your brain, you know, to help you kind
of go through a program that might help you debug your thinking.
Great.
Thank you.
And then as someone brought up earlier, so that was a kind of very Western centric intro
to ethical philosophy there are other ethical lenses and other cultures.
And I've been doing some reading particularly on the, the Maori worldview.
I don't feel confident enough in my understanding that I could represent it, but it is very
good to be mindful that there are other other ethical lenses out there
and I do very much think that, you know, the people being impacted by a technology like,
their, their ethical lens is kind of what matters.
And that, this is, is a particular issue and we have so many kind of multinational corporations.
There's an interesting project going on in New Zealand now where the New Zealand government
is kind of considering its AI approach and is at least ostensibly kind of wanting to,
wanting to include the Maori view on that.
So that's a that's kind of a little, a little bit of theory.
But now I want to talk about some kind of practices you can implement in the workplace.
Again, this is from the Markkula Center.
So this is their ethics toolkit, which I particularly like.
And I'm just, I'm not going to go through all of them, I'm just going to tell you a
few of my favorites.
So, Tool 1 is Ethical Risk Sweeping and this I think is similar to the idea of, kind of
pen testing (that Jeremy mentioned earlier from security), but to have regularly-scheduled
ethical risk sweeps.
And while no vulnerability, vulnerabilities found is generally good news, that doesn't
mean that it was a wasted effort.
And you keep doing it, keep looking for, for ethical risk.
One moment.
And then assume that you missed some risk in the initial project development.
Also, you have to set up the incentives properly where you're rewarding team members for spotting
new ethical risk.
All right.
So got some comments here.
So my comment here is about the learning rate finder, and I'm not going to bother with the
exact mathematical definition (partly because I'm a terrible mathematician and partly because
it doesn't matter), but if you just remember, oh, sorry, that's actually not me, I am just
reading something that Patty Hendrix has trained a language model of me.
So that was me greeting the language model of me, not the real me.
Thank you.
This is a Tool 1.
I would say another kind of example of this, I think it's like red teaming of, you know,
having a team within your org that’s kind of trying to find your vulnerabilities.
Tool 3, another one I really like, Expanding the Ethical Circle.
So whose interests, desires, skills, experiences and values have we just assumed rather than
actually consulted?
Who are all the stakeholders who will be directly affected?
And have we actually asked them what their interests are?
Who might use this product that we didn't expect to use it, or for purposes that we
didn't initially intend?
And so then a great implementation of this comes from the University of Washington's
Tech Policy Lab did a project called Diverse Voices.
And it's neat, they have both a academic paper on it and then they also kind of have like
a guide, lengthy guide on how you would implement this.
But the idea is how to kind of organize expert panels around, around new technology, and
so they, they did a few samples.
One was they were considering augmented reality and they held expert panels with people with
disabilities, people who are formerly are currently incarcerated, and with women to
get their get their input and make sure that that was included.
They did a second one on an autonomous vehicle strategy document and organized expert panels
with youth, with people that don't drive cars, and with extremely low-income people.
And so I think this is a great guide if you're kind of unsure of how do you even go about
setting something like this up to expand your circle, include, include more people and get,
get perspectives that may be underrepresented by your employees.
So I just want to let you know that this resource is out there.
Tool 6 is Think About the Terrible People.
And, and this can be hard because I think we're often, you know, thinking kind of positively,
or thinking about people like ourselves who don't have terrible intentions.
But really, think about who might want to abuse, steal, misinterpret, hack, destroy,
or weaponize what we build?
Who will use it with alarming stupidity or irrationality?
What rewards, incentives, openings, has our design inadvertently created for those people?
And so kind of remembering back to the section on metrics, you know, how are people going
to be trying to game or manipulate this?
And how can how can we then remove those rewards or incentives?
And so this is, this is an important kind of important step to take.
And then, Tool 7 is Closing the Loop, Ethical
Feedback and Iteration, remembering this is never a finished task and identifying feedback
channels that will give you kind of reliable data and integrating this process with quality
management and user support and developing formal procedures and chains of responsibility
for ethical iteration.
And this tool reminded me of a blog post by Alex Feerst that I really like.
Alex Feerst was previously the Chief Legal Officer at Medium.
And, I guess this was a year ago, he interviewed or something like 15 or 20 people that have
worked in trust and safety.
And trust and safety includes content moderation, although it's not not solely content moderation.
And kind of one of the ideas that came up that I really liked was one of, one of the
people .. and so many of these people have worked in Trust and Safety for years at big-name
companies and one of them said, “The separation of product people and trust people worries
me, because in a world where product managers and engineers and visionaries cared about
the stuff, it would be baked into how things get built.
If things stay this way -- that product and engineering are Mozart and everyone else is
Alfred the butler -- the big stuff is not going to change.”
And so, I think at least two people in this kind of talk about this idea of needing to
better integrate trust and safety, which are often kind of on the front lines of seeing
abuse and misuse of a technology product.
Integrating that more closely with product and engs so that it can kind of be more directly
incorporated and you can have a tighter feedback loop there -- about what's going wrong, and
and how, how that can be designed against.
Okay, so those were, these were, well, I link to a few blog posts and research I thought
relevant, but inspired by the Markkula Center’s tools for tech ethics and hopefully those
are practices you could think about potentially implementing at your, at your company.
So next I want to get into diversity, which I know came up earlier.
And so only 12% of Machine Learning researchers are women.
This is kind of a very, very dire statistic.
There's also a kind of extreme lack of racial diversity and age diversity and other factors,
and this is, this is significant.
A kind of positive example of what diversity can help with, and a post Tracy Chou, who
was an early, early an engineer at Quora and later at Pinterest, wrote that the first feature
(and so I think she was like one of the first five employees at Quora), “The first feature
I built when I worked at Quora was the block button.
I was eager to work on the feature because I personally felt antagonized and abused on
the site,” and she goes on to say that If she hadn't been there, you know they might
not have added the the block button as soon.
And so that's kind of like a direct example of how, how having a diverse team can help.
So my kind of key, key advice for anyone wanting to increase diversity is to start at the opposite
end of the pipeline from, from where people talk about the, the workplace.
I wrote a blog post five years ago, “If you think women in tech is just a pipeline
problem, you haven't been paying attention.”
And this was the most popular thing I had ever written until Jeremy and I wrote the,
the Covid 19 post last month, so the second most, most popular thing I've written.
But I linked to a ton of, ton of research in there.
A key statistic to understand is that 41% of women working in tech end up leaving the
field, compared to 17% of men.
And so this is something that recruiting more girls into, into coding or tech is not going
to address this problem, if they keep leaving at very high rates.
I just had a little peek at the YouTube chat, and I see people are asking questions there.
I just want to remind people that we are not, that Rachel and I do not look at that.
If you want to ask, ask questions, you should use the forum thread and, and if you see questions
that you like, then please put them up, such as this one, “How about an ethical issue
bounty program just like the bug bounty programs that some companies have?”
You know, I think that's a neat idea you have, rewarding people for, for finding ethical
issues.
And so the, the reason that women are more likely to leave tech Is and this was found
in a meta-analysis of over 200 books, white papers articles -- women leave the tech industry
because they're treated unfairly, underpaid, less likely to be fast-tracked than their
male colleagues and unable to advance.
And, and too often, diversity efforts end up just focusing on white women, which is
wrong.
Interviews with 60 women of color who work in stem research found that 100 percent had
experienced discrimination and their particular stereotypes varied by race.
And so it's very important to focus on women of color in, in diversity efforts as a, kind
of the top priority.
A study found that men's voices are perceived as more persuasive, fact-based, and logical
than women's voices, even when reading identical scripts Researchers found that women receive
more of a feedback and personality criticism and performance evaluations, whereas men are
more likely to receive actionable advice tied to concrete business outcomes.
When women receive mentorship, it’s often advice on how they should change and gain
more self-knowledge.
When men receive mentorship, it's public endorsement of their authority.
Only one of these has been statistically linked to getting promoted, it's the public endorsement
of authority.
And all these studies are linked to another post I wrote called, “The Real Reason Women
Quit Tech”, and how to address it.
Is that a question, Jeremy?
Yeah, so if you're interested, kind of, these two blog posts I link to a ton of ton of relevant
research on this.
And I think this is kind of the the workplace, is the the place to start in addressing these
things.
So another issue is tech interviews are terrible for everyone.
So now kind of working one step back from people that are already in your in your workplace,
but thinking about the interview process.
And they wrote a post on how to make tech interviews a little less awful, and went through
a ton of research.
And I will, I will say that the the interview problem, I think is a hard one.
I think it's very time consuming and hard to to interview people well.
But kind of the two most interesting pieces of research I came across -- one was from
Triplebyte, which is a recruiting company that interviews, kind of does this first-round
technical interview for people and then they interview at Y Combinator (it's a Y Combinator
company), and they interview at Y Combinator companies.
And they have this very interesting data set where they've kind of given everybody the
same technical interview and then they can see which companies people got offers from
when they were, you know interviewing at many of the same companies.
And the number one finding from their research is that the types of programmers that each
company looks for often have little to do with what the company needs or does, rather
they reflect company culture and the backgrounds of the founders And this is something where
they even, they even gave the advice that if you're job hunting, look for, try to look
for companies where the founders have a similar background to you.
And that's something that while I, that makes sense that's going to be much easier for certain
people to do than others.
And particularly, given the, the gender and racial disparities in VC funding, that's gonna
make a big difference.
Yes.
Actually, I would say that was the most common advice I heard from VCs when I became a founder
in the Bay Area was when recruiting focus on getting people from your network, and people
that are as like-minded and similar as possible.
That was by far the most common advice that I heard.
Yeah, I mean this is one of my controversial opinions -- I do feel like ultimately, like
I get why people hire from their network, and I think that long term, we all need to
be developed.
Well particularly white people need to be developing more diverse networks.
And that's like a, you know, like ten-year project.
That's not something you can do right when you're hiring, but really kind of developing
a diverse network of friends and trusted acquaintances, a kind of over time.
But, yeah, thank you for that perspective to Jeremy.
And then kind of the other study.
I found really interesting, was one where they, they gave people resumes.
And in one case, so one resume had more academic qualifications, and then one had more practical
experience.
And then they switched the gender -- one was a woman, one was a man (or you know male name
and a female name).
And basically people were more likely to hire the male and then they would use a post-hoc
justification of, “Oh, well, I chose him because he had more academic experience, or
I chose him because he had more practical experience.”
And that's something, I think it's very human to use post hoc justifications, but it's a,
it's a real risk that definitely shows up in hiring.
Ultimately, AI, or any other technology I developed or implemented by companies for
financial advantage, i.e., more profit, maybe the best way to incentivize ethical behavior
is to tie financial or reputational risk to good behavior.
In some ways, similar to how companies are now investing in cybersecurity because they
don't want to be the next Equifax.
Can grassroots campaigns help in better ethical behavior with regards to the use of AI.
Oh, that's a good question.
Yeah, and I think there are a lot of analogies with cybersecurity and I know that for a long
time I think was hard for people to make, or people had trouble making the case to their
bosses of why they should be investing in cybersecurity.
Particularly because cybersecurity is you know, something like when it's working, well,
you don't notice it.
And so that can be, can be hard to build the case.
So I think that there, there is a place for grassroots campaigns.
And I'm gonna talk more, I'm gonna talk about policy in a bit.
It can be hard in in some of these cases where there are not necessarily meaningful alternatives
So I do think like monopolies can kind of, kind of make that harder.
That's it, yeah, a good good question.
All right, so next step...
Actually on this slide is the, the need for policy.
And so I'm gonna start with a case study of what, what's one thing that gets companies
to take action?
And so as I mentioned earlier, an investigator for the UN found that Facebook played a determining
role in the Rohingya genocide.
I think the best article I've read on this was by Timothy McLaughlin, who did a super
super in-depth dive on Facebook's role in Myanmar.
And people people warned Facebook executives in 2013, and in 2014, and in 2015, how the
platform was being used to spread hate speech and to incite violence.
One person in 2015 even told Facebook executives that Facebook could play the same role in
Myanmar that the radio broadcast played during the Rwandan genocide, and radio broadcasts
played a very terrible and kind of pivotal role in the Rwandan genocide.
Somebody close to it, this said that's not 20/20 hindsight, the scale of this problem
was significant and it was already apparent.
And despite this in 2015, I believe Facebook only had four contractors who even spoke Burmese,
the language of the of Myanmar.
Question?
That's an interesting one.
How do you think about our opportunity to correct biases in artificial systems, versus
the behaviors we see in humans?
For example, a sentencing algorithm can be monitored and adjusted, versus a specific
biased judge who remains in their role for a long time.
I mean well, theoretically, though … I think I feel a bit hesitant about the it's, it'll
be easier to correct bias in algorithms because I feel like the … you still need people
kind of making the decisions to prioritize that.
Like, it requires kind of an overhaul of the systems priorities, I think.
It also starts with the premise that there are people who can't be fired, or disciplined,
or whatever.
Which I guess, maybe for some judges that's true, but tha kind of maybe suggests that
judges shouldn't be lifetime appointments.
Yeah, even then I think you kind of need the, the change of heart of the people advocating
for the new system, which I think can, would be necessary in either case, kind of.
And that, that's kind of the critical piece of getting the, the people that are wanting
to overhaul the values of a system.
So returning to, to this issue of the Rohinga genocide.
And this is a kind of continuing, continuing issue.
Yeah, this is something that's just kind of really stunning to me that, that there were
so many warnings, and that so many people tried to raise an alarm on this, and that
so little action was taken.
And even, this was last year, Zuckerberg finally said that Facebook would add, or maybe, maybe
this was actually, this was probably two years ago, said that Facebook would add, but this
is you know after genocides already happening, Facebook would add dozens of Burmese language
content reviewers.
So in contrast, wo we have this, this is how Facebook really failed to respond in any any
significant way in Myanmar, Germany passed a much stricter law about hate speech and
NetzDG, and the the potential penalty would be up to like 50 million euros.
Facebook hired 1200 people in under a year because they were so worried about this penalty.
And so, and I'm not saying like this is a law we want to replicate here.
I'm just illustrating the difference between being told that you're contributing, or playing
a determining role in a genocide versus a significant financial penalty.
We have seen what the one thing that makes Facebook take action is.
And so I think that that is really significant in remembering what the, what the power of
a credible threat of a significant fine is.
And it has to be a lot more than you know, just like a cost of doing business
So, I really believe that we need both policy and ethical behavior within industry.
I think that policy is the appropriate tool for addressing negative externalities, misaligned
economic incentives, race to the bottom situations, and enforcing accountability.
However, ethical behavior of Individuals, and of data scientists, and software engineers
working in industry, is very much necessary as well.
Because, the law is not always going to keep up.
It's not going to cover all the edge cases.
We really need the people in industry to be making kind of ethical ethical decisions as
well.
And so I believe both are significant and important.
And then, something to note here is that, many, many examples of kind of AI ethics issues,
and I haven't talked about all of these, but there was Amazon's facial recognition the
ACLU did a study finding that it incorrectly matched 28 members of Congress to criminal
mug shots and this disproportionately included Congress people of color.
And there's also, this was a terrible article.
The article was good, but the story is terrible, of a City that's using this IBM dashboard
for predictive policing and a city official said, oh like whenever you have machine learning
it's always 99% accurate, which is false, and quite concerning.
We had we had the issue in, so in 2016, ProPublica discovered that you could place a housing
ad on Facebook and say “I don't want Latino or black people”, or “I don't want wheelchair
users to see this housing ad”, which seems like a violation of the the Fair Housing Act.
And so there's this article, and Facebook was like we're so sorry, and then over a year
later It was still going and ProPublica went back and wrote another article about it.
There's also this issue of dozens of companies were Placing ads on facebook, job ads and
saying, “We only want young people to see this”.
There's Amazon creating the recruiting tool that penalized resumes that had the word “women's”
in it.
And so something to note about these examples, and many of the examples we've talked about
today, is that many of these are about human rights and civil rights.
It's a good article by Dominique Harrison of the Aspen Institute on this.
And I kind of agree with Anil Dash’s framing.
I mean, he wrote, “There is no technology industry anymore, Tech is being used in every
industry”.
And so I think in particular, we need to consider human rights and civil rights such as housing,
education, employment, criminal justice, voting, and medical care, and think about what rights
we want to safeguard and I do think policy is the appropriate way to do that.
And I think, I mean, it's very easy to be discouraged about regulation, but I think
sometimes we overlook the positive, or the cases where it's worked well.
And so something I really liked about datasheets for data sets by Timnit Gebru et al., is that
they go through three case studies of how standardization and regulation came to different
industries.
And so the electronics industry, around circuits and resistors, and so there that's kind of
around the standardization, of you know, what the specs are and what you write down about
them.
And the pharmaceutical industry, and car safety, and none of these are perfect, but it's still,
it was a kind of, very illuminating, the case studies there.
I mean, in particular I got very interested in the car safety one, and there's also a
great 99% invisible episode, this is a design podcast about it.
And so some things I learned is that early cars had sharp metal knobs on the dashboard
that could lodge in people's skulls in a crash.
Non-collapsible steering columns would frequently impale drivers and then even after the collapsible
steering column was invented, it wasn't actually implemented because there was no economic
incentive to do so.
But it's the collapsible steering column that has saved more lives than anything other than
the seatbelt, when it comes to car safety.
And there was also this widespread belief that cars were dangerous because of the people
driving them and it took, it took consumer safety advocates decades to just even change
the culture of discussion around this and to start kind of gathering and tracking the
data and to put more of an onus on car companies around safety.
GM hired a private detective to trail Ralph Nader and try to dig up dirt on him.
And so this was really a battle that we kind of, I take for granted now, and and so kind
of shows how much how much it can take to to change, change the needle there.
And then, kind of a more recent Issue is that it wasn't until I believe 2011 that it was
required that crash test dummies start representing the average female anatomy, in addition to
…. previously was kind of just crash test dummies were just like men.
And that in a crash with the same impact, women were 40% more likely to be injured than
men, because that's kind of who the cars were being designed for.
So I thought, I thought all this was very interesting and it can be helpful to kind
of remember, and remember some of the successes we've had.
And another area that's very relevant is environmental protections.
And kind of looking back and Maciej Ceglowski has a great article on this.
But you know just remembering like in the US, we used to have rivers that would catch
on fire, and London had terrible terrible smog, and that these are things that were,
you know were very, would not have been possible to kind of solve as an individual.
We really needed kind of coordinated, coordinated regulation.
All right, is then on a kind of closing note.
So I think a lot of the problems I've touched on tonight are really huge, huge and difficult
problems and they're often kind of very complicated and I .. Well, I go into more detail on this
in the course, so please, please check out the course once it's released.
I always try to offer some like steps towards solutions, but I realize they're not they're
not always.
you know.
as satisfying as I would like of like this is gonna solve it and that's cuz these are
really really difficult problems.
And Julia Angwin, a former journalist from ProPublica, and now the editor in chief of
The Markup, gave a really great interview on privacy last year that I liked and found
very encouraging.
She said, “I strongly believe that in order to solve a problem, you have to diagnose it
and that we're still in the diagnosis phase of this.
If you think about the turn of the century and industrialization, we had, I don't know,
30 years of child labor, unlimited work hours, terrible working conditions, and it took a
lot of journalists muckraking and advocacy to diagnose the problem, and have some understanding
of what it was and then the activism to get laws changed.
I see my role as trying to make as clear as possible what the downsides are and diagnosing
them really accurately so that they can be solvable.
That's hard work and lots more people need to be doing it.”
I found that really encouraging and that I do, I do think we should be working towards
solutions.
But I think just at this point, even better, diagnosing and understanding kind of the complex
problems we're facing is valuable work.
A couple of people are very keen to see your full course on ethics.
Is that something that they might be able to attend or buy or something?
So it will be released for free at some point this summer.
And it was, there was a paid in-person version and offered at the Data Institute as a certificate
kind of.
Similar to how this, this course was supposed to be offered, you know, in person.
The data ethics one was in-person, and that took place in January and February.
And then I'm currently teaching a version, version for the Masters of Data Science students
at USF, and I will be releasing the free online version and later, sometime before July.
Hi everybody and welcome to Lesson 6, where we're going to continue looking at training
convolutional neural networks for computer vision, and so we last looked at this the
lesson before last, and specifically we were looking at how to train an image classifier
to pick out breeds of pet, one of 37 breeds of pet, and we've gotten as far as training
a model. We also had to look and figure out what loss function was actually being used
in this model. And so we talked about Cross-entropy loss, which is actually a really important
concept and some of the things we'll talk about today depend a bit on you understanding
this concept. So if you were at all unsure about where we got to with that, go back and
have another look. Have a look at the questionnaire particular, and make sure that you're comfortable
with Cross-entropy loss, if you're not you may want to go back to the 04_mnist_basics
notebook, and remind yourself about MNIST loss, because it's very, very similar. That's
what we have built on, to build-up Cross-entropy loss. So having trained our model, the next
thing we're going to do is look at model interpretation. There's not much point having a model if you
don't see what it's doing and one thing we can do is use a confusion matrix, which in
this case is not terribly helpful. There's a kind of a few too many, and it's not too
bad, we can kind of see some colored areas, and so this diagonal here all the ones that
are classified correctly. So for Persians there were 31 classified as Persians, but
we can see there's some bigger numbers here, like a Siamese, 6 were misclassified they
actually continued, considered a Birman, but for when you've got a lot of classes like
this it might be better, instead, to use the �most_confused� method and that tells
you the combinations which it got wrong the most often. In other words, which numbers
are the biggest. So actually here's the biggest one, 10, and that's confusing, an American
Pitbull Terrier or a Staffordshire Bull Terrier. That's happened 10 times and a Ragdoll is
getting confused with a Birman 8 times. And so I'm not a dog or cat expert, and so I don't
know what this stuff means, so I looked it up on the internet, and I found that American
Pit Bull Terriers and Staffordshire Bull Terriers are almost identical, that, I think, they
sometimes have a slightly different colored nose, if I remember correctly, and Ragdolls
and Birman's are types of cat that are so similar to each other that there's whole long
threads on cat lover forums about �Is this a Ragdoll?� or �Is this Birman?�, and
experts disagreeing with each other. So, no surprise that these things are getting confused.
So when you see your model making sensible mistakes, the kind of mistakes that humans
make, that's a pretty good sign, that it's picking up the right kind of stuff, and that
the kinds of errors you're getting also might be pretty tricky to fix, but, you know, let's
see if we can make it better. And one way to try and make it better is to improve our
learning rate. Why would we want to improve the learning rate? Well, one thing we'd like
to do is to try to train it faster. Get more done in less epochs, and so one way to do
that would be to call our fine-tuned method with a higher learning rate. So last time
we used the default, which, I think, is 1e neg 2 (0.002),, and so if we pump that up
to 0.1, it's going to jump further each time. So remember the learning rate, if you've forgotten
this, have a look again at Notebook 4, that's the thing we multiply the gradients by, to
decide how far to step, and unfortunately when we use this higher learning rate, the
error rate goes from 0.08 in three epochs, to 0.082, so we're getting the vast majority
of them wrong now. So that's not a good sign. So, why did that happen? Well, what happened
it's rather than this gradual move towards the minimum, we had this thing where we step
too far, and we get further further away. So when do you see this happening. which locks
in practice like this, your error rate getting worse right from the start, that's a sign
your learning rate is too high. So we need to find something just right, not too small
that we take tiny jumps and it takes forever, and not too big that we, you know, either
get worse and worse, or we just jump backwards and forwards quite slowly.
So to find a good learning rate we can use something that the researcher Leslie Smith
came up with called �The Learning Rate Finder�, and �The Learning Rate Finder� is pretty
simple. All we do... Remember, when we do Stochastic Gradient Descent, we look at one
mini batch at a time, there are a few images, in this case, at a time, find the gradient
for that set of images for the mini-batch, and jump, step our weights based on the learning
rate and the gradient. Well what Leslie Smith said was �OK let's do the very first mini
batch at a really really low learning rate, like 10 to the minus 7, and then let's increase
by a little bit there like maybe 25% higher and do another step, and then 25 percent higher
and to another step�. So these are not epochs, these are just a single, a similar mini batch,
and then we can plot on this chart here. �OK, at 10 to the minus 7, what was the loss?�,
and, �At 25% higher than that what was the loss?�, and �At 25% higher than that what
was the loss?�. And so, not surprisingly, if you do that at the low learning rates,
the loss doesn't really come down, because the learning rate is so small that these steps
are tiny, tiny, tiny, and then gradually we get to the point where they're big enough
to make a difference and the loss starts coming down, because we've plotted here the learning
rate against the loss. All right? So here the loss is coming down as we continue to
increase the learning rate. The loss comes down until we get to a point where, our learning
rates too high and so it flattens out and then oop it's getting worse again. So here's
the point above like 0.1, where we're in this territory. So what we really want is somewhere
around here, where it's kind of nice and steep. So you can actually ask it, the learning rate
finder. So were used �lr_find� to get this plot. We can we can get back from it,
the minimum and steep. And so steep is where was it steepest, but the steepest point was
5e neg 3 (5e-03), and the minimum point divided by ten, that's quite a good rule of thumb,
is 1e neg 2 (1e-02). Somewhere around this range might be pretty good. So each time you
run it you'll get different values. A different time we ran it, we thought that maybe 3e neg
3 (3e-3) would be good, so we picked that. And you'll notice the learning rate finder
is a logarithmic scale. Be careful of interpreting that. So we can now rerun the learning rate
finder, setting the learning rate to a number we picked from the learning rate finder, which
in this case was 3e neg 3 (3e-3), and we can see now that's looking good. Right? We've
got an 8.3% error rate after three epochs. So this idea of the learning rate finder is
very straightforward. I can describe it to you in a couple of sentences. It doesn't require
any complex math, and yet it was only invented in 2015, which is super interesting. Right?
It, it just shows that there's so many interesting things first to all, to learn and discover.
I think part of the reason perhaps for this it took a while is that, you know, engineers
kind of love using lots and lots of computers. So before the learning rate finder came along,
people would like run lots of experiments on big clusters to find out which learning
rate was the best, rather than just doing a batch at a time, and I think partly also
the idea of having a thing where a human is in the loop where we look at something, and
make a decision is also kind of unfashionable. A lot of folks in research and industry love
things which are fully automated, but anyway it's great we now have this tool, because
it makes our life easier and fast AI certainly the first library to have this, and I don't
know if it's still the only one to have it built in, at least to the basic, the base
library. So, now we've got a good learning rate. How do we fine-tune the weights? So,
so far we've just been running this fine-tuned method without thinking much about what it's
actually doing, but we did mention in Chapter 1, Lesson 1 briefly basically what's happening
with a fine tune, what is transfer learning doing, and before we look at that let's take
a question. �Is the learning rate plot in �lr_find� plotted against one single mini
batch?�. No, it's not it's just... It's actually just the standard, kind of, walking
through the, walking through the data loader, so just getting the usual mini batches of
the shuffled data, and so it's kind of just normal training, and the only thing that's
being different is that we're increasing the learning rate a little bit after each mini
batch, and, and keeping track of it. along with that is is the network reset to
the initial status after each trial. No certainly not we actually want to see how it our learns.
We want to see it improving so we don't reset it to its initial state state until we're
done. So at the end of it we go back to the random weights we started with or whatever
the weights were at the time we ran this. So what we're seeing here is, is something
that's actually the, the actual learning that's happening as we at the same time increase
the learning rate. Why would an ideal learning rate found with a single mini-batch at the
start of training keep being a good learning rate even after several epochs and further
loss reductions? Okay question. It absolutely wouldn't so let's look at that too shall we.
And, ya go on. Can i ask one more?. Of course it is an important point so ask us. It is,
it is very important. For the learning rate finder why use the steepest and not the minimum?
We certainly don't want the minimum because the minimum is the point at which it's not
learning anymore. Right so so the first flat section at the bottom here means in this mini
batch didn't get better. So we want the steepest because that's the mini-batch where it got
the most improved. And that's what we want. We want the weights to be moving as fast as
possible. As a rule of thumb though we do find that the minimum divided by ten works
pretty well. That's Sylvian�s favorite approach. And he's generally pretty spot-on with that.
so that's why we actually print out those two things. LR mean is actually the minimum
divided by 10 and steepest point is suggest the steepest point. Great,good questions all.
So remind ourselves what transfer learning does. but with transfer learning remember
what our neural network is. it's a bunch of linear models basically with activation functions
between them. And our activation functions are generally ReLU�s - rectified linear
units. If any of this is fuzzy have a look at the 04 notebook again to remind yourself.
And so each of those linear layers has a bunch of parameters to the whole neural network
has a bunch of parameters. And so after we train a neural network on something like imagenet
we have a whole bunch of parameters that aren't random anymore. They're actually useful for
something and we've also seen that the early layers seem to learn about fairly general
ideas like gradients and edges and the later layers learn about more sophisticated ideas
like what our eyes look like or what does fur look like or what does text look like.
So with transfer learning we take a model. so in other words a set of parameters which
has already been trained on something like imagenet. We throw away the very last layer
because the very last layer is the bit that specifically says which one of those in the
case of imagenet 1000 categories is this an image in. So we throw that away and we replace
it with random weights. Sometimes with more than one layer of random weights and then
we train that. Now, yes. Oh I just wanted to make a comment and that's that I think
the learning rate finder. And I think after you learn about it the idea almost seems kind
of so simple or approximate that it's like wait this shouldn't work like or you know
shouldn't you have to do something more more complicated or more precise that it's like
I just want to highlight that this is a very surprising result. That some kind of the such
as simple approximate method would be so helpful. Yeah, I would particularly say it's surprising
to people who are not practitioners or who have not been practitioners for long. I've
noticed that a lot of my students at USF have a tendency to kind of jump in to try to doing
something very complex where they account for every possible imperfection from the start.
And it's very rare that that's necessary. So one of the cool things about this is a
good example of trying the easiest thing first and seeing how well it works. And this was
a very big innovation when it came out that I think it's kind of easy to take for granted
now but this was super super helpful when it was. It was super helpful and it was also
nearly entirely ignored. None of the research community cared about it and it wasn't until
fast.ai, I think in our first course talked about it that people started noticing. And
we had quite a few years in fact is still a bit the case where super fancy researchers
still don't know about the learning rate finder. And you know, get, get beaten by you know
first lesson fast.ai students on practical problems because they can pick learning rates
better. And they can do it without a cluster of thousands of Buddhas
okay, so transfer loading. So we've got our pre-trained Network and so it's really important
every time you hear the word pre-trained network you're thinking a bunch of parameters which
have particular numeric values and go with a particular architecture like ResNet 34.
We have thrown away the, the final layer and replace them with random numbers. And so now
we want to train, to fine tune this set of parameters for a new set of images in this
case pets. So fine-tune is the method we call to do that and to see what it does we can
go learn.fine-tune?? and we can see the source code and here is the signature of the function.
And so the first thing that happens is we call freeze. So freeze is actually the method
which makes it so only the last layer�s weights will get stepped by the optimizer.
So the gradients are calculated just for those last layers of parameters and the step is
done just for those last layer of parameters. So then we call fit and we fit for some number
of epochs which by default is one we don't change that very often. And what that fit
is doing is it's just fitting those randomly added weights. Which makes sense right. they're
the ones that are going to need the most work. because at the time in which we add them they're
doing nothing at all they're just random. So that's why we spend one epoch trying to
make them better. After you've done that you now have a model which is much better than
we started with. it's not random anymore. All the layers except the last are the same
as the pretrained network. The last layer has been tuned for this new data set. The
closer you get to the right answer, as you can kind of see in this picture, the smaller
the steps you want to create as this sorry, the smaller steps you want to take, generally
speaking. So the next thing we do is we divide our learning rate by 2 and then we unfreeze.
So that means we make it so that all the parameters can now be stepped and all of them will have
gradients calculated. And then we fit for some more epochs and this is something we
have to the pass to the method. And so that�s now got to train the whole network. So if
we want to we can kind of do this by hand, right. And actually cnn_learner, we have by
default freeze the model for us, freeze the parameters for us. So we actually don't have
to call freeze. So if we just create a learner and then fit for a while. This is three epochs
of training just the last layer. And so then we can just manually do it ourselves unfreeze.
And so now at this point as the question earlier suggested maybe this is not the right learning
rate anymore so we can run LR find again. And this time you don't see the same shape.
You don't see this rapid drop because it's much harder to train a model that's already
pretty good. So instead you just see a very gentle little gradient. So generally here
what we do, is we kind of try to find the bit where it starts to get worse again. And
go about which is about here and go about ten let you know a multiple of ten less than
that. So about 1e-5 I would guess which yeah that's what we picked. So then after unfreezing,
finding our new learning rate and then we can do a bunch more. And so here we are, we
are getting down to 5.9 percent error. Which is okay but there's, there's better we can
do. And the reason we can do better is that at this point here we're training the whole
model at a 1e-5 so ten to the minus five learning rate which doesn't really make sense. because
we know that the last layer is still not that great it's only had three epochs of training
from random. so it probably needs more work. We know that the second last layer was probably
pretty specialized to imagenet and less specialized to pet breeds so that probably needs a lot
of work. Whereas the early layers but kind of gradients and edges probably don't need
to be changed much at all. But what would really like is to have a small learning rate
for the early layers and a bigger learning rate for the later layers and this is something
that we developed at fast.ai and we call it discriminative learning rates. And Jason Jasinski
actually is a guy who wrote a great paper that some of these ideas are based on. He
actually showed that different layers of the network really want to be trained at different
rates. Although he didn't kind of go as far as trying that out and seeing how it goes,
it's more of a theoretical thing. So in fast.ai, if we want to do that we can pass to our learning
rate, rather than just passing a single number, we can pass a slice. Now a slice is a special
built-in feature of Python, it's just an object which basically can have a few different numbers
in it. In this case, we�re passing it two numbers. And the way we read those, basically
what this means in fast.ai is our learning rate is the very first layer. We'll have this
learning rate, 10 to the -6, the very last layer will be 10 to the -4, and the layers
between the two will be kind of equal multiples. So they'll kind of be equally spaced learning
rates from the start to the end. So here we can see, basically doing our kind of own version
of fine-tune. We create the learner, we fit with that automatically frozen version, we
unfreeze when we fit some more. And so when we do that you can see this works a lot better.
We're getting down to 5.3, 5.1, 5.4, and error. Well, that's pretty great. One thing we'll
notice here is that we did kind of overshoot a bit, and it seemed like more like an epoch
number 8 was better. So kind of back before, you know, well actually, let me explain something
about fit_one_cycle. So fit_one_cycle is a bit different to just fit. So what fiit_one_cycle
does is it actually starts at a low learning rate, it increases it gradually for the first
one-third or so of the batches until it gets to a high learning rate. The, the highest
were the, this is why they're called lr_max; it's the highest learning rate we get to.
And then for the remaining two-thirds or so of the batches it gradually decreases the
learning rate. And the reason for that is just that, well largely it's kind of like
empirically, researchers have found that works the best. In fact, this was developed again
by Leslie Smith, the same guy that did the learning rate finder. Again it was a huge
step, you know, it really dramatically accelerated the speed at which we can train your networks,
and also made them much more accurate. And again the academic community basically ignored
it. In fact, the key publication that developed this idea was not even, not even past peer
review. And so the reason I mentioned this now is to say that we can't,we don't really
just want to go back and pick the model that was trained back here because we could probably
do better, because we really want to pick a model that's got a low learning rate. But
what I would generally do here is I�d change this 12 to an 8, because this is, this is
looking good. And then I would retrain it from scratch. Normally you�d find a better
result. You can plot the loss and you can see how the training and validation loss moved
along. And you can see here that, you know the, the error rate was starting to get worse
here. And what you'll often see is, often the validation loss will get worse a bit before
the error rate gets worse. We're not really seeing it so much in this case, but the error
rate and the validation losts don't always, they're not always kind of in lockstep. So
what we're plotting here is the loss but you actually kind of want to look to see mainly
what's happening with the error rate, because that's actually the thing we care about. Remember
the loss is just like an approximation of what we care about that just happens to have
a gradient that works out nicely. So how do you make it better now? We're, we're already
down to just 5.4 or if we'd stopped bit earlier maybe we could get down to 5.1 or less error.
On 37 categories that's pretty remarkable, that's a very very good pet breed predictor.
If you want to do something even better, you could try creating a deeper architecture.
So a deeper architecture is just literally putting more pairs of nonlinear activation
function also known as a non-linearity, followed by these little linear models. Put more pairs
on to the end and they're basically... the number of these
sets of layers you have is the number that you'll see at the end of an architecture so
there's resnet18, resnet34, resnet50, so forth. Having said that, you can't really pick resnet19
or resnet38--I mean you could make one--but nobody's created a pre-trained version of
that for you, so you won't be able to do any fine-tuning. So like you can theoretically
create any number of layers you like, but in practice (most of the time) you'll want
to pick a model that has a pre-trained version so you kind of have to select from the sizes
people have pre-trained and there's nothing special about these sizes: they're just ones
that people happen to have picked out. For the bigger models, there's more parameters
and more gradients that are going to be stored on your GPU and you will get used to the idea
of seeing this error, unfortunately. �Out of memory.� So that's not out of memory
in your RAM, that's out of memory in your GPU. CUDA is referring to the language and
the system used for your GPU so if that happens, unfortunately you actually have to restart
your notebook, so that's a kernel > restart and try again. That's a really annoying thing,
but such is life. One thing you can do if you get an out of memory error is: after your
cnn_learner call, add this magic incantation to_fp16(). What that does is it uses for most
of the operations, numbers that use half as many bits as usual. So they're less accurate,
this half-precision floating point or FP16. That will use less memory and on pretty much
any Nvidia card created in 2020 (or later), and some more expensive cards even created
in 2019, that's often got a result in a 2 to 3 times speed up in terms of how long it
takes as well. So here, if I add in to_fp16() then I will be seeing often much faster training.
And in this case what I actually did is I switched to a resnet50 which would normally
take about twice as long, and my per epoch time has gone from 25 seconds to 26 seconds.
So the fact that we used a much bigger network, and it was no slower, is thanks to to_fp16().
But you'll see our error rate hasn't improved. It's pretty similar to what it was, and so
it's important to realize that just because we increase the number of layers, it doesn't
always get better! So it tends to require a bit of experimentation to find what's going
to work for you. And of course, don't forget the trick is: use small models for as long
as possible to do all of your cleaning up and testing and so forth. And wait until you're
all done to try some bigger models and because they're going to take a lot longer. Ok, questions.
How do you know or suspect when you can �do better?� You have to always assume you can
do better... Because you never know! So you just have to... I mean... Part of it though
is: do you need to do better? Or do you already have a good enough result to handle the actual
task you're trying to do? Often people do spend too much time fiddling around with their
models rather than actually trying to see whether it's already going to be super helpful.
As soon as you can actually try to use your model to do something practical, the better.
Yeah, how much can you improve it? Who knows! You know, go through the techniques that we're
teaching in this course and try them, and see which ones help. Unless it's a problem
that somebody has already tried before and written down their results, in a paper or
a Kaggle competition or something, there's no way to know how good it can get. So don't
forget after you do the questionnaire to check out the further research section. One of the
things we've asked you to do here is to read a paper. So find the learning rate finder
paper and read it, and see if you can kind of connect what you read up to the things
that we've learned in this lesson. See if you can maybe even implement your own learning
rate finder. You know, as manually as you need to and see if you can get something--that
you know, based on reading the paper--to work yourself. You can even look at the source
code of fastai�s learning rate finder, of course. And then can you make this classifier
better? So this is further research, right? So maybe you can start doing some reading
to see what else could you do? Have a look on the forum and see what people are trying.
Have a look on the book website, course website, to see what other people have achieved, and
what they did, and play around. So we've got some tools in our toolbox now for you to experiment
with. so that is that is PET Breeds that is as you
know a pretty tricky computer vision classification problem and we kind of have seen most of the
pieces of what goes into the training of it, we haven't seen how to build the actual
architecture but other than that we've kind of worked our way up to understanding what's
going on. So let's build from there into another kind of Dataset: one that involves multi-label
classification. So what's multi-label classification? or maybe... so maybe let's look at an example.
Here is a multi label dataset where you can see that it's not just one label on each image
but sometimes there's three bicycle, car perso,, I don't actually see the car here maybe it
has been cropped out, so a multi-label dataset is one where you still got one image per row
but you can have 0 1 2 or more labels per row so we're going to have a think about and
look at how we handle that but first of all let's take another question.
R: is dropping floating-point number precision switching from FP 32 to FP 16 have an impact
on final loss. J: yes it does, often it makes it better,
believe it or not, it seems like you know they're kind o, it's doing a little bit of
rounding off, is one way to give it, drop some of that precision and so that creates
a bit more bumpiness, a bit more uncertainty it was you know of a stochastic nature and
you know when you introduce more lightly random stuff into training it very often makes it
a bit better and so yeah FP 16 training often gives us a slightly better result but I you
know I wouldn't say it's generally a big deal either way and that me it's not always better.
R: would you say this is a bit of a pattern in deep learning less us exact stochastic
way ��.? J: for sure not just in deep learning but
machine learning more generally, you know there's been some interesting research looking
at like matrix factorization techniques which if you want them to go super fast you can
use a lots of machines, you can use randomization and you often, when you then use the results
you often find you actually get better, better outcomes.
R: just a brief blog for the fast day I computational linear algebra course which talks a little
bit about �.. J: that's it really well that sounds like
a fascinating course, and look at that it's number-one hit here on Google's, so easy to
find but by somebody called Rachel Thomas hey that's persons got the same name as you,
Rachel Thomas. All right so how we going to do multi-label
classification so let's look at a data set called Pascal which is a pretty famous dataset,
we look at the version that goes back to 2007 been around for a long time and it comes with
a CSV file which we will read in CSV as comma separated values and let's take a look each
row has a file name one or more labels and something telling you whether it's in the
validation set or not so the list of categories in each image is a space delimited string
but doesn't have a horse person it has a horse and a person. pd here stands for Pandas. Pandas
is really important library for any kind of data processing and you'll use it all the
time in machine learning and deep learning so let's have a quick chat about it. not a
real panda, it's the name of a library and it creates things called data frames that's
what the DF here stands for and a dataframe is a table containing rows and columns. pandas
can also do some slightly more sophisticated things than that but we'll treat it that way
for now so you can read in a data frame by saying pd for pandas. Pandas read CSV give
it a file name you've now got a dataframe you can call head to see the first few rows
of it for instance a data frame as a iloc integer location property which you can index
into as if it was an array vector it looks just like numpy so column means every row,
remembers row comma column and zero means zeros column and so here is the first column
of the data frame. now you can do the exact opposite so the zeroth row and every column
it's going to give us the first row and you can see the row has column headers and values
so it's a little bit different to Numpy and remember if there's a comma column or a bunch
of comma column at the end of indexing in Numpy or PyTorch or Pandas whatever you can
get rid of it. And these two are exactly the same , and you can do the same this here by
grabbing the column by name, the first column as Fname, say df.fname you get that first
column you can create new columns so here's a tiny little data frame I've created from
a dictionary and I could create a new column by for example adding two columns and you
can see there it is. So it's like a lot like NUmpy or Pytorch, except that you have this
idea of kind of rows and and column named columns and so it's all about and tabular
data. I find this API pretty unintuitive a lot of people do but it's fast and powerful
so it takes a while to get familiar with it but it's worth taking a while and the creator
of pandas wrote a fantastic book called �Python for data analysis� which I've read both
versions and I found it fantastic it doesn't just cover pandas it covers other stuff as
well like iPython and Numpy and matplotlib so highly recommend this book this is our
table so what we want to do now is construct data loaders that we can train with and we've
talked about the data block API as being a great way to create data loaders but let's
use this as an opportunity to create a data data loaders or a data processor create a
data block and data loaders for this and let's try to do it like right from square one so
let's see exactly what's going on with datablock so first of all let's remind ourselves about
what a dataset and the dataloader is. A dataset is an abstract idea of a class you can create
a data set data set is anything which you can index into it like so or and you can take
the length of it like so so for example the list of the lowercase letters along with a
number saying which lowercase letter it is, I can index into it to get 0 comma �a�.
I can get the length of it, to get 26 and so therefore this qualifies as a dataset and
in particular dataset normally you would expect that when you index into it you would get
back at tuple because you've got the independent and dependent variables, not necessarily always
just two things it could be more that could be less but 2 is the most common so once we
have a dataset we can pass it to a dataloader we can repress we can request a particular
batch size we can shuffle or not and so there's our data loader from �a� we could grab
the first value from that iterator and here is the shuffled, seven is H four is E twenty
is U and so forth and so I remember a mini-batch has a bunch of a mini-batch of the independent
variable and a mini-batch of the dependent variable if you want to see how the two correspond
to each other you can use zip so for zip passing in this list and then this list so B0 and
B 1 you can see what zip does in Python is it grabs one element from each of those in
turn and gives you back the tuples of the corresponding elements since we're just passing
in all of the elements of B to this function, Python has a convenient shortcut for that
which is just say *B and so * means insert into this parameter lists each element of
B just like we did here, so these are the same thing so this is a very handy idiom that
we use a lot in Python zip * something is kind of a way of like transposing something
from one orientation to another. Allright so we've got a dataset we've got a dataloader
and then what about datasets our datasets is an object which has a training data set
and a validation set dataset so let's look at one now normally you don't start with kind
of an enumeration like this like with with an independent variable and a dependent variable
normally you start with like a file name for example and then you, you kind of calculate
or compute or transform your file name into an image by opening it and a label by for
example looking at the file name and grabbing something out of it so for example we could
do something similar here this is what data sets does so we could
start with just the lowercase letters so this is still a dataset right because we can index
into it and we can get the length of it although it's not giving us tuples yet so if we now
pass that list to the datasets plus and index into it we get back the tuple and it's actually
a tuple with just one item this is how Python shows tuple with one item as it puts it in
parenthesis and a comma and then nothing okay so in practice what we'd really want to do
is to say like okay we'll take this and do something to compute an independent variable
and do something to compute the dependent variable so here's a function we could use
to compute an independent variable which is to stick an A on the end and our dependent
variable might just be the same thing with a B on the end but here's two functions so
for example now we can call datasets passing an A and then we can pass in a list of transformations
to do and so in this case I've just got one which is this function at an a on the end
so now for index into it I don't get A anymore I get AA if you pass multiple functions then
it's going to do multiple things I've got F 1 then F 2 a a B that's this one that's
this one and you'll see this is a list of lists and the reason for that is that you
can also pass something. like this: a list containing �f1�, a list
containing �f2�, and this will actually take each element of �a�, pass it through
this list of functions, and there's just one of them, to give you �aa�, and then start
again, and separately pass it through this list of functions, there's just one, to get
�ab�. And so this is actually kind of the main way we build up independent variables
and dependent variables in Fastai, is we start with something like a filename, and we pass
it through two lists of functions: one of them will generally kind of, open up the image
for example, and the other one will kind of parse the filename, for example and give you
a independent variable and a dependent variable. So you can then create a DataLoaders object
from datasets by passing in the datasets, and a batch size, and so here you can see
I've got shuffled �oa�, �ia� etc. �ob�, �ib� etc. So, this is worth
studying to make sure you understand what Datasets and DataLoaders are. We don't often
have to create them from scratch, we can create a DataBlock to do it for us. But now we can
see what the DataBlock has to do. So let's see how it does it. We can start by creating
an empty DataBlock. So an empty DataBlock is going to take our DataFrame, so we're going
to go back to looking at our DataFrame, which remember, was this guy. And so if we pass
in our DataFrame, we can now, we will now find that this Datablock has created Datasets,
a training and a validation Dataset for us, and if we look at the training set, it'll
give his back an independent variable in the dependent variable and we'll see that they
are, the same thing. So this is the first row of the table what's actually shuffled,
so it's a random row of the table, repeated twice. And the reason for that is by default
the DataBlock assumes that we have two things, the independent variable and the dependent
or the input in the target, and by default it just copies, it just keeps exactly whatever
you gave it. To create the training set in the validation set by default it just randomly
splits the data with the 20% validation set. So that's what's happened here. So this is
not much use. What we what we actually want to do if we look at X for example is grab
the �fname�, the filename field, because we want to open this image. That's going to
be our independent variable. And then for the label, we're going to want this here �person
cat�. So we can actually pass these as parameters �get_x� and �get_y�, are functions
that return the the bit of data that we want. And so you can create an user function in
the same line of code in Python by saying lambda. so lambda �r� means create a function,
doesn't have a name, that's going to take a parameter called �r�. We don't even
have to say return it's got to return the �fname� column, in this case. And �get_y�
is something, which is a function that takes an �r� and returns the labels column.
So now we can do the same thing called �dblock dot datasets�. We can grab a row from that,
from the training set, and you can see look here it is there is, there is the image filename
and there is the space delimited list of labels. So here's exactly the same thing again, but
done with functions. Okay so now the one line of code above has become three lines of code,
but it does exactly the same thing. Okay we don't get back the same result because the
training set... well wait why don't we get the same result?
Oh, I know why. Because it randomly shuffles, it's randomly picking a different validation
set. Because the random spit is done differently each time, so that's why we don't get the
same result. One thing to note, be careful of lambdas. If you want to save this DataBlock
for use later, you won't be able to. Python doesn't like saving things that contain lambdas,
so most of the time in the book and the course we normally use, avoid lambdas for that reason
because this is often very convenient to be able to save things. We use the word here
serialization, that just means, basically it means saving something. This is not enough
to open an image because we don't have the path. So to turn this into, so rather than
just using this function to grab the �fname� column, we should actually use pathlib to
go, a �path/train� and then column. And then for the �y�, again the labels it�s
not quite enough, we actually have a split on space. But this is Python, we can use any
function we like, and so then we won't use the same three lines of code as here, and
now we've got a path, and a list of labels. So that's looking good. So we want this path
to be opened as an image, so the DataBlock API, lets you pass a �blocks� argument,
where you tell it, for each of the things in your tuple, so there's two of them, what
kind of block do you need. So we need an ImageBlock to open an image, and then, in the past we've
used a CategoryBlock, for categorical variables, but this time we don't have a single category,
we've got multiple categories, where we have to use a MultiCategoryBlock. So once we do
that, and have a look, we now have an 500 by 375 image as our independent variable,
and as a dependent variable we have a long list of zeros and ones. The long list of zeros
and ones, is the labels as a one hot encoded vector, a rank one tensor, and specifically
there will be a zero in every location where, in the vocab, where there is not that kind
of object in this image, and a one in every location where there is. Sso for this one
there's just a person so this must be the location in the vocab where there's a person.
We have any questions? The one hot encoding is a very important concept, and we didn't
have to use it before, right? We could just have a single integer saying which one thing
is it, but when we've got lots of things, lots of potential labels, it's convenient
to use this one hot encoding. And it's kind of what, it's actually what's going to happen
with, with the, with the actual matrices anyway. When we actually compare the activations of
our neural network to the target, it's actually going to be comparing each one of these.
Okay so the categories, as I mentioned, is based on the vocab, where we can grab the
vocab from our Datasets object, and then we can say okay, let's look at the first row,
and let's look at the dependent variable, and let's look for where the dependent variable
is 1. Okay and, and we can have a look, parse those indexes, there's a vocab and get back
a list of what it actually was there. And again each time I run this, I'm going to get
different results. So each time we run this we're going to get different results because
I called �dot datasets� again here, so it's going to give me a different train-test
split, and so this time it turns out that this actually, a chair, and we have a question.
Shouldn't the tensor be of integers? Why is it a tensor of floats? Yeah, conceptually
this is a tensor of integers, they can only be 0 or 1, but we, we�re going to be
using a cross entropy style loss function, so we're going to actually need to do floating-point
calculations on them. That's going to be faster to just store them as float in the first place
rather than converting backwards and forwards, even though they're conceptually an �int�
we're not going to be doing kind of �int style calculations� with them. Good question.
I mentioned that by default, the DataBlock uses a random split, you might�ve noticed,
in the DataFrame though, it said here's a column saying what validation set to use,
and if the data set you're given, tells you what validations had to use, you should generally
use it because that way you can compare your validation set results to somebody else's.
So you can pass a splitter argument, which again is a function, and so we're going to
pass it a function, that's also called splitter, and the function is going to return the indexes
where it's not valid and that's going to be the training set, and the indexes where it
is valid, that's going to be the validation set. And so the splitter argument is expected
to return two lists of integers, and so if we do that we get again the same thing, but
now we're using the correct train and validation sets. Another question? Sure. Any particular
reason we don't use floating point eight? Is it just that the precision is too low?
Yeah, trying to train with 8-bit precision is super difficult it's, it's so flat and
bumpy, it's pretty difficult to get decent gradients. But you know it's an area of research,
the main thing people do with 8-bit or even one bit data types, is they take a model that's
already been trained with 16-bit or 32-bit floating-point and then they kind of round
it off. It's called discretizing, to create a kind of purely integer or even binary network
which can do inference much faster. Figuring out how to train with such low precision data
is an area of active research. I suspect it's possible, and I suspect, I mean people have
fiddled had some success I think you know it could turn out to be super interesting
particularly for stuff that's been done on low-powered devices that might not even have
a floating-point unit. Right, so the last thing we need to do is to add our item transforms
RandomResizedCrop, we've talked about that enough, so I won't go into it, but basically
that means we now are going to ensure that everything has the same shape so that we can
collate it into a DataLoader so now rather than going �datasets� we go DataLoaders,
and display our data. And remember, if something goes wrong, as we saw last week you can call
�summary�, to find out exactly what's happening in your DataBlock. So now you know
this is something really worth studying this section because data blocks are super handy
and if you haven't used Fastai 2 before, they won't be familiar to you because no other
library uses them, and so like this has really shown you how to go right back to the start
and gradually build them up, so hopefully that'll make a whole lot of sense. Now we're
going to need a loss function again, and to do that let's start by just creating a learner.
Let's create a Resnet18 from the DataLoaders object we just created, and let's grab one
batch of data and then let's put that into our mini-batch of independent and dependent
variables and then �learn dot model� is the thing that actually contains the, the
the model itself, in this case it�s CNN, and you can treat it as a function, and so
therefore we can just pass something to it. And so if we pass a mini-batch of the independent
variable to �learn dot model� it will return the activations from the final layer.
And that is, shape 64 by 20. So anytime you get a tensor back, look at its shape, and
in fact before you look, at its shape, predict what the shape should be. And then make sure
that you're right. If you're not, either you guessed wrong, so try to understand where
you made a mistake, or there's a problem with your code, and this place, 64 by 20, makes
sense because we have a mini-batch size of 64, and for each of those, we're going to
make predictions about what probability is each of these 20 possible categories, and
we have a question. Two questions. Two questions, all right! Is the DataBlock API compatible
with out of core data sets like Dask. Yeah, the DataBlock API can do anything you wanted
to do, so you're passing it, if we go back to the start. So, you can create an empty
one, and then you can pass it anything that is indexable, and yeah so that can be anything
you like, and pretty much anything can be made indexable in Python and that's something
like Dask is certainly indexable. So that works perfectly fine. If it's not indexable,
like it's a, it's a network stream or something like that, then you can�t use the DataLoaders
Datasets API directly which we'll learn about either in this course or the next one. But
yeah, anything that you can index into, which certainly includes Dask, you can use with
DataBlocks. Next question, where do you put images for multi-label with that CSV table?
Should they be in the same directory? They can be any way you like, so in this case we
used a pathlib object like so. And in this case, by default it's going to be using�
Let me think about this. What's happening here is the path is.. Oh it's saying �dot�
okay the reason for that is that path.BASE_PATH is currently set to path, and so that displays
things relative. Well let's get rid of that. Okay so the path we set is here, right? And
so then when we said get_x, it's saying path slash train whatever, right? So this is an
absolute path, and so here is the exact path. So you can put them anywhere you like, you
just have to say what the �path� is. And then if you want to not get confused by having
this big long prefix that we can, don't want to see all the time, just set BASE_PATH to
the path you want everything to be relative to, and then it'll just print things out in
this more convenient manner. All right, so um this is really important that you can do
this, that you can create a learner, you can grab a batch of data that, you can pass it
to the model, this is just plain PyTorch, this line here right no Fastai. You can see
the shape, right? You can recognize why it has this shape, and so now if you have a look,
here are the 20 activations. Now this is not a trained model, it's a pre trained model
with a random set of final layer weights, so these specific numbers don't mean anything,
but it's just worth remembering this is what activations look like, and most importantly
they're not between 0 & 1. And, if you remember from the MNIST Notebook,
we know how to scale things between zero and one, we can pop them into the sigmoid function.
So the sigmoid function is something that scales everything to be between zero and one.
So let's use that. You'll also hopefully remember from the MNIST Notebook that the MNIST loss,
the MNIST loss function, first did sigmoid, and then it did �torch.where�, so and
then it did �.mean�. We're going to use exactly the same thing as the MNIST loss function,
and we're just going to do one thing, which is going to add �.log�, for the same reason
that we talked about when we were looking at softmax, we talked about why log is a good
idea as a transformation. We saw in the MNIST Notebook, we didn't need it but we're gonna
train faster and more accurately, if we use it, because it's just more, it's going to
be better behaved, as we've seen. So this particular function, which is identical to
MNIST loss plus �.log� jhas a specific name and it's called binary cross entropy,
and we used it for the threes vs. sevens problem, to, to decide whether that column is it a
three or not, but because we can use broadcasting in PyTorch and element-wise arithmetic, this
function when we pass it a whole matrix is going to be applied to every column. So is
the first column, you know, what... So it'll basically do a �torch.where� on, on every
column separately and every item separately. So that's great, it basically means that this
binary cross-entropy function is going to be just like MNIST loss, rather than just
being �Is this the number three?� it'll be �Is this a dog?�, �Is this a cat?�,
�Is this a car?�, �Is this a person?�, �Is this a bicycle?�, and so forth. So
this is where it's so cool in PyTorch, we can kind of run, write, one thing and then
kind of have it expand to handle higher dimensional tensors, without doing any extra work. We
don't have to write this ourselves, of course, because PyTorch has one and it's called �F.binary_cross_entropy�.
We can just use PyTorch�s. As we've talked about there's always a equivalent module version
so this is exactly the same thing as a module �nn.BCELoss�, and these ones don't include
the initial sigmoid, actually. If you want to include this initial sigmoid you need F.binary_cross_entropy_with_logits
or the equivalent nn.BCEWithLogitsLoss. So BCE is binary cross-entropy. And so those
are two functions, plus two equivalent classes for multi-label or binary problems, and then
the equivalent for single label like MNIST and pets is �nll_loss� and �cross_entropy�.
That's the equivalent of �binary_cross_entropy� and �binary_cross_entropy_with_logits�.
So these are pretty awful names, I think we can all agree, but it is what it is. So in
our case we have a one hot encoded target, and we want the one with the sigmoid in, so
the equivalent built-in is called �BCEWithLogitsLoss. That we can make that our loss function, we
can compare the activations to our targets, and we can get back a loss, and then that's
what we can use to train, and then finally, before we take our break, we also need a metric.
Now previously we've been using as a metric accuracy or actually error rate. Error rate
is 1 - accuracy. Accuracy only works for single label datasets, like MNIST and pets, because
what it does is it takes the input, which is the final layer of activations, and it
does our �argmax�, what �argmax� does is it says: �What is the index of the largest
number in those activations?�. So, for example, for MNIST, you know, maybe
the largest, the highest probability is seven, so this �argmax� would return seven. And
then it says, �OK there those are my predictions.�, and then it says, �OK is the prediction
equal to the target or not, and then take the floating-point mean�. That's what accuracy
is. So �argmax� only makes sense when there's a single maximum thing you're looking
for. In this case we've got multi-label. So instead we have to compare each activation
to some threshold. Our default is 0.5, and so we basically say, �If the sigmoid of
the activation is greater than 0.5, let's assume that means that category is there and
if it's not let's assume it means it's not there�, and so this is going to give us
a list of trues and falses, for the ones that the based on the activations it thinks are
there, and we can compare that to the target, and then again take the floating point mean.
So we can use the default threshold of 0.5, but we don't necessarily want to use 0.5,
we might want to use a different threshold, and remember we have to pass, when we create
our learner, we have to pass to the metric, the metrics argument, a function. So what
if we want to use a threshold other than 0.5. Well we'd like to create a special function,
which is �accuracy_multi�, with some different threshold, and the way we do that is we use
a special, built-in in Python called partial. Let me show you how partial works. Here�s
a function called �say_hello�, �say_hello� somebody with something. So �say_hello�
Jeremy, where the default is �Hello�, that says �Hello Jeremy�. �say_hello�
Jeremy, comma �Ahoy�, gonna be �Ahoy! Jeremy�. Let's create a special version
of this function that will be more suitable for Sylvain. It's going to use French. So
we can say partial, create a new function, that's based on the �say_hello� function,
but it's always going to set say_what to �Bonjour� and we'll call that �f�. So now f(�Jeremy�)
is �Bonjour Jeremy and f(�Sylvain�) is �Bonjour Sylvain�. So, you see, we've
created a new function, from an existing function, by fixing one of its parameters. So we can
do the same thing for �accuracy_multi�. Hey, let's use a threshold of 0.2, and we
can pass that to metrics, and so let's create a �cnn_learner� and you'll notice here
we don't actually pass a loss function, and that's because fast AI is smart enough to
realize. �Hey, you're doing a classification model, with a, a multi-label dependent variable.
So I know what loss function you probably want�. So it does it for us. We can call
�fine_tune� and here we have an accuracy of 94.5 after the first few, and eventually
95.1. That's pretty good. We've got an accuracy of over 95%, was 0.2 a good threshold to pick.
Who knows? Let's try 0.1. Oh, l that's a worse accuracy. So, I guess, in this case we could
try a higher threshold. 94. Hmm, also not good. What's the best threshold? Well what
we could do is call, �get_preds�, to get all of the predictions, and all of the targets,
and then we could calculate the accuracy, at some threshold, then we could say, �OK,
let's grab lots of numbers between 0.05 and 0.95, and with a list comprehension calculate
the accuracy for all of those different thresholds, and plot them. Ah, looks like we want a threshold
somewhere a bit about 0.5. So, cool, we can just use that, and it's going to give us 96
in a bit, which is going to give us a better accuracy. This is
a, you know, something that a lot of theoreticians would be uncomfortable about - I've used the
validation set to pick a hyper parameter, a threshold, right. And so people might be
like, �Oh, you're overfitting, using the validation set to pick a hyper parameter.�
But if you think about it, this is a very smooth curve, right. It's not some bumpy thing
where we've accidentally kind of randomly grabbed some unexpectedly good value. When
you're picking a single number from a smooth curve � you know, this is where the theory
of like don't use a validation set for for how parameter tuning doesn't really apply.
So it's always good to be practical, right. Don't treat these things as rules, but as
rules of thumb. Okay, so let's take a break for five minutes and I'll see you back here
in five minutes time. Hey,welcome back oh I want to show you something
really cool - image regression. So we are not going to learn how to use a fast.ai image
regression application because we don't need one. Now that we know how to build stuff up
with loss functions and the DataBlock API ourselves, we can invent our own applications.
So there is no image regression application, per se. But we can do image regression really
easily. What do we mean by image regression? Well, remember back to a lesson, I think is
lesson 1, we talked about the two basic types of machine learning, or supervised machine
learning - regression and classification. Classification is when our dependent variable
is a discrete category or set of categories. And regression is when our dependent variable
is a continuous number, like age or XY coordinate, or something like that. So image regression
means our independent variable is an image and our dependent variable is a continuum
or one or more continuous values. And so here's what that can look like which is biwi head
pose dataset. It has a number of things in it, but one of the things we can do is find
the midpoint of a person's face, see. So the biwi head pose dataset, so the biwi head poste
dataset comes from this paper, �Random Forest for Real Time 3D Face Analysis.� So thank
you to those authors. And we can grab it in the usual way, untar_data. And we can have
a look at what's in there, and we can see there's 24 directories numbered from 0, from
1 to 24. 1,2,3, and each one also has a .obj file. We're not going to be using the .obj
file, I'm just the directories. So let's look at one of the directories and as you can see
there's a thousand things in the first directory. So each one of these 24 directories is one
different person that they photographed. And you can see, for each person there's frame
3 pose, frame 3 RGB, frame 4 pose, frame 4 RGB, and so forth. So in each case we've got
the image, which is the RGB, and we've got the pose, which is pose.txt. So as we've seen,
we can grab our use get_image_files to get a list of all of the files image files recursively
in a path. So once we have an image filename, like this one, sorry like this one, we can
turn it into a pose file name by removing the last 1,2,3,4,5,6,7 letters and adding
back on pose.txt. And so here is a function that does that. And so you can see, I can
pass in an image file to img2pose, and get back a pose file, right. So PILImage.create
is the fastai way to create an image, at least a PIL image. It has a shape (in computer
vision, they're normally backwards, they normally do columns by rows but that's why it's this
way around) whereas PyTorch and Numpy tensors and arrays are rows by columns. So that's
confusing, but that's just how things are I'm afraid. So here's an example of an image.
When you look at the ReadMe from the dataset website, they tell you how to get the center
point from, from one of the text files, and it's just this function (the details don�t
matter, it is what it is). They call it get_ctr and it will return the XY coordinate of the
center of the person's head, or face. So we can pass this as get_y, because get_y, remember
is a thing that gives us back the label, okay. So, so, here's the thing, right. We can create
a DataBlock and we can pass in as the independent variables block, ImageBlock as usual. And
then the dependent variables block we can say PointBlock, which is a tensor with two
values in. And now by combining these two things, this says we want to do image regression
with the dependent variable with two continuous values. To get the items, you call get_image_files,
to get the y, we'll call the get_ctr function to split it. So this is important, we should
make sure that the validation set contains one or more people that don't appear in the
training set. So I'm just going to grab person number 13, just grabbed it randomly, and I'll
use all of those images as the validation set. Because I think they did this with a
Xbox Kinect, you know, video thing, so there's a lot of images that look almost identical.
So if you randomly assigned them then you would be massively overestimating how effective
you are. You want to make sure that you're actually doing a good job with a random, with
a new set of people, not just a new set of frames. That's why we use this and so funcSplitter
is a splitter that takes a function. And in this case we're using lambda to create the
function. We will use data augmentation and we will also normalize (so this is actually
done automatically), now this case we're doing it manually. So this is going to subtract
the mean and divide by the standard deviation of the original data set that the pre-trained
model used, which is ImageNet. So that's our data block, and so we can call dataloaders
to get our data loaders, passing in the path. And show_batch, and we can see that looks
good, right. Here's our faces and the points. And so let's like, particularly for as a student
,don't just look at the pictures, look at the actual data. So grab a batch, put it into
an xb and a yb (x batch and y batch), and have a look at the shapes. And make sure they
make sense. So why is this 64 by 1 by 2? So it's 64 in the mini batch (64 rows), and then
the coordinates is a 1 by 2 tensor. So there's a single point with two things in it. It's
like you could have like hands, face, and armpits, or whatever � or nose and ears
and mouth. So in this case we're just using one point and the point is represented by
two values, the x and the y. And then why is this 64 by 3 by 240 by 320? Well there's
240 rows by 320 columns (that�s the pixels it's the size of the images that we're using),
mini-batches 64 items now what's the 3? The 3 is the number of channels, which in this
case means the number of colors. If we open up some random grizzly bear image and then
we go through each of the elements of the first axis, and do a show_image you can see
that it's got the red the green and the blue as the three channels, so
that's how we store a three channel image. It is stored as a three by number of rows
by number of columns rank three tensor and so a mini batch of those is a rank four tensor.
Tat's why this is that shape. So here's a row from the dependent variable okay there's
that XY location we talked about. So we can now go ahead and create a learner passing
in our dataloaders as usual, passing in our pretrained architecture as usual, and if you
think back, you may just remember in Lesson one we learn about y_range. Y_range is where
we tell Fastai what range of data we expect to see in the dependent variable. So we want
to use this generally when we're doing regression though the range of our coordinates is between
minus 1 and 1, that's how Fastai and Pytorch treats coordinates the left hand side is minus
1 or the top is minus 1 and the bottom in the right 1. No point predicting something
that's more than minus 1 or bigger than 1 as that is not in the area that we use for
our coordinates. [Rachel] I have a question. [Jeremy] sure just a moment. So how does Y_range
work? Well it actually uses this function called sigmoid_range which takes the sigmoid
of X multiplies by hi minus lo and adds lo and here is what sigmoid_range looks like
or minus 1 to 1. It's just a sigmoid where the bottom is the lo and the top is the hi
and so that way all of our activations are going to be mapped to the range from minus
1 or 1. Yes Rachel. [Rachel] can you provide images with an arbitrary number of channels
as inputs specifically more than three channels? [Jeremy] yeah you can have as many channels
as you like. We�ve certainly seen images with less than three because we've been grayscale.
More than three is common as well you could have like an infrared band or like satellite
images often have multispectral. There's some kinds of medical images where there are bands
that are kind of outside the visible range. Your pre-trained model will generally have
three channels. Fastai does some tricks to use three channel pre-trained models for non
three channel data but that's the only tricky bit other than that it's just just a you know
it's just an axis that happens to have four things or two things or one thing instead
of three things. There's nothing special about it. Okay we didn't specify a loss function
here so we get whatever you gave us which is a MSE loss. So MSE loss is mean squared
error and that makes perfect sense right you would expect mean squared error to be a reasonable
thing to use for regression we're just testing how close we are to the target and then taking
the square picking the mean. We didn't specify any metrics and that's because mean squared
error is already a good metric like it's not � it has nice gradient so behaves well but
it's also the thing that we care about so we don't need a separate metric to track.
Let's go ahead and use lr_find() and we can pick her learning rate so maybe about n to
the minus 2 we can call fine_tune and we get a valid loss of 0.0001. So that's the mean
squared error so we should take the square root but on average we're about 0.01 off in
a coordinate space that goes between minus 1 and 1. Well that sounds super accurate,
took about 3 and a bit minutes to run. So we can always call in Fastai, and we always
should, our results and see what our results look like
and as you can see Fastai has automatically figured out how to display the combination
of an image independent variable and a point dependent variable on the left is the target
and on the right is the prediction and as you can see it is pretty close to perfect.
One of the really interesting things here is that we used fine_tune even though think
about it the thing we're fine-tuning imagenet isn't even an image regression model. So we're
actually fine-tuning an image classification model that becomes something totally different;
an image regression model. Why does that work so well? Well because an imagenet classification
model must have learnt a lot about kind of how images look like, what things look like
and where the pieces of them are they kind of know how to figure out what breed of animals
something is even if it's partly obscured by borscht shorts, in the shade, or it's turned
in different angles. You know these are pre-trained image models are incredibly powerful computers,
you know computing algorithms. So built into every imagenet pre-trained model is all this
capability that it had to learn for itself. So asking it to use that capability to figure
out where something is, it's just actually not that hard for it and so that's why we
can actually fine tune an imagenet classification model to create something completely different
which is a point image regression model. So I find that incredibly cool I gotta say. So
again look at the further research after you've done the questionnaire and particularly if
you haven't used dataframes before please play with them because we're going to be using
them more and more. [Rachel] I�ve a question. [Jeremy] I'll just do the last one and also
go back and look at the bear classifier from notebook 2 or whatever; hopefully you created
some other classifier for your own data. Because remember we talked about how it would be better
if the bear classifier could also recognize that there's no bear at all or maybe there's
both a grizzly bear and a black bear or a grizzly bear and a teddy bear so if you retrain
using multilabel classification see what happens see how well it works when there's no bears
and see whether it changes the accuracy of the single label model when you turn it into
a multilabel problem. So have a fiddle around and tell us on the forum what you find. You've
got a question rachel [Rachel] is there a tutorial showing how to use pretrained models
on 4-channel images; also how can you add a channel to a normal image; [Jeremy] well
the last one is how do you add a channel to an image? I don�t know what that means.
Okay I don't know; Like an image is an image. You can�t add a channel to an image. it
is what it is. I don't know if there's a tutorial but we can certainly make sure somebody on
the forum has learned how to do it. it's super straightforward; pretty much automatic. Okay
we're going to talk about collaborative filtering. What is Collaborative filtering? Think about
on Netflix or whatever, you might have watched a lot of movies that are sci-fi and have a
lot of action and were made in the 70s. Netflix might not know anything about the properties
of movies you watched. They might just know that they are movies with titles and IDs.
But what it could absolutely see without any manual work is find other people that watched
the same movies that you watched. And it could
see what other movies those people watched that you haven't, and it would probably find
they were also, we would probably find, they're also science fiction, and full of action,
and made in the 70s. So we can use an approach where we recommend things, even if we don't
know anything about what those things, are as long as we know who else has used or recommended
things that are similar, you know, the same kind, you know, many of the same things that
that you've liked or, or used. This doesn�t necessarily mean users and products. In fact
in collaborative filtering instead of saying products we normally say items, and items
could be links you click on, diagnosis for a patient, and so forth. So there's a key
idea here, which is that in the underlying items and we going to be using movies in this
example, there are some, there are some features, they may not be labeled, but there's some
underlying concept of features, of, of those movies, like the fact that there's a action
concept, and a sci-fi concept, and a 1970s concept. Now you never actually told Netflix
you like these kinds of movies, and maybe Netflix never actually added columns to their
movies saying what movies are those types, but as long as like, you know, in the real
world there's this concept of sci-fi, and action, and movie age, and that those concepts
are relevant for at least some people's movie watching decisions, as long as this is true,
then we can actually uncover these they're called latent factors, these things that,
kind of, decide what kind of movies you want to watch, and they�re latent, because nobody
necessarily ever wrote them down or labeled them or or communicated them in any way. So
let me show you what this looks like. So there's a great dataset we can use, called MovieLens,
which contains tens of millions of movie rankings, and so a movie ranking looks like this. It
has a user number, a movie number, a rating and a timestamp. So we don't know anything
about who user number 196 is. I don't know if that is Rachel, or Sylvain, or somebody
else. I don't know what movie number 242 is. I don't know if that's Casablanca or Lord
of the Rings or The Mask. And then rating is a number between, I think, it was 1 and
5. A question. Sure. �In traditional machine learning we perform cross validations and
k-fold training to check for variance and bias trade-off. Is this common in training
deep learning models as well?� So cross-validation is a technique where you don't just let your
data set into one training set and one validation set, but you basically do it five or so times,
like five training sets, and like five validation sets representing different overlapping subsets,
and basically this was this used to be done a lot because people often used to have not
enough data to get a good result, and so this way, rather than, kind of, having 20% that
you would leave out each time you could just leave out like, 10% each time. Nowadays it's
less common that we have so little data that we need to worry about the complexity and
extra time of lots of models. It's done on Kaggle a lot. As on Kegel every little fraction
of a percent matters, but it's not, yeah, it's not a deep learning thing, or a machine
learning thing, or whatever. It's just a, you know, lots of data or not very much data
thing, and do you care about the last decimal place
of� Or not. It's not something we're going to talk about, certainly, in this part of
the course, if ever, because it's not something that comes up in practice that often as being
that important. There are two more questions. �What would be some good applications of
collaborative filtering outside of recommender systems?� Well I mean depends how you define
recommender system. If you're trying to figure out what kind of other diagnoses might be
applicable to a patient, I guess, it's kind of a recommender system, or you're trying
to figure out where somebody is going to click next, or whatever, it's kind of a recommender
system, but, you know, really conceptually it's anything where you're trying to learn
from, from past behavior, where that behavior is, kind of, like a thing happened to an entity.
�What is an approach to training using video streams, i.e., from drone footage instead
of images? Would you need to break up the footage into image frames?� In practice
quite often you would, because images just tend to be pretty big, sorry, videos tend
to be pretty big. There's a lot of... So, I mean, theoretically the time could be the
the fourth channel. Yeah, or, or a fifth channel. Fifth channel. So if it's a full color movie
you can absolutely have, well I guess fourth, because you, you can have, it would be a five,
rank five tensor being batched by time by color by row by column, but often that's too
computationally and too memory-intensive. Sometimes people just look at one frame at
a time, sometimes people use us a few frames around, kind of, the key frame like three
or five frames at a time, and sometimes people use something called a Recurrent Neural Network,
which we'll be seeing in the next week or two, you can treat it as a sequence data.
Yeah, there's all kinds of tricks you can do to try and go and work with that. Conceptually,
though, there's no reason you can't just add an additional axis to your tensors and everything
just work. It's just a practical issue around time and memory. And someone else noted that
it's pretty fitting that you mentioned the movie The Mask. Yes, was another accident
I guess I got masks on the brain. I'm not sure if we're allowed to like that movie anymore,
though and I've liked it when it came out. I don't know of, what I think nowadays, it's
a while. Okay. So, let's take a look. So we can untar data ML_100k. So ML_100k, is a small
subset of the full set. There's another one that we can grab, which is about the whole
lot 25 million, but 100k is good enough for messing around. So if you look at the README
you'll find the main table, the main table is in a file called �u.data�. So let's
open it up with �read_csv� again. This one is actually not comma separated values,
it's tab separated rather confusingly we still use csv, and to say delimiter is a tab, �\t�
is a tab, and there's no row at the top saying what the columns are called, so we say header
is None, and then pass in a list of what the columns are called. �.head� will give
us the first five rows, and we mentioned just before what it looks like. It's not a particularly
friendly way to look at it, so what I'm going to do is I'm going to, cross tab it, and so
what I've done here is I've grabbed the top, I can�t remember how many it was. Well,
who cares? One, two, three, four... fifteen, or twenty movies, based on the most
popular movies and the top bunch of users who watch the most movies. And so I basically
kind of reoriented this, so for each user I have all the movies they've watched and
the rating they gave them. So empty spots represent users that have not seen that movie.
So this is just another way of looking at this same data. So basically what we want
to do is guess what movies we should tell people they might want to watch. And so it's
basically filling in these gaps to tell user 212, do you think we would, they might like
movie 49 or 79 or 99 best to watch next. So let's assume that we actually had columns
for every movie that represented, say, how much sci-fi they are, how much action they
are, and how old they are. And maybe they're between -1 and 1. And so like the last_skywalker
is very sci-fi, barely action, and definitely not old. And then we could do the same thing
for users. So we could say user1 really likes sci-fi, quite likes action, and really doesn't
like old. And so now if you multiply those together, and remember in PyTorch and Numpy,
you have element-wise calculation, so this is going to multiply each corresponding item.
It's not matrix multiplication. If you're a mathematician, don't go there - this is
element-wise multiplication. If we want matrix multiplication, it would be an @ sign. So
if we multiply each element together, next with your equivalent element in the other
one, and then sum them up, that's going to give us a number which will basically tell
us how much do these two correspond. Because remember, two negatives multiplied together
to get a positive. So user1 likes exactly the kind of stuff that the last_skywalker
has in it, and so we get 2.1. Multiplying things together element-wise and adding them
up is called the dot product and we use it a lot, and it's the basis of matrix (shouldn�t
say modification). matrix multiplication. And so make sure you know what a dot product
is, it's this. So Casablanca is not at all sci-fi, not much action, and is certainly
old. So if we do user1 times Casablanca, we get a negative number. So we might think,
�OK, user1 won�t like, won't like this movie. Problem is we don't know what the latent
factors are, and even if we did we don't know how to label a particular user or a particular
movie with them. So we have to learn them. How do we learn them? Well, we can actually
look at a spreadsheet, so I wrote a spreadsheet version. So we have a spreadsheet version
which is basically, what I did was I popped this table into Excel and then I randomly
created a (let�s count this now, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15),
I randomly created a 15x5 table here. These are just random numbers and I randomly created
a 5x15 table here. And I basically said, Okay, well let's just pretend, let's just assume
that every movie and every user has five latent factors. I don't know what they are. And let's
then do a matrix multiply of this set of factors by this set of factors and a matrix multiply
of a row by a column is identical to a dot product of two vectors. So that's why I can
just use matrix multiply now. This is just what this first cell content so they then
copied it to the whole thing. So all these numbers there are being calculated
from the row latent factors dot product with or matrix model play with the column latent
factors. So in other words I'm doing exactly this calculation, but I'm doing them with
random numbers. And so that gives us a whole bunch of values, right. And then what I could
do, is I could calculate a loss by comparing every one of these numbers here to every one
of these numbers here. And then I could do mean squared error. And then I could choose
stochastic gradient descent to find the best set of numbers in each of these two locations.
And that is what collaborative filtering is. So that's actually all we need. So, rather
than doing it in Excel. And we�ll do the Excel version later, if you're interested
(because we can actually do this whole thing and it works in Excel), let's jump and do
it into PyTorch. Now one thing that might just make this more fun is actually to know
what the movies are and MovieLens tells us in u.item what the movies are called and that
uses the delimiter of the pipe sign weirdly enough. So here are, here are the names of
each movie. And so one of the nice things about Pandas, is it can do joins just like
SQL. And so you can use the merge method to combine the ratings table and the movies table
and since they both have a column called movie, by default it will join on those. And so now
here we have the ratings table with actual movie names, that's going to be a bit more
fun. we don't need it for modeling, but it's just going to be better for looking at stuff.
So we could use data blocks API at this point or we can just use the built in application
factory method. Since it's there we may as well use it, so we can create a collaborative
filtering data loaders object from a data frame by passing in the ratings table. By
default the user column is called �user�, and ours is, so fine. By default the item
column is called �item� and ours is not, it's called �title�, so let's pick �title�
and choose a batch size. And so if we now say show_batch, here is some of that data.
And the rating is called �rating� by default, so that worked fine, too. So here's some data.
So we need to now create our (assume we're going to use that five numbers of factors)
so the number of users is however many classes there are for user and the number of movies
is however many classes there are for title. And so these are, so we don't just have a
vocab now, right, we've actually got a list of classes for each categorical variable,
for each set of discrete choices. So we've got a whole bunch of users (944) and a whole
bunch of titles (1635). So for our randomized latent factor parameters, we're going to need
to create those matrices. so we can just create them with random numbers. This is normally
distributed random numbers, that's what randn is. And that will be n_users, okay, so 944
by n_factors, which is 5. That's exactly the same as this except this is just 15. So let�s
do exactly the same thing for movies. Random numbers and movies by 5, okay. And so to calculate
the result for some movie and some user we have to look up the index of the movie in
our movie latent factors, the index of the user in our user latent factors and then do
a cross product. So in other words we would say, Like oh okay, for this particular combination
we would have to look up that numbered user over here and that numbered movie over here
to get the two appropriate sets of latent factors. But this is a problem because look
up in an index is not a linear model. Like remember, our deep learning models really
only know how to just multiply matrices together and do simple element-wise nonlinearities
like ReLu. There isn't a thing called lookup in an index. Okay, I�ll just finish this
bit. Here's a cool thing, though - the lookup in an index can, is actually can be represented
as a matrix product, believe it or not. So if you replace our indices with one-hot encoded
vectors, then one-hot encoded vector times something is identical to looking up in an
index. And let me show you. So if we grab, if we call the one-hot function, that creates
a, as it says here, one-hot encoding. And we're going to one-hot encode the value 3
with n_users classes. And so n_users, as we discussed, is 944, all right, then. So if
we go one-hot one-hot encoding the number 3 in to n_users one_hot_3, you get this big
array big tensor and as you can see in index 3, here 1 2 3, we have a 1 and the size of
that is 924. So if we then multiply that by user_factors, our user_factors, remember,
is that random matrix of this size. Now what's going to happen? so we're going to go 0 by
the first row and so that's going to be your 0s
and then we're going to 0 again we're gonna 0 again and then we're going to finally go
1, right, on the index 3-row and so it's going to return each of them and then we'll go back
to 0 again. So if we do that (remember @ is matrix multiply) and compare that to user_factors_3,
same thing. Isn't that crazy? So. it�s some kind of wierd, inefficient way to do it, right
,but matrix multiplication is a way to index into an array. And this is the thing that
we know how to do SGD with, and we know how to build models with. So it turns out that
anything that we can do with indexing to array, we now have a way to optimize. And we have
a question. There are two questions. One, �How different in practice is collaborative
filtering with sparse data compared to dense data?� We are not doing sparse data in this
course, but there's an excellent course out here called Computational Linear Algebra for
Coders. It has a lot of information about sparse. The fast.ai course. A second question,
�In practice, do we tune the number of latent factors?� Absolutely, we do, yes. It's just
a number of filters like we have in at any kind of a big planning model. All right, so
now that we know that the procedure of finding out which latent set of latent factors is
the right thing, looking something up at an index is the same as matrix multiplication
with a one-hot vector (I already had it over here), we can go ahead and build a model with
that. So basically if we do this for a whole for a few more indices at once, then we have
a matrix of one-hot encoded vectors. So the whole thing is just one big matrix multiplication.
Now the thing is, as I said this is a pretty inefficient way to do an index lookup, so
there is a computational shortcut, which is called an embedding. An embedding is a layer
that has the computational speed of an array lookup and the same gradients as a matrix
multiplication. How does it do that? Well, just internally it uses an index lookup to
actually grab the values, and it also knows what the gradient of a matrix multiplication
by our one-hot encoded vector is, or matrix is, without having to go to all this trouble.
And so an embedding is a matrix multiplication with a one-hot encoded vector where you never
actually have to create the one-hot encoded vector. You just need the indexes. This is
important to remember because a lot of people have heard about embeddings and they think
they�re something special and magical, and, and they're absolutely not. You can do exactly
the same thing by creating a one-hot encoded matrix and doing a matrix multiply. It is
just a computational shortcut, nothing else. I often find when I talk to people about this
in person I have to tell them this six or seven times before they believe me because
they think embeddings are something more clever, and they're not. It's just a computational
shortcut to do a matrix multiplication more quickly with a one-hot encoded matrix by instead
doing an array lookup. Okay, so let's try and create a collaborative filtering model.
In PyTorch, a model, or an architecture, or really an nn.module is a class. So to use
PyTorch to its fullest, you need to understand object-oriented programming, because we have
to create classes. There's a lot of tutorials about this, so I won't go into detail about
it, but I'll give you a quick overview. A class could be something like Dog, or ResNet,
or Circle - and it's something that has some data attached to it and it has some functionality
attached to it. Here�s a class called �Example� - the data it has attached to it is �a�
and the functionality attached to it is �say�. And so we can, for example, create an instance
of this class, an object of this type Example. We pass in �Sylvain�, so �Sylvan�
will now be in ex.a and we can then say �ex.say� and it will call �say� and it will say
passing it �nice to meet you� so that will be X. And so it'll say, �Hello� self.a,
so that�s �Sylvian, nice to meet you�. There, it is. Okay, so in Python the way you
create a class is to say class and its name, then to say what is passed to it when you
create that object, it's a special method called __init__. As we've briefly mentioned
before, in Python there are all kinds of special method names of that special behavior - they
start with two underscores they end with two underscores and we pronounce that �dunder�,
so �dunder init�, __init__. All methods in, all regular methods instance methods in
Python always get passed the actual object itself first (we normally call that self)
and then optionally anything else. And so you can then change the contents of the current
object by just setting self.whatever to whatever you like. So after this self.a is now equal
to �Sylvian�. So we call a method, same thing - get passed self, optionally anything
you pass to it. And then you can access the contents of self which you stashed away back
here when we initialized it. So that's basically how object, or you know the basics of how
object-oriented programming works in Python. There's something else you can do
when you create a new class, which is you can pop something in parentheses after its
name, and that means we're going to use something called inheritance. And what inheritance means
is, I want to have all the functionality of this class, plus I want to add some additional
functionality. So, Module is a PyTorch class, which fast.ai has customized, so it's kind
of a fast.ai version of a PyTorch class. And, probably in the next course we'll see exactly
how it works. And, but it looks a lot like a, it acts almost exactly like just a regular
Python class - we have an __init__, and we can set attributes to whatever we like. And
one of the things we can use is an Embedding. And so an Embedding is just this class that
does what I just described - it's the same as an, as a linear layer with a one-hot encoded
matrix, but it does it with this computational shortcut. You can say how many, in this case
users are there, and how many factors will they have. Now, there is one very special
thing about things that inherit from Module, which is that when you call them, it will
actually call a method called �forward�. So �forward� is a special PyTorch method
name. It's the most important PyTorch method name. This is where you put the actual computation.
So to grab the factors from an embedding, we just call it like a function, right. So
this is going to get passed here the user IDs and the movie IDs as two columns. So let's
grab the zero index column and grab the embeddings by passing them to user_factors. And then
we'll do the same thing for the index one column, that's the movie IDs, pass them to
the movie_factors. And then here there's our element-wise multiplication and then sum.
And now remember we've got another dimension this time. The first axis is the mini-batch
dimension, where we want to sum over the other dimension, the index one dimension. So that's
going to give us a dot product for each user, so for each rating for each user movie combination.
So this is the DotProduct class. So you can see if we look at one batch of our data, its
of size, shape 64x2, because there are 64 items in the mini batch and each one has,
this is the independent variables, so it's got the user ID and the movie ID. And, oh,
�Do deep neural network based models for collaborative filtering work better than more
traditional approaches like SVG or other matrix?� Let's wait until we get there. So, here is
X, right, so here is one user ID movie ID combination, okay. And then for each one of
those 64, here are the ratings. So now we created a DotProduct Module from scratch,
so we can instantiate it, passing in the number of users, the number of movies, and let's
use 50 factors, and now we can create a learner. Now this time we're not creating a cnn_learner
or a specific application learner, It's just a totally generic learner. So this is a learner
that doesn't really know how to do anything clever, it just draws away the data you give
it and the model you give it. And since we're not using an application-specific learner,
it doesn't know what loss function to use, so we'll tell it to use MSE and fit. And that's
it, right. So we've just fitted our own collaborative filtering model where we literally created
the entire architecture, it's a pretty simple one, from scratch.
So that's pretty amazing. Now the results aren't great, if you look at the movie lens
data set benchmarks online you'll see this is not actually a great result. So one of
the things we should do is take advantage of the tip we just mentioned earlier in this
lesson, which is when you're doing regression, which we are here, right. The number between
1 and 5 is like a continuous value we're trying to get as close to it as possible. We should
tell fastai what the range is, we can use the y_range as before, so here's exactly the
same thing we've got a y_range, we've stored it away and then at the end, we use as we
discussed, sigmoid_range, passing in, and look here, we pass in *self.y_range, that's
going to pass in by default 0,5.5. We can see, yeah not really any better, um it's worth
a try, normally this is a little bit better but it always depends on when you run it.
I just run it a second time while it's weird looking. Um, now there is something else we
can do though, which is that if we look back at our little Excel version. The thing is
here, when we multiply these latent factors by these latent factors and add them up, it's
not really taking account of the fact that, this user, may just rate movies really badly
in general, regardless of what kind of movie they are. And this movie, might be just a
great movie in general because everybody likes it, regardless of what kind of stuff they
like. And so it'd be nice to be able to represent this directly, and we can do that using something
we�ve already learned about, which is bias, we could have another single number for each
movie which we just add, and add another single number for each user which we just add, right,
now we've already seen this for linear models. You know this idea that it's nice to be able
to add a bias value. So let's do that. So that means that we're going to need another
embedding for each user, which is of size one it's just a single number we're going
to add, so in other words it's just an array lookup, but remember to do an array lookup
that we can kind of, take a gradient of, we have to say embedding. We will do the same
thing for movie bias and so then, all of this is identical as before and we just add this
one extra line, which is to add the user and movie bias values. And so let's train that
and see how it goes. Well, that was a shame it got worse, so we used to have, not finished
here, 0.87 0.88, 0.89, so it's a little bit worse. Why is that, well if you look earlier
on it was quite better it was point 0.86, so it's overfitting very quickly and so what
we need to do is, we need to find a way that we can train more epochs without overfitting.
Now we've already learned about data augmentation right like rotating images and changing their
brightness and color and stuff, but it's not obvious how we would do data augmentation
for collaborative filtering right. And so how are we going to make it so that we can
train lots of epochs without overfitting. And to do that we're going to have to use
something called regularization, and regularization is a set of techniques which basically allow
us to use models with lots of parameters and train them for a long period of time, but
penalyze them effectively for overfitting. Or, in some way cause them to try to stop
overfitting and so that is what we will look at next week. Okay well thanks everybody,
so there's a lot to take in here so please remember to practice to experiment but listen
to the lessons again because you know for the next couple of lessons things are going
to really quickly build on top of all the stuff that we've learnt. So please be as comfortable
with it as you can, feel free to go back and re-listen to it and go through and follow
through the notebooks, and then try to recreate as much of them yourself. Thanks everybody
and I will see you next week or see you in the next lesson whenever you watch it

Hi everybody and welcome to lesson 7! We're going to start by having a look at a kind
of regularization called weight decay. And the issue that we came to at the end of the
last lesson, is that we were training our simple dot product model with bias, and our
loss started going down, and then it started going up again. And so we have a problem that
we are overfitting. And remember in this case we're using mean squared error, so try to
recall why it is that we don't need a metric here. Because mean squared error is pretty
much the thing we care to care about really. Or we could use mean absolute error if we
like, but either of those works fine as a loss function. They don't have the problem
of big flat areas like accuracy does classification. So what we want to do is to make it less likely
that we're going to overfit by doing something we call reducing the capacity of the model.
The capacity of the model is basically how much space does it have to find answers. And
if it can kind of find any answer anywhere those answers can include basically memorizing
the dataset. So, one way to handle this would be to decrease the number of latent factors.
But generally speaking, reducing the number of parameters in a model, particularly as
we look at more deep learning style models, ends up biasing the models towards very simple,
kind of, shapes. So there's a better way to do it. Rather than reducing the number of
parameters, instead we try to force the parameters to be smaller unless they're really required
to be big. And the way we do that is with weight decay. Weight decay is also known as
l2 regularization. They're very slightly different but we can think of them as the same thing.
And what we do is we change our loss function and specifically we change the loss function
by adding to it, the sum of all the weights squared. In fact all of the parameters squared,
really should say. Why do we do that? Well because if that's part of the loss function,
then one way to decrease the loss, would be to decrease the weights. One particular weight,
or all of the weights, or or something like that. And so when we decrease the weights,
if you think about what that would do, then think about, for example, the different possible
values of a in y equals ax squared. The larger a is for example a is 50, you get these very
narrow peaks. In general big coefficients are going to cause big swings, big changes
in the, in the loss for small changes in the parameters. And when you have these, kind
of, sharp peaks or valleys, it means that a small change to the parameter, can make
a, sorry, small change to the input, can make a big change to the loss. And so, if you have,
if you're in that situation, then you can basically fit all the data points close to
exactly with a really complex jagged function with sharp changes which exactly tries to
sit on each data point rather than finding a nice smooth surface which connects them
all together or goes through them all. So if we limit our weights, by adding in the
loss function, the sum of the weights squared, then what is going to do is, it's going to
fit less well on the training set because we're giving it less room to try anything
that it wants to but we're going to hope that it would result in a better loss on the validation
set or the test set so that it will generalize better. One way to think about this is that
the loss with weight decay is just the loss plus the sum of the parameters squared, times
number we pick, a hyper parameter, sometimes it's like .1, 0.01 or 0.001 kind of region.
This is basically what loss with weight decay looks like as an equation. But remember, when
it actually comes to what's, how was the loss used in
stochastic gradient descent it's used by taking its gradient so what's the gradient of this,
well if you remember back to when you first learn calculus, it's okay if you don't, the
gradient of something squared is just two times that's something we change from parameters
to weight which is a bit confusing though this used weight here to keep it consistent
maybe parameters is better, so the derivative of weight squared is just 2 * weight, so in
other words to add in this term to the gradient we can just add to the gradients weight decay
* 2 * weight and since weight decay is just a hyper parameter we can just replace it with
weight decay * 2 so that would just give us weight decay times weight though weight decay
refers to adding on the to the gradients the weights times some hyperparameter and so that
is going to try to create these kind of more shallow, less bumpy, surfaces do that simply
when we call fetch will fit one cycle or whatever we can pass in a wd parameter and that's just
this number here, so if we pass in point one then the training loss goes from .29 to 0.49
that's much worse right because we can't overfit anymore but the valid loss goes from 0.89
to 0.82 much better so this is an important thing to remember for those of you that have
done a lot of more traditional statistical models is in kind of more traditional statistical
models we try to avoid overfitting and we try to increase generalization by decreasing
the number of parameters but in a lot of modern machine learning and certainly deep learning
we tend to instead use regularization such as weight decay because it gives us more flexibility
it lets us use more non-linear functions and still avoid you know still reduces the capacity
of the model great so we're down to 0823 this is a good model this is really actually a
very good model and so let's dig into actually what's going on here because in our in our
architecture remember we basically just had or embedding layers or what's an embedding
layer we've described it conceptually but let's write our own and remember we said that
an embedding layer is just a computational shortcut for doing a matrix multiplication
by a one hot and coded matrix and that, that is actually the same as just indexing into
an array so it embedding is just a indexing into an array and so it's nice to be able
to create our own versions or things that exist in PyTorch and Fast.ai so let's do that
for everybody so if we're going to create our own kind of layer which is pretty cool
we need to be aware of something which is normally a layer is basically created by inheriting
as we've discussed from module or NN module so for example this is an example here of
a module where we've created a class called T that inherits from module and when it's
constructed remember that's what dunder init (__init__) does we're just going to sit this
is just a dummy little module here we're gonna set self taught a through the number one repeated
three times as a tensor, now if you remember back to notebook4 we talked about how the
optimizers in PyTorch and Fast.ai rely on being able to grab the parameters attribute
to find a list of all the parameters now if you want to be able to optimize self.a you
would need to it to appear in parameters but actually there's nothing there why is that
that's because PyTorch does not assume that everything that's in a module is something
that needs to be learnt the tell us that it's something that needs to be learned you have
to wrap it with nn.Parameter so here's exactly the same class but torch.ones which is just
a list of three ones In this case. It is wrapped in nn.Parameter().
And now if I go T().parameters(), I see I have a parameter: the three ones in it. And
that's going to automatically call requires_grad_() for us as well. We haven't had to do that
for things like nn.Linear() in the past because PyTorch automatically uses nn.Parameter()
internally. So if we have a look at the parameters for something that uses nn.Linear() with no
bias layer you'll see again we have here parameter with three things in it. So we want to in
general be able to create a parameter, a tensor with a bunch of things in it and generally
we want to randomly initialize them. So to randomly initialize we can pass in the size
we want, we can initialize a tensor of zeroes of that size, and then randomly generates
some normally distributed random numbers with a mean of 0 and a deviation of 0.01. No particular
reason I'm picking those numbers just know how this works. So here is something that
will give us back our parameters of any size we want. And so now we're going to replace
everywhere that used to say embedding, I've got to replace it with create_params. Everything
else here is the same in the __init__(). And then the forward() is very very similar to
before; as you can see I'm grabbing the zero index column from X, that's my users, and
I just look it up as you see in that user_factors array. And the cool thing is I don't have
to do anything with gradients myself for this manual embedding layer because PyTorch can
figure out the gradients automatically as we've discussed but then I just got the dot
product as before, add on the bias as before, do the sigmoid range as before, and so here's
a DotProductBias() without any special PyTorch layers and we fit and we get the same result.
So I think that is pretty amazingly cool. We've really shown that the embedding layer
is nothing fancy, is nothing magic right it's just indexing into an array. So hopefully
that removes a bit of the mystery for you. So let's have a look at this model that we've
created and we've trained and find out what it's learned. Its� already useful, we've
got something we can make pretty accurate predictions with. But let's find out what
those... what the model looks like. Remember when we create � [Rachel] we have a question.
[Jeremy]okay let's take a question before we look at this. [Rachel] what's the advantage
of creating our own embedding layer over the stock PyTorch one?. [Jeremy] oh nothing at
all. We're just showing that we can. It's great to be able to dig under the surface
because at some point you'll want to try doing new things. So a good way to learn to do new
things is to be able to replicate things that already exist and you can check you understand
how they work. It's also a great way to understand the foundations of what's going on is to actually
create in code your own implementation but I wouldn't expect you to use this implementation
in practice. But basically it removes all the mystery. So if you remember we've created
a learner called learn and to get to the model that's inside it you can always call learn.model
and then inside that there's going to be automatically created for it... well sorry not automatically...
we've created all these attributes movie_factors movie_bias user_bias and so forth...where
we can grab learn.model.movie_bias. And now what I'm going to do is I'm going to sort
that vector and I'm going to print out the first five idxs. And so what this is going
to do is it's going to print out though the movies with the smallest bias and here they
are. What does this mean? Well it kind of means these are the five movies that people
really didn't like. But it's more than that. It's not only do people not like them but
if we take account of the genre they're in the actors they have you
know whatever the latent factors are, people liked them a lot less than they expected so
maybe, for example people... this is kind of... I haven't seen any of these movies luckily
perhaps this is a sci-fi movie so people who kind of like these sci-fi movies found it
so bad they still didn't like it. So we can do the exact opposite which is to sort ascending
and here are the top five movies and specifically they're the top five by bias right. So these
are the movies that even after you take account of the fact that � LA Confidential... I
have seen all of these ones...So la confidential is a kind of a murder mystery cop movie I
guess and people who don't necessarily like that genre ...or I think Guy Pearce was in
it so maybe they don't like Guy Pearce very much whatever ...people still like this movie
more than they expect. So this is a kind of a nice thing that we can look inside our model
and see what it's learned. Now we can look out not only at the bias vector but we can
also look at the factors. Now there are 50 factors which is too many to visualize so
we could use a technique called pca principal components analysis. The details don't matter
but basically they're going to squish those 50 factors down to 3 and then we'll plot the
top two as you can see here. And what we see when we plot the top 2 is we can kind of see
that the movies have been spread out across a space of some kind of latent vectors. So
if you look at the far right there's a whole bunch of kind of big budget actiony things
and on the far left there's more like cult kind of things Fargo, Schindler's List, Monty
Python. By the same token at the bottom we've got some English Patient, Harry Met Sally
so kind of romance drama kind of stuff and at the top we've got action and sci-fi kind
of stuff. So you can see even as though we haven't passed in any information about these
movies, what we've seen is who likes what, these latent vectors have automatically kind
of figured out a space or a way of thinking about these movies based on what kinds of
movies people like and what are other kinds of movies they like along with those. That's
really interesting to kind of try and visualize what's going on inside your model. Now we
don't have to do all this manually. We can actually just say give me a collab_learner
using this set of data loaders with this number of factors in this y_range and it does everything
we've just seen again about the same number. Okay?Well now you can see... this is nice
right... we've actually been able to see right underneath inside the collab_learner part
of the FastAI application the collaborative filtering application and we can build it
all ourselves from scratch we know how to create the SGD know how to create the embedding
layer we know how to create the model the architecture so now you can see you know we've
really built up from scratch our own version of this. So if we just type learn.model you
can see here the names are a bit more generic this is a user weight item weight user bias
item bias but it's basically the same stuff we've seen before and we can replicate the
exact analyses we saw before by using this same idea okay? Slightly different order this
time because it is a bit random but pretty similar as well. Another interesting thing
we can do is we can think about the distance between two movies. So let's grab all the
movie factors or just pop them into a variable and then let's pick a movie and then let's
find the distance from that movie to every other movie and so one way of thinking
about distance is you might recall the Pythagorean formula or the distance on the hypotenuse
of a triangle which is also the distance to a point in a Cartesian plane on a chart which
is root x squared plus y squared you might know... doesn't matter if you don't... but
you can do exactly the same thing for 50 dimensions. It doesn't just work for two dimensions. There's
a... that tells you how far away a point is from another point if x and y are actually
differences between two movie vectors. So then what gets interesting is you can actually
then divide that kind of by the length to make all the lengths the same distance to
find the angle between any two movies... and that actually turns out to be a really good
way to compare the similarity of two things that's called cosine similarity ...and so
the details don't matter you can look them up if you're interested �.but the basic
idea here is to see that we can actually pick a movie and find the movie that is the most
similar to it based on these factors. Kind of interesting. [Rachel] we have a question.
[Jeremy] alright. [Rachel] what motivated learning at a 50 dimensional embedding and
then using pca to reduce to three versus just learning a few dimensions? [Jeremy] because
the purpose of this was actually to create a good model though the visualization part
is normally kind of the exploration of what's going on in your model and so with 50 latent
factors you're going to get a more accurate view. So that's one approach is this dot product
version. There's another version we could use which is we could create a set of user_factors
and a set of item_factors and the just like before we could look them up. But what we
could then do instead of doing a dot product is we could concatenate them together into
a tensor that contains both the user and the movie factors next to each other and then
we could pass them through a simple little neural network: Linear, ReLU, Linear and then
sigmoid_range as before. So importantly here the first linear layer the number of inputs
is equal to the number of user_factors plus the number of item_factors and the number
of outputs is however many activations we have and which we just default to a hundred
here and then the final layer will go from a hundred to one because we're just making
one prediction. So we could create or call that collabNN and we can instantiate that
to create a model we can create a learner and we can get. It's not going quite as well
as before it's not terrible but it's not quite as good as our dot product version. But the
interesting thing here is it does give us some more flexibility which is that since
we're not doing a dot product we can actually have a different embedding size for each of
users versus items and actually FastAI has a simple heuristic if you call get_emb_sz()
and pass in your data loaders it will suggest appropriate size embedding matrices for each
of your categorical variables, each of your user and item enters. so that's if we pass
in *emb that's going to pass in the user tuple and the item tuple which we can then pass
to embedding. This is star prefix we learned about in the last class in case you forgot.
So this is kind of interesting we can you know we can see here that there's two different
architectures we could pick from, it wouldn't be necessarily obviously ahead of time which
one's going to work better, I mean in this particular case the simplest one the the dot
product one actually turned out to work a bit better which is interesting. This particular
version here if you call, �collab_learner� and pass
use_nn=True, then what that's going to do is, it's going to use this version, the version
with concatenation and the linear layers. So �collab_learner� use_nn=True, again
we get about the same result as you'd expect, because it's just a raw cut for this version,
and it's interesting actually, we have a look at �collab_learner�, it actually returns
an object of type �embeddingNN�, and it's kind of cool, if you look inside the fast
AI source code or use the double question mark trick to see the source code for embeddingNN,
you'll see it's three lines of code. How does that happen? Because we're using this thing
called �TabularModel�, which we will learn about in a moment, but basically this neural
net version of collaborative filtering is literally just a �TabularModel� in which
we pass no continuous variables and embedding sizes. So we'll see that in a moment. OK,
so that is collaborative filtering and again take a look at the further research section
in particular, after you finish the questionnaire and because there's some really important
next steps you can take to push your knowledge and your skills. So let's now move to Notebook
9 Tabular, and we're going to look at tabular modeling and do a deep dive, and let's start
by talking about this idea that we were starting to see here, which is embeddings, and specifically
let's move beyond just having embeddings for users and items, at embeddings for any kind
of categorical variable, but really because we know an embedding is just a lookup into
an array, it can handle any kind of discrete categorical data. So things like age are not
discrete they�re continuous numerical data, but something like sex or postcode are categorical
variables, they have a certain number of discrete levels. The number of discrete levels they
have is called their cardinality. So to have a look at an example of a data set that contains
both categorical and continuous variables, we're going to look at the �Rossman sales
competition� that ran on Kaggle a few years ago, and so basically what's going to happen
is we're going to see a table that contains information about various stores in Germany,
and the goal will be to try and predict how many sales there's going to be, for each day
in a couple of weeks period, for each store. One of the interesting things about this competition
is that one of the gold medalists used deep learning, and it was one of the earliest known
example of a state of the art deep learning tabular model. I mean, this is not long ago
2015 or something, but really this idea of creating state of the art tabular models with
deep learning has not been very common, and for not very long. You know, interestingly
compared to the other gold medalists in this competition, the folks that used deep learning
used a lot less feature engineering, and a lot less domain expertise, and so they wrote
a paper called �Entity Embeddings of Categorical Variables�, in which they basically described
the exact thing that you saw in the in Notebook 8, the way you can think of one hot encodings
as just being embeddings you concatenate them together and you can put them through a couple
of layers, they call them dense layers, we call them linear layers, and create a neural
network out of that. So this is really neat, you know, it's, kind of, simple and obvious,
in hindsight, trick, and they actually did exactly what we did in the paper, which is
to look at the results of the trained embeddings, and so for example they had an embedding matrix
for regions in Germany, because there was... What there wasn't really metadata about this,
these were just learnt embeddings, just like we learnt embeddings about movies and so
then they just created, just like we did before, a chart where they popped each region, according
to, I think probably a PCA of their embeddings, and then if you circle the ones that are close
to each other, in blue, you'll see that they're actually close to each other in Germany, and
ditto for red, and ditto for green, and then here's the brown. So this is, like, pretty
amazing, is the way that we can see that it's, kind of, learnt something about what Germany
looks like based entirely on the purchasing behavior of people in those states. Something
else they did was to look at every store, and they looked at the distance between stores
in practice, like how many kilometers away they are, and then they looked at the distance
between stores in terms of their embedding distance, just like we saw in the previous
notebook, and there was this very strong correlation, that stores that were close to each other
physically, ended up having close embeddings as well, even as though the actual location
of these stores in physical space was not part of the model. Ditto with days of the
week. So the days of the week are another embedding, and the days of the week that were
next to each other, ended up next to each other in embedding space, and ditto for months
of the year. So pretty fascinating the way, kind of, information about the world ends
up captured just by looking at browning embeddings, which as we know are just index lookups into
an array. So the way we then combine these categorical variables, with these embeddings,
with continuous variables... What was done in both the entity embedding paper, that we
just looked at, and then also described in more detail by Google, when they described
how their recommendation system in Google Play works. This is from Google's paper. As
they have the categorical features, that go through the embeddings, and then there are
continuous features, and then all the embedding results, and the continuous features are just
concatenated together, into this big concatenated table, that then goes through, in this case,
three layers of a neural net, and interestingly they also take the, kind of, collaborative
filtering bit, and do the dot product as well and combine the two. So they use both of the
tricks we used in the previous notebook and combine them together. So that's the basic
idea we're going to be seeing or proving beyond just collaborative filtering, which is just
two categorical variables, to as many categorical and as many continuous variables as we like,
but before we do that let's take a step back and think about other approaches, because
as I mentioned, the idea of deep learning as a, kind of a, best practice for tabular
data, it is still pretty new and it's still kind of controversial. It's certainly not
always the case but it's the best approach. So when we're not using deep learning, what
would we be using? Well what would probably be using is something called an ensemble of
decision trees, and the two most popular are Random Forests, and Gradient Boosting Machines
or something in the line. So basically between multi-layered neural networks learnt with
SGD, and ensembles of decision trees that, kind of, covers the vast majority of approaches
that you're likely to see for tabular data, and so we're going to make sure we cover them
both, of course. Today in fact. So although deep learning is nearly always clearly superior
for stuff like images, and audio, and natural language texts, these two approaches tend
to give somewhat similar results a lot of the time for tabular data. So let's take a
look. Some� You know, you really should generally try both and see which works best
for you for each problem you look at. Why does the range go from 0 to 5.5, if the
maximum is 5? That's a great question. Um, the reason is, if you think about it, for
sigmoid it's actually impossible for a sigmoid to get all the way to the top or all the way
to the bottom: there's a asymptote, so, no matter how far, how big your X is, it can
never quite get to the top, or no matter how small it is, it can never quite get to...so
if you want to be able to actually predict a rating of 5, then you need to use something
higher than 5 as your maximum. Are embeddings used only for highly cardinal categorical
variables, or is this approach general? For low cardinality can one use one-hot encoding?
I�ll remind you cardinality is the number of discrete levels in variable, and remember
that an embedding is just a computational shortcut for a one-hot encoding, so there's
really no reason to use a one-hot encoding because, as long as you have more than two
levels, it's always going to be more memory and slower, and give you exactly mathematically
the same thing. And if there's just two levels, then it is basically identical. So there isn't
really any reason not to use it. Thank you for those great questions. Okay, so, one of
the most important things about decision tree ensembles is that at the current state of
the technology, they do provide faster and easier ways of interpreting the model. I think
that's rapidly improving for deep learning models on tabular data, but that's where we
are right now. They also require less hyperparameter tuning, so they're easier to kind of get right
the first time, so my first approach analyzing a tabular dataset is always an ensemble of
decision trees, and specifically I pretty much always start with a random forest, because
it's just so reliable. Yes! Your experience for highly imbalanced datasets such as fraud
or medical data, what usually is best out of random forest, XGboost or neural networks?
Um, I'm not sure that whether the data is balanced or unbalanced is a key reason for
choosing one of those about the others. I would try all of them and see which works
best. So the exception to the guideline about starting with decision tree ensembles as your
first thing to try, would be if there's some very high cardinality categorical variables,
then they can be a bit difficult to get to work really well in decision tree ensembles.
Or if there's something like, most importantly if it's like plain text data or image data
or audio data or something like that, and then you'll definitely kind of need to use
a neural net in there. But you could actually ensemble it with a random forest as we'll
see. Okay, so, really we're going to need to understand how decision tree ensembles
work. So, PyTorch isn't a great choice for decision tree ensembles: they're really designed
for gradient-based methods and random forests and decision tree growing are not really gradient-based
methods in the same way. So, instead we're going to use a library called scikit-learn,
referred to as sklearn as a module. Scikit-learn does a lot of things, we're only going to
touch on a tiny piece of them, what we need to do to train decision trees and random forests.
We've already mentioned before Wes McKinney's book, also a great book for understanding
more about scikit-learn. So, the dataset for learning about decision tree ensembles is
going to be another dataset, it's going to, it's called the Blue Book for Bulldozers dataset,
and it's a Kaggle competition. So, Kaggle competitions are fantastic, they are machine
learning competitions where you get interesting datasets, you get feedback on whether your
approach is any good or not, you can see on a leaderboard what approaches are working
best and then you can read blog posts from the winning
contestants sharing tips and tricks. It's certainly not a substitute for actual practice
doing end-to-end data science projects, but for becoming good at creating predictive models
that are predictive it's a really fantastic resource. Highly recommended, and you can
also submit to most old competitions to see how you would have gone without having to
worry about, you know, the kind of stress of like whether people will be looking at
your results, because they're not publicized or published if you do that. There�s a question:
can you comment on real time applications of random forests? In my experience they tend
to be too slow for real-time use cases, like a recommender system neural network is much
faster when run on the right hardware. Let's get to that once we see what they are, shall
we? Now you can't just download and untar Kaggle datasets using the untar data thing
that we have in fastai, so you actually have to sign up to Kaggle and then follow these
instructions for how to download data from Kaggle. Make sure you replace creds here with
what it describes; you need to get a special API code and then run this one time to put
that up on your server. And now you can use Kaggle to download data using the API. So
after we do that we're going to end up with a bunch of, as you see, CSV files. So let's
take a look at this data. So the main data, the main table, is trained on CSV -- remember
that's comma separated values -- and the training set contains information such as unique identifier
of the sale, or the unique identifier of the machine, sale price sale date. So what's going
on here is one row of the data represents a sale of a single piece of heavy machinery
like a bulldozer at an auction. So it happens at a date, has a price, it's of some particular
piece of equipment and so forth. So if we use pandas again to read in the CSV file (let's
combine training and valid together) we can then look at the columns to see. There's a
lot of columns there and many things which I don't know what the hell they mean, like
blade extension and pad type and ride control, but the good news is we're going to show you
a way that you don't have to look at every single column and understand what they mean,
and random forests are going to help us with that as well. So once again we're going to
be seeing this idea that models can actually help us with data understanding and data cleanup.
One thing we can look at is ordinal columns, a good place to look at that now. If there's
things there that you know are discrete values that have some order like product size it
has medium and small and large medium and mini, these should not be in you know alphabetical
order or some random order; they should be in this specific order, right? They have a
specific ordering so we can use .astype to turn it into a categorical variable, and then
we can say set categories ordered equals true, to basically say this is an ordinal column.
So it's got discrete values, but we actually want to define what the order of the classes
are. We need to choose which is the dependent variable and we do that by looking on Kaggle,
and Kaggle will tell us that the thing we're meant to be predicting is sale price. And
actually, specifically, they'll tell us the thing we're meant to be predicting is the
log of sale price, because root mean squared log error is the -- what we're actually going
to be judged on in the competition, where we take the log. So we're going to replace
sale price with its log, and that's what we'll be using from now on. So a decision tree ensemble
requires decision trees -- well let's start by looking at decision trees. So a decision
tree in this case is a, something that asks a series of binary, that is yes or no, questions
about data. So such as is somebody less than 30? Yes they are. Are they eating healthily?
Yes they are and so okay, then we're going to say they're fit or unfit. So like there's
an example of some arbitrary decision tree that somebody might have come up with. It's
a series of binary yes and no choices and at the bottom are leaf nodes that make some
prediction. Now of course for our bulldozers competition we don't know what binary questions
to ask about these things, and in what order, in order to make a prediction about sale price.
So we're doing machine learning, so we're going to try and come up with some automated
way to create the questions. And there's actually a really simple procedure for doing that.
You have a think about it. So if you want to kind of stretch yourself here, have a think
about what's an automatic procedure that you can come up with that would automatically
build a decision tree, where the final answer would do a, you know, significantly better
than random job of estimating the sale price of one of these options. Right so here's the
approach that we could use. Loop through each column of the data set. We're going to go
through each of (but obviously not sale prices, the dependent variable) sale ID, machine ID,
auctioneer, year made etc. And so one of those will be, for example, product size And so
then what we're going to do is we're going to loop through each possible value of product
size (large, large-medium, medium, etc.) and then we're going to do a split basically like
where this comma is. And we're going to say okay let's get all of the options of large
equipment and put that into one group, and everything that's smaller than that and put
that into another group, and so that's here: split the data into two groups based on whether
they're greater than or less than that value. If it's a categorical non-ordinal variable,
it'll be just whether it's equal or not equal to that level. And then we're going to find
the average sale price for each of the two groups. So for the large group what is the
average sale price? For the smaller than large group what was the average sale price, and
that will be our model. Our prediction will simply be the average sale price for that
group. And so then you can say, well how good is that model? If our model was just to ask
a single question with a yes/no answer, put things into two groups and take the average
of the group as being our prediction, and we can see how good would that model be? What
would be the root mean squared error from that model? And so we can then say all right,
how good it would it be if we used large as a split. And then let's try again -- what
if we did large/medium as a split? What if we did medium as a split? And so in each case
we can find the root mean squared error of that incredibly simple model. And then once
we've done that for all of the product size levels, we can go to the next column and look
at -- have a look -- UsageBand and do every level of UsageBand. And then state -- your
level or state and so forth. And so there'll be some variable and some split level which
gives the best root mean squared error of this really, really simple model. And so then
I'll say okay that would be our first binary decision:, it gives us two groups and then
we're going to take each one of those groups separately and find another single binary
decision for each of those two groups using exactly the same procedure. And then we'll
have four groups, and then we'll do exactly the same thing again separately for each of
those four groups, and so forth. So let's see what that looks like. And, in fact, once
we�ve gone through this, you might even want to see if you can implement
this algorithm yourself. It's not trivial but it doesn't require any special coding
skills so hopefully you can find you will be able to do it. There's a few things we
have to do before we can actually create a decision tree in terms of just some basic
data munging. One is if we're going to take advantage of dates, we actually want to call
fastai�s add_date_part function. And what that does is it creates a whole bunch of different
bits of metadata from that data. Sale year, sale month, sale a week, sale day and so forth.
The sale date, of itself doesn't have a whole lot of information directly, but we can pull
lots of different information out of it. This is an example of something called feature
engineering, which is where we take some piece of data and we try to create lots of other
pieces of data from it. So is this particular date at the end of a month, or not? At the
end of a year or not? And so forth, but that handles dates. There's a bit more cleaning
we want to do, and fastai provides some things to make that easier. We can use the TabularPandas
class. We create a tabular data set in pandas, and specifically we're going to use two tabular
processors, (procs). A tabular processor is basically just to transform, and we�ve seen
transforms before, so go back and remind yourself what a transform is. In, like three lines
of code it�s actually going to modify the object in place rather than creating a new
object and giving it back to you. That's because often these tables of data are kind of really
big and we don't want to waste lots of RAM. And it's just gonna run the transform once
and save the result rather than doing it lazily when you access it, but for the same reason
we just want to make this a lot faster, so just think of them as transforms really. So
one of them is called categorify, which is going to replace a column with numeric categories
using the same basic idea like a vocab like we've seen before. fill_missing is going to
find any columns with missing data and it's going to fill in the missing data with the
median of the data and additionally create a new column, a boolean column which is set
to True for anything that was missing. These two things are basically enough to get you
to a point where most of the time you'll be able to train a model. Now the next thing
we need to do is think about our validation set as we discussed in lesson one, a random
validation set is not always appropriate, and certainly for something like predicting
auction results it almost certainly is not appropriate because we're going to be wanting
to use a model in the future, not at some random date in the past. So the way this Kaggle
competition was set up was that the test set, the thing that you had to fill in and submit
for the competition, was two weeks of data that was after any of the training set. So
we should do the same thing for the validation set. We should create something which is where
the validation set is the last couple of weeks of data, and so then the training set will
only be data before that. So we basically can do that by grabbing everything before
October 2011. I'll create our training and validation sets based on that condition, and
grabbing those bits. So that's going to spit our training set and validation set by date,
not randomly. We're also going to need to tell when you create a TabularPandas object,
you're going to be passing in a data frame, your tabular procs, and you also have to say
what are your categorical and continuous variables. We can use fastai�s conte_cat_split(), which
automatically splits a data frame to continuous and categorical variables for you. So we can
just pass those in, tell it what is the dependent variable -- you can have more than one -- and
what are the indexes to split into training and validation sets. And this is a Tabular
object, so it's got all the information you need about the training set ,the validation
set, the categorical and continuous variables, and the dependent variable, and any processes
to run. It looks a lot like a Datasets object but has .train and .dot valid properties,
and so if we have a look at .show(), we can see the data. But .show() is going to show
us the string data, but if we look at .items() you can see internally it's actually stored
in these very compact numbers which we can use directly in a model.
So fastai has basically got us to a point where our training and validation set is created
and the data is in a format ready for modeling. To see how these numbers relate to these strings
we can again, just like we saw last week, use the .classes attribute which is a dictionary,
it basically tells us the vocab. So this is how we look up, for example that ProductSize
category 6 is �Compact�. That processing takes a little while to run, so you can go
ahead and save the Tabular object, so then you can load it back later without having
to rerun all the processing. So that's a nice kind of fast way to quickly get back up and
running without having to reprocess your data. So we've done the basic data munging we need.
So we can now create a decision tree, and in scikit-learn the decision tree model where
the dependent variable is continuous is called a DecisionTreeRegressor. Now let's start by
telling it we just want a total of four leaf nodes (we'll see what that means in a moment).
In scikit-learn you generally call fit so it looks quite a lot like fastai. And you
pass in your independent variables and your dependent variable, and we can grab those
straight from our TabularObject training set. It�s train.xs and train.y and we can do
the same thing for validation. Question: do you have any thoughts on what
data augmentation for tabular data might look like?
Answer: I don't have a great sense of data augmentation with tabular data. We'll be seeing
later either in this course, or in the next part ,dropout and mixup and stuff like that,
which they might be able to do that in later layers in the tabular model. Otherwise I think
you'd need to think about the semantics of the data, and think about what are things
you could do to change the data without changing the meaning.
Question: does fastai distinguish between ordered categories such as low, medium, high,
and unordered categories? Answer: that that was that ordinal thing I
told you about before and all it really does is it ensures that your classes list has a
specific numerical order, and as you'll see that's actually going to turn out to be pretty
important for how we train our Random Forest. Okay so we can create a DecisionTreeRegressor,
we can fit it, and then we can draw it, using the fastai function draw_tree(), and here
is the decision tree we just trained. Behind the scenes this actually used basically the
exact process that we described back here. Right so this is where you can try and create
your own decision tree implementation if you're interested in stretching yourself. So we're
going to use an implementation that already exists, and the best way to understand what
it's done is to look at this diagram from top to bottom. So this first step says the
initial model it created is a model with no binary splits at all. Specifically it's always
going to predict the value 10.1, for every single row. Why is that? Well, because this
is the simplest possible model -- to always predict the average of the dependent variable.
So this pretty much your baseline for regression. There are 404,710 rows that we're averaging,
and the mean squared error (MSE) of this incredibly simple model in which there are no rules at
all is just a single average is 0.48. So then, the next most complex model is to take a single
column, Coupler_System, and a single binary decision -- is Coupler_System less than or
equal to 0.5? There are 360,847 examples where it's True and 43,863 examples where it's False.
And now interestingly in the False case (right side) you can see that there are no further
binary decisions, so this is called a leaf node -- it's a node where this is as far as
you can get. And so if your Coupler_System is not less than or equal to 0.5 (False) then
the prediction this model makes for the sale price is 9.21, versus 10.1 if it's True. So
you can see it's actually found a very big difference here and that's why it picked this
as the first binary split. And so the mean squared error for this section here is 0.12
which is far better than we started out at. The left branch, which is Coupler_System less
than 0.5, has 360,847 examples in it. Next it does another binary split, this time on
YearMade, asking whether this piece of equipment was made before 1991.5; if the answer isTrue,
then we get a leaf node and the predicted price is 9.97, with mean squared error 0.37.
If the value is False we don't have a leaf node and we have another binary split. And
you can see eventually we get down to here Coupler_System True, YearMade False, ProductSize
False, mean squared error 0.17. So all of these leaf nodes have MSEs that are smaller
than that original baseline model of just taking the mean. So this is how you can grow
a decision tree, and we only stopped here because we set max_leaf_nodes=4. And so if
we want to keep training it further we can just use a higher number. There's actually
a very nice library by Terence Parr called dtree_viz, which can show us exactly the same
information, and so here are the same leaf nodes, and you can see the chart of how many
are there. This is the split of Coupler_System at 0.5, here are the two groups, you can see
the sale price in each of the two groups, and then here's the leaf node. And then the
second split was on YearMade. And you can see here something weird�s going on with
YearMade -- there's a whole bunch of YearMade = 1000, which is obviously not a sensible
year for a bulldozer to be made, so presumably that's some kind of missing value. So looking
at a graph representation like this can give us some insights about what's going on in
our data, and so maybe we should replace those thousands with 1950, because that's you know
obviously a very very early year for a bulldozer, and so we can kind of pick it arbitrarily.
It's actually not really going to make any difference to the model that's created, because
all we care about is the order, because we're just doing these binary splits. Bit it�ll
make it easier to look at as you can see -- here's our YearMade=1950 cluster now, and so now
it's much easier to see what's going on in that binary split.
So, let's now get rid of max_leaf_nodes, and build a bigger decision tree. And then let's
create a couple of little functions -- one to create the root mean squared error, which
is just here -- and another one to take a model, some independent variables and a dependent
variable, predict from the model on the independent variables, and then take the root mean squared
error with the dependent variable. so that's going to be our model�s root mean squared
error. So for this decision tree, in which we didn't have a stopping criteria -- allowing
as many leaf nodes as you like -- the model�s root mean squared error is zero! So we've
just built the perfect model. So this is great news, right? We have built the perfect option
trading system. Well, remember that we actually need to check the performance on the validation
set. Let's check the MSE with a validation set, and oh it's worse than zero. So our training
set has MSE = 0, but our validation set is much worse than zero. Why has that happened?
Well one of the things that a random forest in sklearn can do is it can tell you the number
of leaf nodes. There are 341,000 leaf nodes with 400,00 data points, so in other words
we have nearly as many leaf nodes as data points! Most of our leaf nodes only have a
single thing in them, so they're taking an average of a single thing. Clearly this makes
no sense at all, so what we should actually do is pick some different stopping criteria.
Let's adopt the rule: don't split at a node if that would create a leaf node with less
than 25 things in it. And now if we fit, and we look at the root mean squared error for
the validation set, it's going to go down from 0.33 to 0.32. So the training sets got
worse from 0.2248 but the validation set got better, and now we only have 12,000 leaf nodes
so that is much more reasonable. All right so let's take a five-minute break,
and then we're going to come back and see how we get the best of both worlds. How are
we going to get something which has the kind of flexibility to get
really deep trees, but also without overfitting. And the trick will be to use something called
bagging. We�ll come back and talk about that in five minutes.
Okay welcome back, so we're going to look at how we can get the best of both worlds
as we discussed. Let's start by having a look at what we're doing with categorical variables,
first of all. So you might notice that previously, with categorical variables, for example in
collaborative filtering, we had to think about how many embedding levels we have. For example,
if you've used other modeling tools you might have experience with creating dummy variables.
With Random Forests, you don't have to. The reason is that as we've seen, all of our categorical
variables have been turned into numbers. And so we can perfectly well have decision tree
binary decisions which use those particular numbers. Now the numbers might not be ordered
in any interesting way, but if there's a particular level which kind of stands out as being important,
it only takes two binary splits to split out that level into a single piece. So generally
speaking, I don't normally worry too much kind of about encoding categorical variables
in a special way, except that as I mentioned, I do try to encode ordinal variables by seeing
what the order of the levels is, because often as you would expect sizes, for example, you
know medium and small are gonna mean kind of next to each other, and large and extra-large
would be next to each other. That's good to have those as similar numbers. Having said
that, you can one-hot encode a categorical variable if you want to, using get_dummies
in pandas, but there's not a lot of evidence that that actually helps. That has actually
been explored in a paper, and so I would say in general for categorical variables, don't
worry about it too much, just use what we've shown you.
Do you have a question? For ordinal categorical variables, how do you deal with, when they
have like n/a or missing values, where do you put that in the order? So in fast.ai,
na, missing values always appear as the first item; they�ll always be the 0 index item.
And also if you get something in the validation or test set which was a level we haven't seen
in training, that will be considered to be that missing or na value as well. All right.
So what we're gonna do to try and improve our random forest is we're going to use something
called bagging. And this was developed by a retired Berkeley professor named Leo Breiman
in 1994. And he did a lot of great work, and perhaps you could see that most of it happened
after he retired. His technical report was called bagging predictors and he described
how you could create multiple versions of a predictor, so multiple different models.
And you could then aggregate them by averaging over the predictions. And specifically, the
way he suggested doing this was to create what he called bootstrap replicas. In other
words, randomly select different subsets of your data, train a model on that subset and
store it away as one of your predictors. And then do it again a bunch of times. And so
each of these models is trained on a different random subset of your data. And then you (to
predict you) predict on all of those different versions of your model and average them. And
it turns out that bagging works really well. So this, the sequence of steps is basically:
randomly choose some subset of rows, train a model using that subset, save that model
and then return to Step 1. Do that a few times to train a few models. And then to make a
prediction, predict with all the models and take the average. That is bagging. And it's
very simple but it�s amazingly powerful. And the reason why is that each of these models
we've trained, although they are not using all of the data, so they're kind of less accurate
than a model that uses all of the data, each of them is � the errors are not correlated.
You know, the errors because they�re using that smaller subset are not correlated with
the errors of the other models. Because they're random subsets. And so when you take the average
of a bunch of kind of errors which are not correlated with each other, the average of
those errors is zero. So, therefore, the average of the models should give us an accurate prediction
of the thing we're actually trying to predict. So as I say here, it�s an amazing result
- we can improve the accuracy of nearly any kind of algorithm by training it multiple
times on different random subsets of data and then averaging the predictions. So then
Breimen in 2001 showed a way to do this specifically for decision trees where not only did he randomly
choose a subset of rows for each model. But then for each binary split, he also randomly
selected a subset of columns. And this is called the random forest. And it's perhaps
the most widely used most practically important machine learning method, and astonishingly
simple. To create a random forest regressor you use sklearn�s RandomForestRegressor.
If you pass n jobs minus one it will use all of the CPU cores that you have to run as fast
as possible. n_estimators says how many trees, how many
models to train. max samples says how many rows to use - randomly chosen rows to use
in each one. max_features is how many randomly chosen columns to use for each binary split
point. min_samples_leaf is the stopping criteria and we'll come back to that. So here's a little
function that will create a random first regressor and fit it to some set of independent variables
and a dependent variable. So we can give it a few default values and create a random forest
and train. And our validation set RMSE is 0.23. If we compare that to what we had before,
you, we had 0.32 - so dramatically better by using a random forest. So, so, what's happened
when we called RandomForestRegressor is it's just using that decision tree builder that
we've already seen but it's building multiple versions with these different random subsets.
And for each binary split it does it's also randomly selecting a subset of columns. And
then when we create a prediction it is averaging the predictions of each of the trees. And
as you can see it's giving a really great result. And one of the amazing things we'll
find is that it's gonna be hard for us to improve this very much. You know, the kind
of the default starting point tends to turn out to be pretty great. The, the sklearn docs
have lots of good information, and one of the things it has is this nice picture that
shows as you increase the number of estimators, how does the accuracy improve, error rate
improve for different max_features levels. And in general the more trees you add, the
more accurate your model. There, the, it's not going to overfit, right, because it's
averaging more of these, these week models, more of these models that are trained on subsets
of the data. So train as many, use as many estimators as you like. It�s really just
a case of how much time do you have, and whether you�ve kind of reached a point where it's
not really improving anymore. You can actually get at the underlying decision trees in a
model, in a random first model, using estimators_. So with a list comprehension we can call predict
on each individual tree. And so here's an array, a Numpy array containing the predictions
from each individual tree for each row in our data. So if we take the mean across the
zero axis, we�ll get exactly the same number. Because remember, that�s what a random forest
does -- it takes the mean of the trees� predictions. So one cool thing we could do
is we could look at the 40 estimators we have and grab the predictions for the first i of
those trees and take their mean. And then we can find the root mean squared error. And
so in other words, here is the accuracy when you've just got one tree, two trees, three
trees, four trees five trees, etc. And you can see � so it's kind of nice, right. You
can, you can actually create your own kind of build your own tools to look inside these
things and see what's going on. And so we can see here that as you add more and more
trees, the accuracy did indeed keep improving, or the root mean squared error kept improving.
Although it, the improvement slowed down after a while. The validation set is worse than
the training set and there's a couple of reasons that could have happened. The first reason
could be because we're still overfitting, which is not necessarily a problem. This is
something we could handle. Or maybe it's because the fact that we're trying to predict the
last two weeks is actually a problem, and that the last two weeks are kind of different
to the other options in our data set. Maybe something changed over time. So how do we
tell which of those two reasons there are, what, what is the reason
that our validation set is worse. We can actually find out using a very clever trick called
out-of-bag error, oob_error. And we use oob_error for lots of things. You can grab the oob_error,
or you can grab the oob predictions from the model with oob_prediction_, and you can grab
the RMSE. And you can find that the oob_ error RMSE is 0.21, which is quite a bit better
than 0.23. So let me explain what oob_error is. What oob_error is, is, we look at each
row of the training set, not the validation set, each row of the training set. And we
say, so for row number 1, which trees included row number 1 in the training. And we�ll
say, okay, let's not use those for calculating the error because it was part of those trees�
training. I would just calculate the error for that row using the trees where that row
was not included in training that tree. Because remember every tree is using only a subset
of the data. So we do that for every row - we find the prediction using only the trees that
were not used, that, that, that row was not used. And those are the oob predictions but
in other words this is like giving us a validation set result without actually needing a validation.
But the thing is, it's not with that time offset, it�s not looking at the last two
weeks, it's looking at the whole training set. But this basically tells us how much
of the error is due to overfitting versus due to being the last couple of weeks. So
that's a cool trick, oob_error is something that very quickly kind of gives us a sense
of how much we're, we're overfitting. And we don't even need a validation set to do
it. But there's our oob_error, so that's telling us a bit about what's going on in our model.
Then there's a lot of things we'd like to find out from our model. And I've got five
things in particular here which I generally find pretty interesting. Which is, how confident
are we about our predictions, there's some particular prediction we're making? Like we
can say this is what we think the prediction is, but how confident are we - is that exactly
that, or is it just about that, or we really have no idea? And then for particular predicting
a particular item, which factors were the most important in that prediction? And how
did they influence it? Overall which columns are making the biggest difference? Which ones
could be maybe throw away and it wouldn't matter? Which columns are basically redundant
with each other (ao we don't really need both of them)? And as we vary some column, how
does it change the prediction. So those are the five things that work that I'm interested
in figuring out. And we can do all of those things with a random forest. Let's start with
the first one. So the first one, we've already seen that we can grab all of the predictions,
for all of the trees, and take their mean to get the actual predictions of the model,
and then to get the RMSE. But what if instead of saying �mean�, we did exactly the same
thing, like so, but instead it said �standard deviation�. This is going to tell us for
every row in our dataset, how much did the trees vary? And so if our model really had
never seen kind of data like this before, it was something where, you know, different
trees were giving very different predictions, it might give us a sense that maybe this is
something that we're not at all confident about. And as you can see when we look at
the standard deviation of the trees for each prediction let's just look at the first five,
they vary a lot - 0.2, 0.1, 0.09, nearly 0.3, okay. So this is a really interesting, it's
not something that a lot of people talk about, but I think
it's a really interesting approach to kind of figuring out whether we might want to be
cautious about a particular prediction, because maybe we're not very confident about it. There's
one thing we can easily do with the random forest. The next thing, and this is I think
the most important thing for me in terms of interpretation, is feature importance. Here's
what feature importance looks like. We can call feature importance on a mode. Here's
some independent variables. Let's say grab the first ten. This says these are the 10
most important features in this random forest. These are the things that are the most strongly
driving sale price. Or we could plot them. And so you can see here, there's just a few
things that are by far the most important - what year the equipment was made (bulldozer
or whatever), how big is it, coupler system (whatever that means) and the product class
(whatever that means). And so you can get this by simply looking inside your trained
model and grabbing the feature importances attribute. And so here for making it better
to print out I'm just sticking that into a data frame and sorting descending by importance.
So how is this actually being done? It's, it's actually really neat what scikit-learn
does and (and Breiman the inventor of random forests described), is that you can go through
each tree and then start at the top of the tree and look at each branch and at each branch
see what feature was used the split. Which binary, which, the binary split was based
on which column. And then how much better was the model after that split compared to
beforehand. And we basically then say, �Okay that column was responsible for that amount
of improvement.� And so he can add that up across all of the splits, across all of
the trees, for each column and then you normalize it so they all add to 1. And that's what gives
you these numbers, which we show the first few of them in this table. And the first 30
of them here in this chart. So this is something that's fast, and it's easy and it kind of
gives us a good sense of like, �Well maybe the stuff that are less than 0.005, we could
remove. So if we did that that would leave us with only 21 columns. So let's try that,
let's just, just say, Okay, xs, which are important, the xs which are in this list of
ones to keep, do the same for valid. Retrain our random forest and have a look at the result.
And basically our accuracy is about the same, but we've gone down from 78 columns to 21
columns. So I think this is really important. It's not just about creating the most accurate
model you can, but you want to kind of be able to fit it in your head as best as possible.
And so 21 columns it's going to be much easier for us to check for any data issues and understand
what's going on. And the accuracy is about the same, or the RMSE. So I would say, Okay
let's do that, let's just stick with xs important for now on. And so here's this entire set
of the 21 features you can see it looks now like YearMade and ProductSize are the two
really important things. And then there's a cluster of kind of mainly product related
things that are kind of at the next level of importance. One of the tricky things here
is that we've got like a ProductClassDesc, modelID secondary desk, model desk, base model,
a model descriptor - they all look like there might be similar ways of saying the same thing.
So one thing that can help us to interpret the feature importance better and understand
better what's happening the model is to remove redundant features
So one way to do that is to call fast.ai use cluster_columns, which is basically a thin
wrapper for stuff that scikit-learn already provides. And what that's going to do is it's
going to find pairs of columns which are very similar. You can see here saleYear and saleElapsed,
see how this line is way out to the right whereas machineID and modelID is not at all,
it�s way out to the left. But that means that saleYear and saleElapsed are very very
similar - when one is lower the other tends to be low and vice versa. Here's a group of
three which all seem to be much the same. And then productGroupDesc and productGroup
and then fiBaseModel and fiModelDes, but these all seem like things where maybe we could
remove one of each of these pairs because they basically seem to be much the same. You
know, they're very, you know they're. when one is higher the other is high and vice versa.
So let's try removing one of each of these. Now it takes a little while to train a random
forest and so for the, just to see whether removing something makes it much worse, we
could just do a very fast version. So we could just train something where we only have 50,000
rows per tree, train for each tree and we'll just use 40 trees. And let's then just get
the oob_score. And so for that fast simple version our oob with our important ex�s
is 0.87 7. And here, for oob, a higher number is better. So then let's try going through
each of the things we thought we might not need and try dropping them, then getting the
oob_error for our exes with that one column removed. And so compared to 877, most of them
don't seem to hurt very much. saleEapsed hurt quite a bit, alright. So for each of those
groups, let's go and see which one of the ones seems like we could remove it. Here's
the five I found, remove the whole lot and see what happens. And so the oob went from
.877 to .874. So hardly any difference at all, despite the fact we managed to get rid
of five of our variables. So let's create something called ex_final, which is the ex
is important and then dropping those five. Save them for later, we can always load them
back again. And then let's check our random forest using those and again 0.233, 0.234.
So we've got about the same thing, but we've got even less columns now. So we're getting
a kind of a simpler and simpler model without hurting our accuracy. That is great. So the
next thing we said we were interested in learning about is di the columns that are, particularly
the columns that are most important, how does, what's the relationship between that column
and the dependent variable. Sorry, for example, what's the relationship between ProductSize
and sale price. Now the first thing I would do would be just to look at a histogram. So
one way to do that is with value_counts in Pandas. And we can see here our different
levels of ProductSize. And one thing to note here is actually missing is actually the most
common, and then next most is Compact and Small and then Mini is pretty tiny. So we
can do the same thing for YearMade. Now for YearMade, we can't just see the, the basic
bar chart, here we have a Pandas bar chart. For YearMade we actually need a histogram,
which Pandas has stuff like this built in so we can just call histogram. And that 1950,
you remember we created it - that's kind of this missing value thing which used to be
1000, but most of them seemed to have been well into the �90s and 2000�s. So let's
now look at something called a partial dependence plot. I'll show it to you
first. Here is a partial dependence plot of YearMade against PartialDependence. What does
this mean? Well, we should focus on the part where we actually have a reasonable amount
of data. So at least, well into the 80s, so around here, and so let's look at this bit
here. Basically what this says is that as year made increases, the predicted sale price,
log sale price, of course also increases, you can see. And the log sale price is increasing
linearly on other roughly so roughly then this is actually an exponential relationship
between YearMade and sale price. Why do we call it a partial dependence? So we're just
plotting a kind of the year against the average sale price. Well no, we're not. We can't do
that because a lot of other things change from year to year. Example, maybe more recently
people tend to buy bigger bulldozers or more bulldozers with air conditioning, or more
expensive models of bulldozers. And we really want to be able to say, like no, just what's
the impact of a year and nothing else? And if you think about it from a kind of an inflation
point of view, you would expect that older bulldozers would be kind of like, that bulldozers
would get a kind of a constant ratio, cheaper the further you go back. Which is what we
see. So what we really want to say is, all other things being equal, what happens if
only the year changes? And there's a really cool way we can answer that question with
a random forest. So how does YearMade impact sale price, all other things being equal?
What we can do is we can go into an actual data set and replace every single value in
the YearMade column with 1950, and then can calculate the predicted sale price for every
single option, and then take the average over all the options. And that's what gives us
this value here. And then we can do the same for 1951, 1952, and so forth, until eventually
we get to our final year of 2011. So this isolates the effect of only YearMade. So it's
a kind of a bit of a curious thing to do but it's actually it's a pretty neat trick, like
trying to kind of pull apart and create this partial dependence and say what might be the
impact of just changing YearMade. And we can do the same thing for ProductSize. And one
of the interesting things if we do it for ProductSize is we see that the lowest value
of predicted sale price, log sale price, is #na#, which is a bit of a worry because we
kind of want to know � well that means it's really important. The question of whether
or not the product size is labeled is really important. And that is something that I would
want to dig into before I actually use this model to find out, Well why is it that sometimes
things aren't labeled and what does it mean? You know why is it that, that's actually,
that's just important predictor. So that is the partial dependence plot and it's so really
clever trick. So we have looked at four of the five questions we said we wanted to answer
at the start of this section, so the last one that we want to answer is one here ... we're
predicting with a particular row of data what were the most important factors and how did
they influence that prediction. This is quite related to the very first thing we saw. So
it's like, imagine you were using this auction price model in real life you had something
on your tablet and you went into some auction and you looked up what the predicted auction
price would be for this lot that's coming up to find out whether it seems like it's
being under or overvalued and then you decide what to do about that. So one thing we said
we'd be interested to know is like, well are we actually confident in our prediction and
then we might be curious to find out like, �Oh, I'm really surprised it was predicting
such a high value. Why was it predicting such a
high value?� So to find the answer to that question we can use a module called treeInterpreter.
And treeInterpreter, the way it works is that you pass in a single row, so it's like here's
the auction that's coming up, here's the model, here's the auctioneer ID, etc., etc., please
predict the value - from the random forest, what's the expected sale price. And then what
we can do is we can take that one row of data and put it through the first decision tree
and we can see what's the first split that's selected. And then based on that split, does
it end up increasing or decreasing the predicted price compared to that kind of raw baseline
model of just take the average. And then you can do that again at the next split and again
at the next split and the next split. So for each split, we see what the increase or decrease
in the prediction, that's not right. We see what the increase or decrease in the prediction
is (while I'm here) compared to the parent node. And so then do that for every tree,
and then add up the total change and importance by split variable. And that allows you to
draw something like this. So here's something that's looking at one particular row of data.
And overall we start at 0, and so 0 is the initial 10.1. Do you remember this number?
10.1 is the average log sale price of the whole data set, they call it the bias. And
so if we call that 0, then for this particular row we're looking at, YearMade has a -4.2
impact on the prediction. And then ProductSize has a +0.2. Coupler_System has a positive
0.046. modelID has a positive 0.127, and so forth, right. And so the red ones are negative
and the green ones are positive. And you can see how they all join up, until eventually,
overall the prediction is that it's going to be -.122 compared to 10.1, which is equal
to 9.98. So this kind of plot is called a waterfall plot and so basically when we say
treeInterpreter.predict, it gives us back the prediction, which is the actual number
we get back from the random forest, the bias, which is just always this 10.1 for this data
set, and then the contributions, which is all of these different values. It�s how
much, how important was each factor and here I've used a threshold which means anything
that was less than 0.08 all gets thrown into this other category. I think this is a really
useful kind of thing to have in production because it can help you answer questions,
whether it will be to the customer or for, you know, whoever's using your model, if they're
surprised about something � why is that prediction? So I'm going to show you something
really interesting using some synthetic data and I want you to really have a think about
why this is happening before I tell you. Pause the video if you're watching the video when
I get to that point. Let's start by creating some synthetic data like so. So we're going
to grab 40 values evenly spaced between 0 and 20. And then we're just going to create
the y equals x line and add some normally distributed random jitter on that. Here's
the scatter plot. So here's some data we want to try and predict. If we're going to use
a random forest, you know, kind of a bit of an overkill here. Now in this case we only
have one independent variable. Scikit-learn expects us to have more than one, so we can
use unsqueeze in PyTorch to add, to go from a shape of 40, in other words a
vector with 40 elements to a shape of [40,1]. In other words a matrix of 40 rows with one
column. But this unsqueeze(1) means add a unit axis here. I don't use unsqueeze very
often because I actually generally prefer the index with a special value None. This
works in PyTorch and Numpy. And this, the way it works is to say, okay, x_lin (remember
that size is a vector of length 40) every row. And then None means insert a unit axis
here. So these are two ways of doing the same thing, but this one is a little bit more flexible,
so that's what I use more often. But now that we've got the shape that is expected which
is a back to tensor and an array with two dimensions or axes we can create a random
forest, we can fit it. And let's just use the first 30 data points, and it's so kind
of top here. And then let's do a prediction, all right. So let's plot the original data
points and then also plot a prediction. And look what happens on the prediction. It acts,
it's kind of nice and accurate, and then suddenly what happens? Yes, this is the bit where if
you watch in the video I want you to pause and think, �Why is this flat?� So what's
going on here? Well remember a random forest is just taking the average of predictions
of a bunch of trees. And a tree, the prediction of a tree is just the average of the values
in a leaf node. And remember we fitted using a training set containing only the first 30.
So none of these appeared in the training set, so the highest we could get would be
the average of values that are inside the training set. In other words, there�s this
maximum we can get to. So random forests cannot extrapolate outside of the bounds of the data
that they have as a training set. This is going to be a huge problem for things like
time series prediction where there's like an underlying trend, for instance. But really
it's a more general issue than just time variables. It's going to be hard for, random forests,
or impossible, often, for random forests to just extrapolate outside the types of data
that it's seen, in a general sense. So we need to make sure that our validation set
does not contain out of domain data. So how do we find out of domain data? So we might
not even know if our test set is distributed in the same way as our training data. So,
if they're from two different time periods, how do you kind of tell how they vary, right?
Or if it's a Kaggle competition, how do you tell if the test set and the training set
which Kaggle gives you have some underlying differences? There�s actually a cool trick
you can do -- which is you can create a column called is_valid contains 0 for everything
in the training set and 1 for everything in the validations set. It's concatenating all
of the independent variables together. So it's so so it's concatenating the independent
variables for both the training and validation set together. So this is our independent variable
and this becomes our dependent variable. And we're going to create a random forest not
for predicting price, but a random forest that predicts is this row from the validation
set or the training set. If the validation set and the training set are from kind of
the same distribution, if they're not different, then this random forest should basically have
zero predictive power. If it has any predictive power, then it means that our training and
validation sets are different and to find out the source of that difference we can use
feature importance. And so you can see here that the difference between the validation
set and the training set is not surprisingly saleElapsed.
So that's the number of days since I think like 1970 or something so it's basically the
date. Oh, yes, of course, you can predict whether something is in the validation set
or the training set by looking at the date, because that's actually how to find them.
That makes sense. This is interesting, SalesID. So it looks like the sales ID is not some
random identifier, but it increases over time. And ditto for MachineID. And then there's
some other smaller ones here. That kind of makes sense. So I guess for something like
ModelDesk. I guess there are certain models that were only made in later years for instance
can see these top three columns are a bit of an issue. So then we could say, �Like,
okay, what happens if we look at each one of those columns, those first three and remove
them, and then see how it changes our RMSE on our sales price model on the validation
set. We start from 0.232, and removing salesID actually makes it a bit better; saleElapsed
makes it a bit worse; machineID, about the same. But we can probably remove salesID and
machine ID without losing any accuracy. And yep, it's actually a slight improvement. But
most importantly it's going to be more resilient over time, right. Because we're trying to
remove the time-related features. Another thing to note is that since it seems that,
you know, this kind of saleElapsed issue that maybe it's making a big difference is maybe
looking at the saleYear distribution (this is the histogram), most of the sales are in
the last few years anyway. Now what happens if we only include the most recent few years.
So let's just include everything out to 2004. So that is xs_filt. And if I trained on that
subset, then my accuracy goes, improves a bit more, from .231 to .230. Oh, so that's
interesting, right. We're actually using less data, less rows, and getting a slightly better
result because the more recent data is more representative. So that's about as far as
we can get with our random forest. But what I will say is this � this issue of extrapolation
would not happen with a neural net, would it? Because a neural net is using the kind
of the underlying layers, linear layers, and so linear layers can absolutely extrapolate.
So the obvious thing to think then at this point, is well maybe would a neural net there
a better job of this? That's gonna be the thing next, after this question. A question
first. How do, how does feature importance relate to correlation? Feature importance
doesn't particularly relate to a correlation. Correlation is a concept for linear models,
and this is not a linear model. So remember, feature importance is calculated by looking
at the improvement in accuracy as you go down each tree and you go down each binary spit.
If you're used to linear regression, then I guess correlation sometimes can be used
as a measure of feature importance, but this is a much more kind of direct version. It�'s
taking account of these nonlinearities and interactions, as well, so it's a much more
flexible and reliable measure than feature importance. Any more questions? So do the
same thing with a neural network. I'm going to just copy and paste the same lines of code
that I had from before, but this time I call it nn_df_nn. And these are the same lines
of code, and I'll grab the same list of columns we had before in the dependent variable to
get the same dataframe. Now as we've discussed, for categorical columns we probably want to
use embeddings. So to create embeddings we need
to know which columns should be treated as categorical variables. And as we discussed
we can use cont_cat_split for that. One of the useful things we can pass that is the
maximum cardinality. so max_card=9000 means if there's a column with more than 9000 levels,
you should treat it as continuous; and if it's got less than 9000 levels, treat it as
categorical. So that's you know it's a simple little function that just checks the cardinality
and splits them based on how many discrete levels they have. And of course, if their
data type, if it's not actually a numeric data type, it has to be categorical. So there's
our, there's our split. And then from there what we can do is we can say, �Oh, we've
got to be a bit careful of saleElapsed, because actually saleElapsed I think has less than
9000 categories, but we definitely don't want to use that as a categorical variable. The
whole point was to make it that this is something that we can extrapolate. Certainly anything
that's kind of time dependent, or you think that we might see things outside the range
of inputs in the training data, we should make them continuous variables. So let's make
saleElapsed, put it in continuous neural net and remove it from categorical. So here's
the number of unique levels, this is from Pandas for everything in our neural net data
set for the categorical variables. And I get a bit nervous when I see these really high
numbers, so I don't want to have too many things with like lots and lots of categories.
The reason I don't want lots of things with lots and lots of categories is just they�ve
got to take up a lot of parameters. Because you know in an embedding matrix, this is,
you know, every one of these is a row in an embedding matrix. In this case, I notice ModelID
and modelDesc might be describing something very similar, so I�d quite like to find
out if I could get rid of one. And an easy way to do that would be to use a random forest.
So let's try removing the ModelDesc and let's create a random forest, see what happens.
And, oh it's actually a tiny bit better and certainly not worse. So that suggests that
we can actually get rid of one of these levels, or one of these variables. So let's get rid
of that one. So now we can create a tabular Panda's object just like before, but this
time we're going to add one more processor which is normalized. And the reason we need
normalize � so normalize is subtract the mean, divide by the standard deviation. We
didn't need that for a random forest because for a random forest we're just looking at
less than or greater than through our binary splits. So all that matters is the order of
things how they're sorted; doesn't matter whether they're super big or super small.
But it definitely matters for neural nets because we have these linear layers. So we
don't want to have, you know, things with kind of crazy distributions with some super
big numbers and super small numbers, because it's not going to work. So it's always a good
idea to normalize things. Neural nets, where we can do that in a tabular neural net by
using the Normalize tabular proc. So we can do the same thing that we did before with
creating our tabularPandas -- tabular object, the neural net. And then we can create data
loaders from that with a batch size, and this is a large batch size because tabular models
don't generally require nearly as much GPU RAM as a convolutional neural net or something
or an RNN or something. Since it�s a regression model, we're going to want our range, so let's
find the minimum and maximum of our dependent variable.
Then we can now go ahead and create a tabular_learner. But a tabular_learner is going to take our
data loaders, our y_range, how many activations do you want in each of the linear layers.
And so you can have as many linear layers as you like. How many outputs are there? This
is a regression with a single output. And what loss function do you want? We can use
lr_find and then we can go ahead and use fit_one_cycle (there's no pre trained model obviously, because
this is not something where people have got pre train models for industrial equipment
options). We just use fit_one_cycle and train for a minute. And then we can check and our
our MSE is 0.226, which, here it was 0.230. But that's amazing, we actually have you know
straight away a better result and the random forest. It's a little more fussy, it took
something takes a little bit longer. And as you can see, you know, for interesting data
sets like this we can get some great results with neural nets. So here's something else
we could do -- the random forest and the neural net, they each have their own pros and cons
-- some things they�re good at and some things they're less good. So maybe we can
get the best of both worlds and a really easy way to do that is to use ensemble. We've already
seen that a random forest is a decision tree ensemble, but now we can put that into another
ensemble. You can have an ensemble of the random forest and the neural net. There's
lots of super fancy ways you can do that, but a really simple way is to take the average.
So sum up the predictions from the two models, divide by two, and use that as a prediction.
So that's our ensemble prediction -- it�s just literally the average of the random forest
prediction and the neural net prediction. And that gives us .223 (versus .226). So how
good is that? Well, it's a little hard to say because unfortunately this competition
is old enough that we can't even submit to it and find out how we would have gone on
Kaggle, so we don't really know. And so we're relying on our own validations that, but it's
quite a bit better than even the first place score on the test set. So if the validation
set is, you know, doing a good job then this is a good sign that this is a really really
good model. Which wouldn't necessarily be that surprising, because you know in the last
few years I guess we've learned a lot about building these kinds of models and we're kind
of taking advantage of a lot of the tricks that have, that have appeared in recent years.
And, maybe this goes to show that, well I think it certainly goes to show that both
random forests and neural nets have a lot to offer and try both and maybe even find
both. We've talked about an approach to ensembling called bagging, which is where we train lots
of models on different subsets of the data and take the average. Another approach to
ensembling, particularly ensembling of trees, is called boosting. And boosting involves
training a small model which underfits your data set. Maybe like just have a very small
number of if notes then you calculate the predictions using the small model and then
you subtract the predictions from the targets. So these are kind of like the errors of your
small underfit model, we call them residuals. And then go back to step one but now instead
of using the original targets use the residuals to train a small model which underfits your
dataset attempting to predict residuals. And do that again, and again, and again .. until
you reach some stopping criterion such as the maximum number of trees. Now, that will
leave you with a bunch of models which you don't average but which you sum, because each
one is creating a model that's based on the residual of the previous one. So we've subtracted
the predictions of each new tree from the residuals of the previous tree. But the residuals
get smaller and smaller and then to make predictions, we just have to do the opposite, which is
to add them all together. So there's lots of variants of this, but you'll see things
like GBMs or gradient boosted machines or GBDTs, or gradient boosted decision trees.
And there's lots, minor details surround, you know, and they're insignificant details,
but the basic idea is, is what I've shown you. Two questions. All right, let's take
the questions. Are dropping features in a model as a way to reduce the complexity of
the model and thus reduce overfitting - is this better than adding some regularization
like weight decay? I didn't claim that we removed columns to avoid overfitting. We removed
the columns to simplify things, analyze. And then it should also mean we don't need as
many trees, but there's no particular reason to believe that this will regularize. And
well the idea of regularization doesn't necessarily make a lot of sense to random forests. And
always add more trees. Is there a good heuristic for picking the number of linear layers in
the tabular model. Not really. Well, if there is I don't know what it is. I guess two, three
hidden layers works pretty well. So you know what I showed, those numbers I showed her
pretty good for a large-ish model. At default, it uses 200 and 100, so maybe start with the
default and then go up 500 and 250. If that gives an improvement, and like, just keep
doubling them while it keeps improving, or you run out of memory. Another thing to note
about boosted models is that there's nothing to stop us from overfitting. You add more
and more trees. The bagging model, sort of a random forest, it's going to get, going
to generalize better and better as your each time you're using your model which is based
on a subset of the data. But boosting each model will fit the training set better and
better. Eventually overfit, so boosting methods do require hyperparameter tuning and fiddling
around with, you know. You certainly have regularization boosting. They're pretty sensitive
to their hyperparameters, which is why they're not normally much first go-to. They they often,
they more often win Kaggle competition than random forests do. Like they tend to be good
at getting that last little bit of performance. But the last thing I'm going to mention is
something super-neat which a lot of people don't seem to know exists (it�s a shame,
because it�s super-cool) which is something from the entity embeddings paper, the table
from it, where what they did was they built a neural network, they got the entity embeddings,
ee and then they try to random forest using the entity embedding as predictors, rather
than the approach I described with just the raw categorical variables. And the, the error
for a random forest went from 0.16 to 0.11, a huge improvement. And a very simple method,
KNN, went from .29 to 0.11. Basically all of the methods when they used entity embeddings
suddenly improved a lot. The one thing you you should try if you have a look at the further
research section after the questionnaire is it asks to try to do this, actually take
those entity embeddings that we trained in the neural net, and use them in the random
forest. And then maybe try ensembling again and see if you can beat the .223 that we had.
This is a really nice idea, it's like, you get you know, all the benefits of boosted
decision trees, but all of the nice features of entity embeddings. And so this is something,
that not enough people tend to be playing with for some reason. So, overall, you know,
random forests are nice and easy to train, you know, they're very resilient, they don't
require much pre-processing, they're trained quickly, they don't overfit, you know, they
can be a little less accurate, and they can be a bit slow at inference time, because for
inference you have to go through every one of those trees. Having said that, a binary
tree and be pretty heavily optimized. Though you know it is something you can basically
create a totally compiled version of a tree and they can certainly also be done entirely
in parallel, so that's something to consider. Gradient boosting machines are also fast to
train on the whole but a little more fussy about hyper parameters, you have to be careful
about overfitting but get more accurate. Neural nets may be the fussiest to deal with, they've
kind of got the least rules of thumb around, or tutorials around saying this is how to
do it. It's just a bit a bit newer, a little bit less well understood, but they can give
better results in many situations than the other two approaches or at least with an ensemble
can improve the other two approaches. So I would always start with a random forest and
then see if you can beat it using these. So yeah why don't you now see if you can find
a Kaggle competition with tabular data, whether it's running now pr it's a past one and see
if you can repeat this process for that and see if you can get in the top 10 percent of
the private leaderboard. That would be a really great, stretch goal at this point. Implement
the decision tree algorithm yourself, I think that's an important one, to see if you really
understand it and then from there create your own random forests from scratch. You might
be surprised it's not that hard. And then go and have a look at the tabular model source
code, and at this point this is pretty exciting, you should find you pretty much know what
all the lines do with two exceptions. And if you don't, you know, dig around and explore
an experiment and see if you can figure it out. And with that, we are, I am very excited
to say at a point where we've really dug all the way in to the end of these real, valuable,
effective, fastai applications and we're understanding what's going on inside them. What should we
expect for next week. Oh for next week we will look at NLP and computer vision. And
we�ll do the same kind of ideas, delve deep to see what's going on. Thanks everybody,
see you next week!
Hi everybody and welcome to lesson eight, the last lesson of part one of this course.
Thanks so much for sticking with us. Got a very interesting lesson today where we're
going to dive into natural language processing, and remind you we did see natural language
processing in Lesson One. This was it here: we looked at a dataset where we could pass
in many movie reviews like so and get back probabilities that it's a positive or negative
sentiment, and we trained it with a very standard-looking classifier trainer approach. But we haven't
really talked about what's going on behind the scenes there, so let's let's do that and
we�ll also learn about how to make it better. So we were getting about 93%, so 93% accuracy
for sentiment analysis which is actually extremely good, and it only took a bit over ten minutes,
but let's see if we can do better. So we're going to go to notebook number 10 and in notebook
number ten we are going to start by talking about what we are going to do to train an
NLP classifier. So a sentiment analysis which is: is this movie review positive or negative
sentiment, is just a classifier; the dependent variable is binary, and the independent variable
is the kind of the interesting bit. So we're going to talk about that, but before we do,
we're going to talk about what was the pre-trained model that got used here? Because the reason
we got such a good result so quickly, is because we are doing fine tuning of a pre-trained
model. So what is this pre-trained model, exactly? Now the pre-trained model is actually
a pre-trained language model. So what is a language model? A language model is a special
kind of model, and it's a model where we try to predict the next word of a sentence. So
for example if our language model received �even if our language model knows the�
then its job would be to predict �basics.� Now, the language model that we use as our
pre-trained model was actually trained on Wikipedia. So we took all the, you know, non-trivial
sized articles in Wikipedia, and we built a language model which attempted to predict
the next word of every sequence of words, in every one of those articles, and it was
a neural network of course. And we then take those pre-trained weights, and those are the
pre-trained weights that when we said text_classifier_learner were automatically loaded in. So, conceptually,
why would it be useful to pre-train a language model? How does that help us to do sentiment
analysis, for example? Well, just like an ImageNet model has a lot of information about
what pictures look like and what they're consisting of, a language model tells us a bit, a lot
about what sentences look like and what they know about the world. So a language model,
for example, if it's going to be able to predict the end of the sentence, �In 1998 this law
was passed by president...what?� So a language model to predict that correctly would have
to know a whole lot of stuff. It would have to know about, well, how English language
works in general and what kind of sentences go in what places; that after the word President
would usually be the surname of somebody; it would need to know what country that law
was passed in and would need to know what President was president of that country in
nineteen-- what did I say? 1998. So it�d have to know a lot about the world it'd have
to know a lot about language. To create a really good language model is really hard,
and in fact this is something that people spend many many many millions of dollars on
creating language models of huge datasets Our particular one doesn't take particularly
long to pre-trained but there's no particular reason for you to pre-train one of these language
models because you can download them through fastai or through other places. So what happened
in Lesson One is we downloaded this pre-trained wikipedia model and then we fine-tuned it.
So as per usual we threw away the last layer which was specific for
predicting the next word of Wikipedia and fine-tuned the model, initially just the last
layer to learn to predict sentiment of movie reviews, and then as per usual, then fine-tune
the rest of the model and that got us 93%. Now, there's a trick we can use, though, which
is we start with this Wikipedia language model, and the particular subset we use is called
a WikiText-103, and rather than just jumping straight to a classifier, which we did in
Lesson One, we can do even better if we first of all create an IMDB language model. That
is to say, a language model that learns to predict the next word of a movie review. The
reason we do that is that this will help it to learn about IMDB-specific kinds of words,
like, like it'll learn a lot more about the names of actors and directors; it'll learn
about the kinds of words that people use in movie reviews. And so if we do that first
then we would hope we will end up with a better classifier. So that's what we're going to
do in the first part of today's lesson, and we're going to kind of do it from scratch.
And we're going to show you how to do a lot of the things from scratch even though later
we'll show you how fastai does at all for you. So how do we build a language model?
So as we point out here, sentences can be different lengths, and documents like movie
reviews can be very long. So how do we go about this? Well, a word is basically a categorical
variable and we already know how to use categorical variables as an independent variable in a
neuron net, which was we make a list of all of the possible levels of a categorical variable,
which we call the vocab, and then we replace each of those categories with its index so
they all become numbers. We create an initially random embedding matrix for each, so each
row then is for one element from the vocab, and then we make that the first layer of a
neural net. So that's what we've done a few times now, and we've even created our own
embedding layer from scratch, remember? So we can do the same thing with text, right?
We can make a list of all the possible words in -- in the whole corpus, the whole dataset,
and we can replace each word with the index of the vocab and create an embedding matrix.
So in order to create a list of all levels -- in this case a list of all possible words
-- let's first of all concatenate all the documents, all the movie reviews, together
into one big long string, and split it into words. Okay and then our independent variable
will basically be that sequence, starting with the first word in the long list and ending
with the second last, and our dependent variable will be the sequence of words starting with
the second word and ending with the last. So they're kind of offset by one. So as you
move through the first sequence you're then trying to predict the next word in the next,
in the second part. That's kind of what we're doing, right? We'll see more detail in a moment.
Now when we create our vocab by finding all the unique words in this concatenated corpus,
a lot of the words we see will be already in the embedding matrix, already in the vocab
of the pre-trained Wikipedia model. But there's also going to be some new ones, right? There
might be some particular actors that don't appear in Wikipedia, or maybe some informal
slang words and so forth. So when we build our vocab and then our embedding matrix for
the IMDB language model, any words that are in the vocab of the pre-trained model we'll
just use them as is. But for new words, we�ll create a new random vector. So here's the
process we're going to have to go through. First we're going to have to take our big
concatenated corpus and turn it into a list of tokens: could be words, could be characters,
could be substrings. That's called tokenization. And then we'll do numericalization, which
is basically these two steps, which is replacing each word with its index in a vocab, which
means we have to create that vocab. So create the vocab and then convert. Then
we're going to need to create a data loader that has lots of substrings, lots of sequences
of tokens from our IMDB corpus as an independent variable, and the same thing offset by one
as a dependent variable, and then we're going to have to create a language model. Now, a
language model is going to be able to handle input lists that can be arbitrarily big or
small, and we're going to be using something called a recurrent neural network to do this,
which we'll learn about later. So basically, so far we've always assumed that everything
is a fixed size, a fixed input, so we're going to have to mix things up a little bit here
and deal with architectures that can be different sizes. For this notebook, notebook 10, we're
going to kind of treat it as a black box, it's just going to be just a neural net, and
then later in the lesson we'll look at delving inside what's happening in that architecture.
Okay so let's start with the first of these, which is tokenization. So converting a text
into a list of words or a list of tokens. What does that mean? Is a full-stop a token?
What about �don't� -- is that single word or is it two words, do-n't? Or is it -- would
I convert it to a �do not?� What about long medical words that are kind of made up
of lots of pieces of medical jargon that are all stuck together? What about hyphenated
words? And really interestingly then, what about something like Polish where you -- or
Turkish -- where you can create really long words? All the time they create really long
words that are actually, lots of separate parts are concatenated together. Or languages
like Japanese and Chinese that don't use spaces at all? They don't really have a well defined
idea of �word?� Well, there's no right answer, but there's basically three approaches.
We can use a word-based approach, which is what we use by default at the moment for English
(although that might change) which is: we split a sentence on spaces, and then there
are some language-specific rules, for example turning don't into do-n�t and putting punctuation
marks as a separate token most of the time. Really interestingly, there are tokenizers
that are subword-based, and this is where we split words into smaller parts based on
the most commonly occurring substrings. We'll see that in a moment. Or the simplest, character-based:
split a sentence into its characters. We're going to look at word- and sub-word tokenization
in this notebook. And then if you look at the questionnaire at the end, you'll be asked
to create your own character-based tokenizer, so please make sure you do that if you can,
it'll be a great exercise! So fastai doesn't invent its own tokenizers. We just provide
a consistent interface to a range of external tokenizers, because there's a lot of great
tokenizers out there. So you can switch between different tokenizers pretty easily. So let's
start. Let's grab our IMDb dataset like we did in lesson 1 and in order to try out a
tokenizer, let's grab all the text files. So we can, instead of calling get_image_files,
we'll call get_text_files, and it'll -- to have a look at what that's doing, that get,
we can even look at the source code and you can see actually it's calling a more general
thing called get_files and saying what extensions at once right so if anything in fast AI doesn't
work quite the way you want and there isn't an option which works the way you want, you
can often look, always look underneath to see what we're calling, and you can call the
lower-level stuff yourself. So files is now a list of files. So we can grab the first
one, we can open it, we can read it, have a look at the start of this review, and here
it is. Okay so at the moment at the default English word tokenizer we use is called spaCy,
which uses a pretty sophisticated set of rules -- the
special rules for particular words and URLs, and so forth -- but we're just going to go
ahead and say WordTokenizer which will automatically use fastai�s default word tokenizer, currently
spaCy. And so if we pass a list of documents (we'll just make it a list of one document
here) to the tokenizer we just created and just grab the first, since we just created
a list, that's going to show us, as you can see, the tokenized version. So you can see
here that �This movie which I just discovered at the video store has� etc. it changed
�it�s� into �it� and ��s� and it's put a comma as a separate punctuation
mark, and so forth. Okay so you can see how it has tokenized this review. Let's look at
a more interesting one: �The US dollar� blah blah blah, and you can see here it actually
knows that US is special so it doesn't put the full-stop in as a separate place for U.S.;
it knows about 1.00 is special, so you can see there's a lot of tricky stuff going on
with spaCy to try and be as kind of thoughtful about this as possible. Fastai then provides
this tokenizer wrapper which provides some additional functionality to any tokenizer,
as you can see here which is, for example, the word �it� here which previously was
capital �It� has been turned into a lowercase �it� and then a special token xxmaj has
appeared at the front. Everything starting with xx is a special fastai token, and this
means that the next match means that the next word was previously started with a capital
letter. So here's another one: �this� used to be capital T, so we make it lowercase
and then add xxmaj; xxbos means this is the start of a document. So there's a few special
rules going on there. So why do we do that? Well if you think about it, if we didn't lowercase
�it� for instance or �this� then the capitalized version and the lowercase version
are going to be two different words in the embedding matrix. Which probably doesn't make
sense, you know, regardless of the capitalization they probably basically mean the same thing.
Having said that, sometimes the capitalization might matter, so we kind of want to say all
right, use the same embedding every time you see the word �this,� but add some kind
of marker that says that this was originally capitalized. Okay so that's why we do it like
this. So there's quite a few rules, you can see them in text_proc_rules, and you can see
the source code. Here's a summary of what they do but let's look at a few examples.
So if we use that tokenizer we created and pass in for example this text, you can see
the way it's tokenized, we get the xx beginning of stream, or beginning of string, beginning
of document. This HTML entity has become a real unicode. We've got the xxmaj we discussed.
Now here www has been replaced by xx rep 3 w. That means the letter w is repeated three
times. So for things where you've got like, you know, a hundred exclamation marks in a
row or the word �so� with like fifty o�s, this is a much better representation. And
then you can see all uppercase has been replaced with xxup followed by the word. So there's
some of those rules in action. Oh you can also see multiple spaces have been replaced
with just making it standard tokens. So that's the word tokenizer. The really interesting
one is the subword tokenizer. So how, why would you need a subword tokenizer? Well,
consider for example this sentence here [reads Chinese]. So this is �My name is Jeremy,�
but the interesting thing about it is there's no spaces here, right, and that's because
there are no spaces in Chinese and there isn't really a great sense of what a word is in
Chinese. I mean this particular sentence it's fairly clear what the words are but it's not
always obvious. Sometimes the words are actually split, you know, so some of it�s at the
start of a sentence, and some of its at the end. So you can't really do word tokenization
to something like Chinese. So instead we use sub word tokenization, which is where we look
at a corpus of documents, and we find the most commonly occurring groups of letters,
and those commonly occurring groups of letters become the vocab. So, for example, we would
probably find �Wo de� would appear often, because that means �my�, �m�ngz�,
whoopsie-daisy, and then �m�ngz�, for example is �name�, and this is my westernized
version of a Chinese name, which wouldn't be very common to call, common to call at
all, so they would probably appear separately. So let's look at an example. Let's grab the
first 2000 movie reviews, and let's create the default SubwordTokenizer(), which currently
uses something called �SentencePiece�, that might change. And now we're going to
use something special, something very important which is called �.setup�. Transforms in
fastai, you can always call this special thing called �.setup()�, it often doesn't do
anything, but it's always there, but some transforms like a SubwordTokenizer() actually
need to be set up before you can use them. In other words you can't tokenize into sub
words until you know what the most commonly occurring groups of letters are. So passing
a list of texts, and here this list of texts, to �.setup()� will, will train the sub
word tokenizer. It'll find those commonly occurring groups of letters. So having done
that we can then, this is just for experimenting, we're going to pass in some size, we'll say
what vocab size we want for our SubwordTokenizer(), we'll set it up with our texts, and then we
will have a look at a particular sentence. So for example if we create a SubwordTokenizer()r
with a thousand tokens, and it returns this, this tokenized string. Now this kind of long
underscore thing is what we replace space with because now we're using sub word tokens
we kind of want to know where the sentences actually start and stop. Okay, and you can
see here a lot of sentence words are common enough sequences of letters that they get
their own vocab item, or else discovered, �did� was not, wasn't common enough. So
that became �dis�, �c�, �over�, �ed�. Right? �Video� appears enough,
whereas �store� didn't. So that became �st�, �or�, �e�, so you get the
idea. Right? So if we wanted a smaller vocab size, that would, as you see, even �this�
doesn't become its own word, �movie� is so common that it's a, is its own word, so
it �just� becomes �j�, �us�, �t�, for example. We have a question. Okay. �How
can we determine if the given pre-trained model, in this case wikitext 103, is suitable
enough for our downstream task. If we have limited vocab overlap should we need to add
an additional dataset to create a language model from scratch?� If it's in the same
language, so if you're doing English, it's always, it's almost always, sufficient to
use Wikipedia. We've played around this a lot and it was one of the key things that
Sebastian Ruder and I found when we created the ULMFiT paper. Was, before that time people
really thought you needed corpus specific pre-trained models, but we discovered you
don't, just like you don't that often need corpus specific pre-trained vision models.
ImageNet works surprisingly well across a lot of different domains. So Wikipedia has
a lot of words in it. It would be really really... I haven't come across an English corpus that
didn't have a very high level of overlap with Wikipedia. On the other hand if you're doing
ULMFiT with like genomic sequences or Greek or whatever then
obviously you're going to need a different pre-trained model so once we got to a 10,000
word vocab, as you can see, basically every word, at least common word, becomes its own
vocab item in the subword vocab, except say �discovered� which becomes �discover�,
�ed�. So my guess is that sub word approaches are going to become, kind of, the most common,
maybe they will be by the time you watch this. We've got some fiddling to do to get this
working super well for fine tuning, but I think I know what we have to do so hopefully
we'll get it done pretty soon. All right. So after we split it into tokens the next
thing to do is numericalization. So let's go back to our word tokenized text, which
looked like this, and in order to numericalize we will first need to call �.setup()�.
So to save a bit of time let's create a subset of our text. So just create a couple of hundred
of the corpuses, that's so a couple hundred of the reviews. So here's an example of one,
and we'll create our Numericalize() object and we will call �setup� and that's the
thing that's going to create the vocab for us, and so after that we can now take a look
at the vocab. This is �coll_repr()� is showing us a representation of a collection.
It's what the L class uses underneath, and you can see, when we do this, that the vocab
starts with the special tokens and then we start getting the English tokens in order
of frequency. So the default is a vocab size of 60,000. So that'll be the size of your
embedding matrix by default and if there are more than 60,000 unique words in your vocabulary,
in your corpus, then any, the least common ones will be replaced with a special xxunk
unknown token. So that'll help us avoid having a too big embedding matrix. All right, so
now we can treat the Numericalize() object, which we created as if it was a function,
as we so often do in both fastai and PyTorch, and when we do it will replace each of our
words with numbers. So two for example is 0, 1, 2 beginning of string beginning of,
beginning of stream. 8 0, 1, 2, 3, 4, 5, 6, 7, 8. Okay, so a capitalized letter there
they are, xxbos, xxmaj, etc. Okay, and then we can come and get them back by indexing
into the vocab and get back what we started with. Okay? Right, so now we have done the
tokenization, we've done the numericalization, and so the next thing we need to do is to
create batches. So let's say this is the text that we want to create batches from, and so
if we tokenize that text it'll convert it into this, and so let's, let's
take that and write it out here. Let's do it here. Let's take that and write it out
here, so �xxbos xxmaj in this chapter�, �xxbos xxmaj in this chapter, we will go
back over the example of classifying�, and then next row starts here �movie reviews
we studied in chapter one and dig deeper under the surface full stop xxmaj�, �first we
will look at the[...]� etc. Okay? So we've taken these ninety tokens, and to create a
batch size of six we've broken up the text into six contiguous parts, each of length
15. One, two, three, four, five, six, and then we have 15 columns. Okay? So 6 by 15.
Now ideally we would just provide that to our model as a batch, and if indeed that was
all the data we had we could just pass it in as a batch, but that's not going to work
for IMDb, because IMDb once we concatenate all the reviews together and then, let's say,
we want to use a batch size of 64, then we're going to have 64 rows and, you know, probably
there's a few million tokens of IMDb, so a few million divided by 64 across, it's going
to be way too big to fit in our GPU. So what we're going to do then is we're going to split
up that big wide array, and we're going to split it up horizontally so we'll start with
�xxbos xxmaj in this chapter�, and then down here �we will go back�, �over the
example of classifying�, �movie reviews we studied in�, �chapter 1 and
dig deeper�, �under the surface[...]�, etc. So this would become our first mini-batch.
Right, and so you can see what's happened is the, kind of, second row, right, actually
is fit, is is continuing what was like way down here, and so we basically treated each
row as totally independent. So when we predict the second, from the second mini-batch, you
know, the second mini-batch is going to follow from the first, and then each row to row one
in the second mini-batch will join up to row one of the first, row two of the second mini-batch
will join up to row two of the first. So please look at this example super carefully, because
we found that this is something that every year a lot of students get confused about,
because it's just not what they expected to see happen. Right? So go back over this and
make sure you understand what's happening in this little example. So that's what our
mini-batches are going to be. So the good news is that all these fiddly steps you don't
have to do yourself, you can just use the language model data loader or LMDataLoader.
So if we take those, all the tokens from the first 200 movie reviews, and map them through
our Numericalize object. Right? So now we've got numericalized versions of all those tokens,
and then pass them into LMDataLoader, and then grab the first item from the data loader,
and we have 64 by 72. Why is that? Well 64 is the default batch size, and 72 is the default
sequence length. You see here we've got 1, 2, 3, 4, 5, here we used a sequence length
of five. Right? So what we do in practice is we use a default sequence length of 72.
So if we grab the first of our independent variables and grab the first few tokens, and
look them up in the vocab... Here it is, �this movie which I just something at the video
store�. So that's interesting so this was not common enough to be in a vocab, �has
apparently sit around for a�, and then if we look at the exact same thing but for the
dependent variable rather than being �xxbos xxmaj match this movie�, its �xxmaj this
movie�, so you can see it's offset by 1, which means the end rather than being around
for �a�, it's �for a couple�. So this is exactly what we want. This is offset by
1 from here. So that's looking good. So we can now go ahead and use these ideas to try
and build our even better IMDb sentiment analysis, and the first step will be to as, we discussed,
create the language model but let's just go ahead and use the fastai built in stuff to
do it for us, rather than doing all that messing around manually, so we can just create a DataBlock,
and our blocks are, it's going to be a TextBlock, �.from_folder�, and the items are going
to be text files from these folders, and we're going to split things randomly and then going
to turn that into �.dataloaders� with a batch size of 128, and a sequence length
of 80. In this case our blocks... We're not just passing in a class directly, but we're
actually passing in here a class method, and that's so that we can allow the tokenization,
for example, to be saved, to be cached in some path, so that the next time we run this
it won't have to do it all from scratch. So that's why we have a slightly different syntax
here. So once we've run this we can call �show_batch�, and so you can see here we've got for example
�what xxmaj I've read, xxmaj death[...]� bla bla bla, and you can see so that's the
independent variable and so the dependent variable is the same thing offset by one.
So we don't have to �what� anymore it just goes straight to �xxmaj I've read�
and then at the end this was �also, this�, and of course in the dependent variable �also,
this is�. So this is that offset by one, just like we were hoping for. �show_batch�
is automatically denumericalizing it for us turning back into
strings but if we look at the actual (or you should look at the actual) x and y to confirm
that you actually see numbers there -- that'll be a good exercise for you, is to make sure
that you can actually grab a mini-batch from dls_lm. So now that we've got the data loaders
we can fine-tune our language model. So fine tuning the language model means we're going
to create a learner which is going to learn to predict the next word of a movie review.
So that's our data, the data loaders for the language model. This is the pre-trained model,
it's something called AWD_LSTM which we�ll see how to create from scratch in a moment,
or something similar to it. Dropout we'll learn about later, there we see how much dropout
to use, this is how much regularization we want, and what metrics do we want. We know
about accuracy; perplexity is not particularly interesting so I won't discuss it but feel
free to look it up if you're interested. And let's train with fp16 to use less memory on
the GPU, and for any modern GPU it'll also run two or three times faster. So this gray
bit here has been done for us, the pre-training of the language model for Wikitext103, and
now we're up to this bit which is fine-tuning the language model for IMDB. So let's do one
epoch, and as per usual the using a pre-trained model automatically calls freeze so we don't
have to freeze. So this is going to just actually train only the new embeddings initially and
we get an accuracy after a ten minutes or so of 30 percent. So that's pretty cool. So
about a bit under a third of the time, our model is predicting the next word of a string.
So I think that's pretty cool. Now, since this takes quite a while for each epoch, we
may as well save it and you can save it under any name you want. And that's going to put
it into your path, into your learner's path, into a model subfolder, and it will give it
a dot PTH extension for PyTorch. And then later on you can load that with learn.load()
after you create the learner. And so then we can unfreeze, and we can train a few more
epochs, and we eventually get up to an accuracy of thirty-four percent. So that's pretty great.
So once we've done all that we can save the model but actually all we really need to do
is to save the encoder. What's the encoder? The encoder is all of the model except for
the final layer. Oh and we're getting a thunderstorm here! That could be interesting. We've never
done a lesson with a thunderstorm before, but that's the joy of teaching during COVID-19
-- you get all the sound effects. So yeah, the final layer of our language model is the
bit that actually picks a particular word out, which we don't need. So when we say save
encoder, it saves everything except for that final layer. And that's the pre-trained model
we're going to use; that is a pre-trained model of a language model that is fine-tuned
from Wikipedia, fine-tuned using IMDB, and doesn't contain the very last layer. Rachel,
any questions at this point? �Do any language models attempt to provide meaning? For instance,
�I'm going to the store� is the opposite of �I'm not going to the store.� Or �I
barely understand this stuff� and �That ball came so close to my ear I heard it whistle�
both contain the idea of something almost happening, being right on the border. Is there
a way to indicate this kind of subtlety in a language model?� Yeah, absolutely our
language model will have all of that in it, or hopefully it will have it, or learn about
it. We don't have to program that. The whole point of machine learning is it learns it
for itself, but when it sees a sentence like �Hey careful that ball nearly hit me,�
the expectation of what word is going to happen next is going to be different to the sentence,
�Hey that ball hit me!� So, so yeah, language models generally you see in
practice tend to get really good at understanding all of these nuances of, of English or whatever
language it's learning about. Okay so we have a fine-tuned language model so the next thing
we're going to do is we're going to try to fine-tuning a classifier, but before we do,
just for fun, let's look at text generation. We can create, write ourselves some words
like �I liked this movie because,� and then we can create, say, two sentences each
containing, say, 40 words. And so we can just go through those two sentences and call learn.predict,
passing in this text and asking it to predict this number of words, with this amount of
kind of randomization, and see what it comes up with. �I liked this movie because of
its story and characters. The story line is very strong, very good for a sci-fi. The main
character, Alucard, was very well developed and brought the whole story�� But second
attempt: �I liked this movie because I like the idea of the premise of the movie the very
convenient virus which well when you have to kill a few people the evil machine has
to be used to protect� blah blah blah. So as you can see it's done a good job of inventing
language. There are much -- well I shouldn't say more sophisticated -- there are, there
are more careful ways to do a generation from a language model. This learn.predict() uses
the most kind of basic possible one, but even with a very simple approach, you can see we
can get from a fine-tuned model some pretty authentic-looking text. And so in practice
this is really interesting, because we can now, you know, by using the the prompt you
can kind of get it to generate a, you know, appropriate context-appropriate text, particularly
if you fine-tune for it from a particular corpus. Anyway that was really just a little
demonstration of something we accidentally created on the way, because of course the
whole purpose of this is actually just to be a pre-trained model for classification.
So to do that we're going to need to create another DataBlock, and this time we've got
two blocks, not one. We've got a text block again just like before, but this time we're
going to ask fastai not to create a vocab from the unique words, but using the vocab
that we already have from the language model. Because otherwise, obviously there's no point
reusing a pre-trained model if the vocab is different: the numbers would mean totally
different things. So that's the independent variable, and the dependent variable, just
like we've used before, is a category. So CategoryBlock is for that. As we've used many
times, we're going to use parent label to create our dependent variable, that's a function.
get_items we use get_text_files just like before, and we'll split using GrandparentsSplitter
as we've used before for vision. So this has been used for vision, this has been used for
vision and then we'll create our data loaders with a batch size of 128, a sequence length
of 72. And now show_batch here we can see an example of subset of a movie review and
the category. Yes, question? Do the tokenizers use any tokenization techniques like stemming
or lemmatization, or is that an outdated approach? That would not be a tokenization approach.
So stemming is something that actually removes the stem and we absolutely don't want to do
that. That is certainly an outdated approach. In English, we have stems for a reason: they
tell us something, so we don't like to remove anything that can give us some kind of information.
We used to use that for a kind of pre-deep learning NLP quite a bit, because that we
didn't really have good ways like embedding matrices of handling, you know, big vocabs
that just differed in the little, kind of the end of a word, but nowadays we definitely
don't want to do that. Oh yeah one other difference here is previously
we had an is_lm equals true when we said TextBlock.from_folder to save as a language model. We don't have
that anymore because it's not a language model. Okay. Now, one thing with a language model
that was a bit easier was that we could concatenate all the documents together and then we could
split them by batch size to create -- well not split them by batch size -- fit them into
a number of substrings based on the batch size. And that way we could ensure that every
mini-batch was the same size. It would be batch size by a sequence length. But for classification
we can't do that. We actually need each dependent variable label to be associated with each
complete movie review. And we're not showing the whole movie review here, we've truncated
it just for display purposes, but we're going to use the whole movie review to make our
prediction. Now the problem is that if we're using a batch size of 128 then, and our movie
reviews are often like 3,000 words long we could end up with something that's way too
big to fit into the GPU memory. So how are we going to deal with that? Well again, we're
going, we can split them up, so first of all let's grab a few of the movie reviews just
to demo here and numericalize them, and if we have a look at the length, so map the length
over each, you can see that they do vary a lot in length. Now we can we can split them
into sequences and indeed we have asked to do that -- sequence length 72 -- but when
we do so we're, you know, we don't even have the same number of sub-sequences. When we
split each of these into 72 long sections, they're going to be all different lengths.
So how do we deal with that? Well just like in vision we can handle different sized sequences
by adding padding. So we're going to add a special xx_pad token to every sequence in
a mini-batch. So like in this case it looks like 581 is the longest. So we would add enough
padding tokens to make this 581 and this 581 and this 581 and so forth, and then we can
split them into sequence_len and to 72 long, and it's where I'm in the mini-batches, and
we'll be right to go. Now obviously, if your lengths are very different like this, adding
a whole lot of padding is going to be super wasteful, so another thing that fastai does
internally is, it tries to shuffle the documents around so that similar length documents are
in the same mini-batch. It also randomizes them but it kind of approximately sorts them
so it wastes a lot less time on padding. Okay so that is how, that is what happens when,
we don't have to do any of that manually. When we call TextBlock.from_folder without
the is_lm, it does all that for us. And then we can now go ahead and create a learner.
This time it's going to be a text_classifier_learner; again we're going to base it off AWD_LSTM,
pass in the data loaders we just created; for metric we�ll just use accuracy, make
it fp16 again. And now we don't want to use a pre-trained Wikipedia model, in fact there
is no pre-trained Wikipedia classifier because, you know, what you classify matters a lot.
So instead we load the encoder (so remember, everything except the last layer) which we
saved just before. So we're going to load as our pre-trained model our language model
for predicting the next word of a movie review. So let's go ahead and fit one cycle, and again
by default it will it be frozen so it's only the final layer which is the randomly added
classifier layer that's going to be trained. It took 30 seconds and look at this -- we
already have 93 percent! So that's pretty similar to what we got back in lesson one
but rather than taking about 12 minutes once all the pre-training has been done it
takes about 30 seconds. This is quite cool: you can create a language model for your kind
of general area of interest and then you can create all kinds of different classifiers
pretty quickly. And so that's just with, that's just looking at fine-tuning the final randomly
added layer. So now we could just unfreeze and keep learning, but something we found
is for NLP it's actually better to only unfreeze one layer at a time, not to unfreeze the whole
model. So we've, in this case we've automatically unfrozen the last layer and so then to unfreeze
the last couple of layer groups, we can say freeze_to(-2) and then train a little bit
more, and look at this! We're already beating, after a bit over a minute, easily beating
what we got in Lesson 1. And then freeze_to(-3) to unfreeze another few layers. Now we're
up to 94. And then finally unfreeze the whole model and we're up to about ninety four point
three percent accuracy, and that was literally the state of the art for this very heavily
studied dataset just three years ago. If you also reverse all of the reviews to make them
go backwards and train a second model on the backwards version, and then average the predictions
of those two models as an ensemble. you get to ninety-five point one percent accuracy.
And that was the state of the art that we actually got in the ULMFIT paper, and it was
only beaten for the first time a few months ago, using a way, way bigger model, way more
data, way more compute, and way more data augmentation. I should mention actually with
the data augmentation, one of the cool things they did do was they actually figured out
also a way to even beat our 95.1 with less data as well. So I should mention that actually
the data augmentation has become a really, since we created the ULMFIT paper, has become
a really, really important approach. Any questions, Rachel? �Can someone explain how a model
trained to predict the last word in a sentence can generalize to classify sentiment? They
seem like different domains.� Yeah, that's a great question. They're very different domains
and it's it's really amazing and basically the trick is that to be able to predict the
next word of a sentence, you just have to know a lot of stuff about not only the language,
but about the world. So if you know... let's say we wanted to finish the next word of this
sentence: �By training a model on all the text read backwards and averaging the predictions
of these two models, we can even get to 95.1% accuracy, which was the state of the art introduced
by the --� what? So to be able to fill in the word �ULMFIT� you would have to know
a whole lot of stuff about, you know, the fact that there's a thing called fine-tune,
you know, pre-trained language models and which one gets which results and that ULMFIT
got this particular result. I mean that would be an amazing language model that could fill
that in correctly. I'm not sure that any language models can but to give you a sense of like,
what you have to be able to do to be good at language modeling. So if you're going to
be able to predict the next word of a sentence, like, �Wow I really love this movie, I love
every movie containing Meg�... what? Right, maybe it's Ryan. You would have to know about
like the fact that Meg Ryan is an actress, and actresses are in movies, and so forth,
so when you know so much about English and about the world, to then turn that into something
which recognizes that �I really love this movie� is a good thing rather than a bad
thing is just not a very big step. And as we saw, you can actually get that far using
just fine-tuning just the very last layer or two. So it is, it's amazing, and I think
that's super super cool. All right another question. �How would you do data
augmentation on text?� [Laughs] Well� You would probably google for unsupervised
data augmentation, and read this paper, and things that have cited it. So this was the
one that easily beat our IMDb result with only 20 labeled examples, which is amazing,
right, and so they did things like, if I remember correctly, translate every sentence into a
different language and then translate it back again. So you kind of get like, different
rewordings of the sentence that way. Yeah. So kind of tricks like that. Now let's go
back to the generation thing. So remember we saw that we can generate context appropriate
sentences, and it's important to think about what that means in practice. When you can
generate context appropriate sentences... Have a look, for example, at even before this
technology existed in 2017. The FCC asked for comments about a proposal to repeal net
neutrality, and it turned out that less than 800,000 of the 22 million comments actually
appeared to be unique, and this particular person Jeff Kao discovered that a lot of the
submissions were slightly different to each other by, kind of, like picking up different,
you know, the green bit would either be �citizens�, �or people like me�, or �Americans�
and the red bit would be �as opposed to� or �rather than�, and so forth. So like...
And, and that made a big difference to, I believe to, American policy. You know, here's
an example of a Reddit conversation, �You're wrong the defense budget is a good example
of how badly the U.S. spends money on the military.� dot dot dot. Somebody else, �Yeah,
but that's already happening. There's a huge increase in the military budget[...]� dot
dot dot. �I didn't mean to sound like �stop paying for the military.� I'm not saying
that we cannot pay the bills[...]� dot dot dot. Now, all of these are actually created
by a language model or GPT-2, and this is a very concerning thing around disinformation.
Is that never mind fake news, never mind deep fakes. Think about like what would happen
if somebody invested a few million dollars in creating a million Twitter bots, and Facebook
Groups bots, and Weibo bots, and made it so that 99% of the content on social networks
were deep learning bots, and furthermore they were trained not just to optimize the next
word of a sentence, but were trained to optimize the level of disharmony created, or the level
of agreeableness, from some of the half of them, and disagreeableness for the other half
of them. You know you could create like a whole lot of, you know, just awful toxic discussion,
which is actually the goal of a lot of propaganda outfits. Is not so much to push a particular
point of view, but to make people feel like �there's no point engaging because the truth
is too hard to understand� or, or whatever. So I'm... Rachel and I are both super worried
about what could happen to discourse, now that we have this incredibly powerful tool,
and I'm not even sure we have... We don't have a great sense of what to do about it.
Algorithms are unlikely to, are unlikely to save us here. If you could create a classifier
which could do a good job of figuring out whether something was generated by a algorithm
or not, then I could just use your classifier as part of my training loop to train an algorithm
that can actually learn to trick your classifier. So this is a real worry, and the only solutions
I've seen are those which are, kind of, based on cryptographic signatures, which is another
whole can of worms, which has never really been properly sorted out, at least not in
the Western world, and a privacy centric way. All right. So� Oh... Yes, before we have
a break. Just on that note I'll add and I'll link to this on the forums, I gave a keynote
at SciPy Conference last summer, which is the Scientific Python conference, and went
into a lot more detail about the, the threat that Jeremy is describing about,
using advanced language models to manipulate public opinion, and so if you want to, kind
of, learn more about the dangers there and exactly what that threat is, you can find
that in, in my SciPy keynote. Great. Thanks so much Rachel. So let's have a five-minute
break, and see you back here in five minutes. So we're going to finish with a kind of a
segue into what will eventually be part two of the course, which is to go right underneath
the hood, and see exactly how more complex architecture works, and specifically we're
going to see how a neu recurrent neural network works. Do we have a question first? �In
the previous lesson MNIST example, you showed us that under the hood the model was learning
parts of the image like curves of a 3 or angles of a 7. Is there a way to look under the hood
of the language models to see if they are learning rules of grammar and syntax? Would
it be a good idea to fine-tune models with examples of domain-specific syntax like technical
manuals, or does that miss the point of having the model learn for themselves?� Yeah there
are tools that allow you to, kind of, see what's going on inside an NLP model. We're
not going to look at them in this part of the course. Maybe we will in part two, but
certainly worth doing some research to see what you can find and they�re certainly
PyTorch libraries you can download and play with. Yeah, I mean, I think it's a perfectly
good idea to incorporate some kind of technical manuals and stuff into your training corpus.
There's actually been some recent papers on this general idea of, trying to, kind of,
create some carefully curated sentences as part of your training corpus. It's unlikely
to hurt and it could well help. All right so let's have a look at RNNs. Now, when Sylvain
and I started creating the RNN stuff for fastai, the first thing I did actually was to create
a new dataset, and the reason for that is I didn't find any datasets that would allow
for quick prototyping, and really easy debugging. So I made one which we call �human numbers�,
and it contains the first 10,000 numbers, written out in English. And I am surprised
at how few people create datasets. I create datasets frequently. You know, I specifically
look for things that can be, kind of, small, easy to prototype, good for debugging, and
quickly trying things out, and very, very few people do this, even though like this
�human numbers� dataset, which has been so useful for us, took me, I don't know, an
hour or two to create. So this is definitely an underappreciated, underutilized technique.
So we can grab the �human numbers� dataset, and we can see that there's a training and
a validation text file. We can open each of them, and for now we're just going to concatenate
the two together into a file called �lines�, and you can see that the contents are 1, 2,
3, etc., and so there's a new line at the end of each. We can concatenate those all
together and put a full-stop between them. As so... Okay? And then you could tokenize
that by splitting on spaces, and so for example here's tokens 100 to 110. New number 42, new
number 43, new number, 44, and so forth. So you can see I'm just using plain Python here,
there's not even any PyTorch, certainly not any fastai. To create a vocab we can just
create all the unique tokens, of which there are 30, and then to create a lookup from...
So that's a lookup from a word to an ID. Sorry from an ID to a word. To go from a word to
an ID, we can just enumerate that, and create a dictionary from word to ID. So then we can
numericalize our tokens by calling word to index on each one, and so here's our tokens,
and here is the equivalent numericalized version. So you can
see on fairly small datasets, when we don't have to worry about scale, and speed, and
have the details of tokenization in English, you can do the whole thing in just plain Python.
The only other thing we used, did for to save a little bit of time was use L(), but you
could easily do that with the Python Standard Library in about the same amount of code.
so hopefully that gives you a good sense of really what's going on with tokenization and
numericalization, all done by hand. So let's create a language model. So one way to create
a language model would be to go through all of our tokens, and let's create a range from
0 to the length of our tokens minus 4, and every 3 of them, and so that's going to allow
us to grab three tokens at a time 1 dot 2 dot 3 dot 4 dot five dot 6 dot 7 dot 8, and
so forth. Right? So here's the first three tokens, and then here's the 4th token, and
here's the second three tokens, and here's the seventh token, and so forth. So these
are going to be our independent variables and this will be our dependent variable. So
here's a super, super, kind of, naive simple language model dataset for the �human numbers�
question. So we can do exactly the same thing as before, but use the numericalized version
and create tensors. So this is exactly the same thing as before, but now is as through
numericalized tensors, and we can create a DataLoaders() object �.from_datasets�,
and remember these are datasets because they have a length, and we can index into them.
Right? And so we can just grab the first 80% of the tokens as the training set, the last
20% is the validation set, like so. Batch size 64, and we're ready to go. So we really
used very, very little, I mean, the only Pytorch we used was to create these tensors, and the
only fastai we used was to create the DataLoaders(), and it's just grabbing directly from the datasets,
so it's really not doing anything clever at all. So let's see if we can now create a neural
network architecture, which takes three numericalized words at a time as input, and tries to predict
the fourth, as dependent variable. So here is just such language model. It's a three
layer neural network. So we've got a linear layer here, which we're going to use once,
twice, three times, and after each of them, we call relu(), as per usual, but there's
a little bit more going on. The first interesting thing is that rather than each of these being
a different linear layer, we've just created one linear layer here, which we've reused,
as you can see, one, two, three times. So that's the first thing that's a bit tricky,
and so there's a few things going on. It's a bit, a little bit different to usual, but
the basic idea is here. We've created an Embedding, an nn.Linear, another nn.Linear, and in here
we've used the linear layers, and relu(), so it's very nearly a totally standard three
layer neural net. We have a.... I guess four really, because this is an output layer. Yes
Rachel? We have a question. Sure. �Is there a way to speed up fine-tuning the NLP model?
10 plus minutes per epochs slows down the iterative process quite a bit. Any best practices
or tips?� I can't think of any obviously other than to say you don't normally need
to fine-tune it that often. You know, the work is often more at the classifier stage.
So yeah I tend to, kind of, just leave it running overnight, or while I have lunch,
or something like that. Yeah, just don't make sure you, make sure you don't sit there watching
it. Go and do something else. This is where it can be quite handy to have a second GPU,
or fire up a second AWS instance, or whatever, so you can, kind of, keep, keep moving while
something's training in the background. All right, so what's going on here in this model?
To describe it we're actually going to develop a little, kind of, pictorial representation,
and the pictorial representation is going to work
like this: Let's start with a simple linear model to define this pictorial representation.
A simple linear model has an input of size batch size by number of inputs and so we're
going to use a rectangle to represent an input. We're going to use an arrow to represent a
layer of computation. So in this case, there's going to be a matrix product for a simple
linear model there'd be a matrix, actually sorry, this is a single hidden layer model.
There'll be a matrix product followed by a ReLU. So that's what this arrow represents,
and out of that we're going to get some activations. And so, circles represent computed activations,
and it would be, we call this a hidden layer, it would be of size batch size by number of
activations. That's its size. And then, to create a neural net we're going to do a second
matrix product and this time of softmax so the computation again, represented by the
arrow, and then output activations are a triangle. So the output would be batch size by num classes.
So let me show you the pictorial version of this. So this is going to be our legend, triangle
is output, circle, hidden, rectangle, input. And here it is, we're going to take the first
word as an input, it's going to go through a linear layer and ReLU, and you'll notice
here, I've deleted the details of what the operations are at this point and I've also
deleted the sizes. So every arrow is basically just a linear layer followed by a non-linearity,
so we take the word one input, and we put it through the layer, the linear layer and
the non-linearity, to give us some activations. So there's our first set of activations. And
when we put that through another linear layer, and non-linearity to get some more activations,
and at this point we get word two. And word two, is now, goes through linear layer and
a non-linearity, and these two, when two arrows together come to a circle, it means that we
add or concatenate (either is fine), the two sets of activations. So we'll add the set
of activations from this input, to the set of activations from here to create a new set
of activations, and then we'll put that through another linear layer and a ReLU. And again
word three is now going to come in and go through a linear layer and a ReLU, and they'll
get added to create another set of activations and then they'll find, go through a final
linear layer and ReLU, and I guess, softmax to create our output activations. So this
is our model, it's basically a standard one, two, three, four, layer model but a couple
of interesting things are going on the first is that we have inputs coming in to later
layers and get added, so that's something we haven't seen before, and the second is,
all of the arrows that are the same color, use the same weight matrix. So every time
we get an input, we're going to put it through a particular weight matrix, and every time
we go from one set of activations to the next or put it through a different weight matrix,
and then to go through the activations to the output we'll use the different weight
matrix. So if we now go back to the code, to go from input to hidden, not surprisingly,
we always use an embedding. So in other words an embedding is, the green okay, and you'll
see we just create one embedding, and here is the first, so here's X which is the three
words. So here's the first word X zero, and it goes through that embedding. And word two,
goes through the same embedding, and word three index number two, goes through the same
embedding. And then each time you see, we add it to the current set of activations.
And so having put the, got the embedding, we then put it through this linear layer.
And again, we get the embedding, add it to the hid, to the activations, and put it through
the linear, that with linear layer. And again, the same thing here, put it through the same
linear layer, so h_h is the orange. So these set of activations we call the hidden state,
okay. And so the hidden state is why it's called �h� and so if you follow through
these steps, you'll see how each of them corresponds to a step in this diagram. And then finally
at the end, we go from the hidden state to the output (which is this linear layer) hidden
state to the output, okay. and then we don't have the actual Softmax there, because as
you'll remember, we can incorporate that directly into the loss function, the cross-entropy
loss function using PyTorch. So one nice thing about this is, everything
we're using we have previously created from scratch. So there's nothing magic here. We've
created our own embedding layer from scratch, we've created our own linear layer from scratch,
we've created our own ReLU from scratch, we've created our own cross entropy loss from scratch.
So you can actually try building this whole thing yourself, from scratch. So why do we,
just in terms of the nomenclature, �i_h�, so �h� refers to hidden so this is a layer
that goes from input to hidden. This is one that goes from hidden to hidden, this is one
that goes from hidden to output. So if any of this is feeling confusing at any point,
go back to where we actually created each one of these things from scratch and create
it from scratch again. Make sure you actually write the code, so that nothing here is mysterious.
So why do we use the same embedding matrix each time we have a new input word, for an
input word index 0, 1, and 2? Well because conceptually, they all represent English words
of content, you know for human numbers. So why would you expect them to be a different
embedding? They all should have the same representation, because they alll have the same meaning. Same
for this hidden to hidden - at each time we're basically describing how to, how to go from
one token to the next of our language model, so we would expect it to be the same computation.
So that's basically what's going on here. So having created that model, we can go ahead
and instantiate it. So we�re going to have to pass in the vocab size for the embedding
and the number of hidden, right, so that's the number of activations. So here we create
the model, and then we could create a learner by passing in a model and our data loaders
and a loss function and optionally metrics. And we can fit. Now of course this is not
pre-trained, right. This is not an application-specific learner, so it wouldn't know what pre-trained
model to use. So this is all random. And we're getting somewhere around the 45% to 50% or
so accuracy. Is that any good? Well you should always compare to random or, not random - you
should always compare to like the simplest model, where the simplest model is like some
average or something. So what I did is, I grabbed the validation set (so all the tokens),
put it into a Python standard library counter (which simply counts how many times each thing
appears). I found that the word �thousand� is the most common. And then I said, okay,
what if we used 7104 with �thousand� (that's here) and divide that by the length of the
tokens, and we get 15%. Which in other words means if we always just predicted, I think
the next word will be �thousand�, we would get 15% accuracy. But in this model we got
around 45% to 50% accuracy. So in other words, our model is a lot better than the simplest
possible baseline, so we've learnt something useful, that's great. So the first thing we're
going to do is we're going to refactor this code. Because you can see, we've got x going
into i_h, going into h_h, going into relu. x going into i_h, going into h_h, going into
relu, going into i_h, going into h_h, going into relu. How would you refactor that in
Python? You would, of course, use a for loop so let's go ahead and write that
again. So these lines are code or identical. In fact, these lines of code are identical
(as is this), and we're going to instead of doing all that stuff manually we�ve got
a loop that goes through three times and in each time it goes - i_h add to our h_h value
and then at the end, hidden to output. So this is exactly the same thing as before,
but it's just reflected with a for loop. And we can train it again, and again we get the
same basically 45% to 50% as you would expect, because it's exactly the same, it's just been
refactored. So here's something crazy - this is a recurrent neural network. Even though
it's like exactly the same as ... it's exactly the same as this, right? It's just been refactored
into a loop. And so believe it or not, that's actually all an RNN is. An RNN is a simple
refactoring of that model with that deep learning linear model we saw. I shouldn't say linear
model, a deep learning model of simple linear layers with ReLUs. So let's draw our pictorial
representation again (so remember this was our previous pictorial representation). We
can refactor the picture as well, so instead of showing these dots separately, we can just
take this arrow and represent it it, represent it as a loop. Because that's all that's happening,
right. So the word 1 goes through an embedding, goes into this activations, which then just
gets repeated from 2 to, 2 to n-1, where n at this time is, you know, we've got three
words basically, for each word coming in as well. And so we've just refactored our diagram.
And then eventually it goes through our blue to create the output. So this diagram is exactly
the same as this diagram, just replacing the middle with that loop. So that's a recurrent
neural net and so �h�, remember was something that we just kept track of here �h�, �h�,
�h�, �h�, �h�, �h�, as we added each layer to it. And here we just have
it inside the loop. We initialize it as 0, which is kind of tricky. And the reason we
can do that is that 0 plus a tensor will broadcast a 0. So that's a little neat feature. That's
why we don't have to make this a particular size tensor to start with. Okay. So we're
going to be seeing the word hidden state a lot. And so it's important to remember that
hidden state simply represents these activations that are occurring inside our recurrent neural
net and a recurrent neural net is just a refactoring of a particular kind of fully connected deep
model. So that's it, that's what an RNN is. No questions at this point? Rachel? Something
that's a bit weird about it, though, is that for every batch we're setting our hidden state
to zero, even although we're going through the entire set of numbers, the human numbers
dataset in order. So you would think that by the time you've gone like 1, 2, 3 you shouldn't
then forget everything we've learnt when you go to 4, 5, 6, right. It would be great to
actually remember where we're up to and not reset the hidden state back to 0 every time.
So we can absolutely do that. We can maintain the state of our RNN. And here's how we would
do that. Rather than having something called �h� we'll call it self.h and we'll set
it to 0 at the start when we first create our model. Everything else here is the same,
and everything else here is the same. And then there's just one extra line of code here.
What's going on here. Well here's the thing. If �h� is something which persists from
batch to batch, then effectively this loop is effectively kind of becoming infinitely
long, right. Our deep learning model, therefore, is getting effectively (well, not infinitely
deep), but as deep as the entire size of our dataset because every time we're stacking
new layers on top of the previous layers. The reason this matters is that when we then
do back propagation, when we then calculate the gradients, we're
going to have to calculate the gradients all the way back through every layer going all
the way. So by the time we get to the end of the dataset, we're going to be effectively
backpropagating, not just through this loop, but remember self.h was created also by the
previous call to forward. and the previous call forward and the previous call to forward.
So we're going to have this incredibly slow calculation of the gradients all the way back
to the start. It's also going to use up a whole lot of memory, because it's going to
have to store all those intermediate gradients in order to calculate them. So that's a problem.
And so the problem is easily solved by saying �detach�. And what detach does is it basically
says, �Throw away my gradient history, forget that I, forget that I was calculated in terms
of other gradients.� So the activations are still stored, but the gradient history
is no longer stored. And so this kind of cuts off the gradient computation. And so this
is called truncated backpropagation. So, exactly the same lines of code as the other two models.
h=0 is being has been moved into self.h=0. These lines of code are identical. And we've
added one more line of code. So the only other thing is that from time to time, we might
have to reset self.h to 0. So I've created a method for that. And we'll see how that
works shortly. Okay, so back propagation .. Oh sorry, I was using the wrong jargon - �backpropagation
through time� is what we call it when we calculate the backprop over going back through
this loop. All right, now we do need to make sure that the samples are seen in the correct
order. You know, given that we need to make sure that every batch connects up to the previous
batch. So go back to a Notebook 10 to remind yourself of what that needs to look like.
But basically the first batch, we see that the number (the), the length of our sequences
divided by the batch size is 328. So the first batch will be index #0, then m, then 2*m,
and so forth. The second batch will be 1, m+1, 2*m +1, and so forth. So the details
don't matter, but here's how we create, you know, do that indexing. So now we can go ahead
and call that group_chunks function to calculate, to create our training set and our validation
set. And certainly don't shuffle, because that would break everything in terms of the
ordering. And then there's one more thing we need to do, which is we need to make sure
that at the start of each epoch we call reset. Because at the start of the epoch, we're going
back to the start of our natural numbers. So we need to set self.h back to zero. So
something that we'll learn about in Part 2 is that fast.ai has something called Callbacks.
And callbacks are classes which allow you to basically say, �During the training loop
I want you to call some particular code.� And in particular, this is going to call this
code. And so you can see callbacks are very small, or can be very small, they're normally
very small. When we start training, it'll call reset. When we start validation, it'll
call reset (so this is each epoch). And when we're all finished fitting, it will call reset.
And what does reset do? It does whatever you tell it to do and we told it to set self.h
to equal zero. So if you want to use a callback, you can simply add it to the callbacks list,
cbs, when you create your Learner. And so now when we train, that's way better, okay.
So we've now actually kept this, it�s called a stateful RNN. It's actually keeping the
state, keeping the hidden state from batch to batch. Now we still got a bit of a obvious
problem here, which is that if you look back to the data
that we created, we used these first three tokens to predict the fourth, and then the
next three tokens to predict the seventh, and then the next three tokens to predict
the one after, and so forth. Effectively what we�d rather do you would think is, is predict
every word, not just every fourth word. It seems like we're throwing away a lot of signal
here, which is pretty wasteful. So we want to create more signal, and so the way to do
that would be rather than putting, rather than putting this output stage outside the
loop. Right? So this dotted area is the bit that it's looped. What if we put the output
inside the loop? So in other words, after every hidden state was created we immediately
did a prediction, and so that way we could predict after every time step, and our dependent
variable could be the entire sequence of numbers offset by one. So that would give us a lot
more signal. So we have to change our data, so the dependent variable has each of the
next three words after each of the three inputs, so instead of being just the numbers from
i to i+sl as input, and then i+sl+1 as output, we're going to have the entire set offset
by one as our dependent variable. So, and then, we can now do exactly the same as we
did before to create our DataLoaders(), and so you can now see that each sequence is exactly
the previ[...], is exactly the independent variable and the dependent variable. The same
thing, but offset by one. Okay? And then we need to modify our model very slightly. This
code is all exactly the same as before, but rather than now returning one output, we�ll
create a list of outputs, and will append to that list after every element of the loop.
And then at the end we'll stack them all up, and then this is the same. So it's nearly
exactly the same. Okay? Just a very minor change. Our loss function needs to... We need
to create our own loss function, which is just the cross entropy loss, but we need to
just flatten it out. So the target gets flattened out, the input gets flattened out, and so
then... We can now pass that as our loss function. Everything else here is the same, and we can
fit, and we've gone from.... I can't remember. 58 to 64. So it's improved a little bit, so
that's good. You know, I did, we did find this a little, little flaky. Sometimes it
would train really well, sometimes it wouldn't train great, but sometimes, you know, we often
got this reasonably good answer. Now, one problem here is although effectively we have
quite a deep neural net, if you, kind of, go back to the version... So this, this version
where we have the loop in it, is kind of the normal way to think about an RNN, but perhaps
an easier way to think about it is what we call the unrolled version, and the unrolled
version is when you at it like this. Now, if you unroll this stateful neural net we
have, you know, it's, it is quite deep, but every single one of the hidden to hidden layers
uses exactly the same weight matrix. So really, it's not really that deep at all, because
it can't really do very sophisticated computation, because it has to use the same weight matrix
every time. So in some ways it's not really any smarter than a plain linear model. So
it would be nice to try to, you know, create a truly deep model. Have multiple different
layers that it can go through. So we can actually do that easily enough by creating something
called a Multilayer RNN, and all we do is we basically take that diagram we just saw
before, and we repeat it, but, and this is actually a bit unclear, the, the dotted arrows
here are different weight matrices, to the non dotted arrows here. So we can have a different
hidden to hidden weight matrix in the, kind of, second set of RNN layers, and a different
weight matrix here for this second set, and so this is called a stacked RNN or a multi-layer
RNN. And so here's the same thing in the unrolled version. Right? So this is exactly the
same thing, but showing you the unrolled version. Writing this out by hand, maybe that's quite
a good exercise, or particularly this one would be quite a good exercise, but it's kind
of tedious, so we're not going to bother, instead we're going to use PyTorch�s RNN
class, and so PyTorch�s RNN class is basically doing exactly what we saw here. Right? And
specifically this, this part here, and this part here. Okay? But it's nice that it also
has an extra number of layers parameter, that lets you tell it how many to stack on top
of each other. So it's important when you start using PyTorch�s RNN to realize there's
nothing magic going on. Right? You're just using this refactored for loop that we've
already seen. So we still need the input to hidden embedding. This is now the hidden to
hidden, with the loop all done for us, and then this is the hidden to output, just as
before, and then this is our hidden, just like before. So now we don't need the loop
we can just call �self.rnn()� and it does the whole loop for us. We can do all the input
to hidden at once to save a little bit of time, because thanks to the wonder of embedding
matrices, and as per usual we have to go to call �detach()� to avoid getting a super
deep effective network, and then pass it through our output linear layer. So this is exactly
the same as the previous model, except that we have just refactored it using an �nn.RNN()�,
and we said we want more than one layer. So let's request, say two layers. Now, we still
need the ModelReseter, just like before, because remember nothing's changed, and let's go ahead
and fit, and oh, it's terrible. So, why is it terrible? Well the reason it's terrible
is that now we really do have a very deep model, and very deep models are really hard
to train, because we can get exploding or disappearing activations. So what that means
is, we start out with some initial state and we're gradually putting it through all of
these layers, and all of these layers. Right? And so each time we're doing a matrix multiplication,
which remember is just doing a whole bunch of multiplies and adds, and then we multiply
and add, and we multiply and add, and we multiply and add, and if you do that enough times you
can end up with very, very, very big results, or it's so that would be if the kind of things
are multiplying and adding by a pretty big, or very, very, very, very small results. Particularly
because we're putting it through the same layer again and again. Right? And why is that
a problem? Well if you multiply by two a few times, you get 1, 2, 4, 8, etc., and after
32 steps you're already at four billion, or if you start at one and you're multiply by
half, a few times after, 32 steps you're down with this tiny number. So a number even slightly
higher or lower than one can, kind of, cause an explosion or disappearance of a number,
and matrix multiplication is just multiplying numbers and adding them up. So exactly the
same thing happens to a matrix multiplication, you kind of have matrices that, that grow
really big or grow really small. And when that does that you're also going to have exactly
the same things happening to the gradients. They'll get really big or really small. And
one of the problems here is that numbers are not stored precisely in a computer. They're
stored using something called floating point, so we stole this nice diagram from this article
called, �What you never wanted to know about floating point but will be forced to find
out�, and here we're at this point where we're forced to find out, and it's basically
showing us the granularity with which numbers are stored, and so the numbers that are further
away from zero are stored much less precisely than the numbers that are close to zero, and
so if you think about it that means that the gradients further away from zero, could actually,
for very big numbers, could actually become zero themselves, because you could actually
end up in, kind of, with two numbers that is, that are between these, kind of, little
gradations here. And you actually end up with the same thing with the really small numbers,
because the really small numbers although they're closer together, the numbers that
they represent are also very close together. So in both cases the, kind of, the relative
accuracy gets worse and worse. So you really want to avoid this happening. There's a number
of ways to avoid this happening, and this is the same for really deep convolutional
neural nets, or really deep, kind of, tabular, standard tabular networks. Anytime you have
too many layers, it become, it can become difficult to train and you generally have
to use like the, really small learning rates, or you have to use special techniques that
avoid exploding or disappearing activations or gradients. For RNNs one of the most popular
approaches to this is to use an architecture called an LSTM, and I am not going to go into
the details of an LSTM from scratch today, but it's in the, it's in the book, and in
the notebook, but the key thing to know about an LSTM is... Let's have a look.... Is that
rather than just being a matrix multiplication, it is this, which is that there are a number
of linear layers that it goes through, and those linear layers are combined in particular
ways, and the way they're combined, which is shown in this, kind of, diagram here, is
that it basically is designed such that the, that there are like little mini neural networks
inside the layer, which decide how much of the previous state is kept, how much is thrown
away, and how much of the new state is added. And by letting it have little neural nets
to, kind of, calculate each of these things it allows the LSTM layer, which, again, is
shown here, to decide how much of, kind of, how much of an update to do at each time,
and then with that capability it basically allows it to avoid, kind of, updating too
much or updating too little, and by the way this, this code you can refactor, which Sylvain
did here into a much smaller amount of code, but these two things are exactly the same
thing. So, as I said, I'm not going to worry too much about the details of how this works,
now. The important thing, just to know, is that you can replace the matrix multiplication
in an RNN with this sequence of matrix multiplications, and sigmoids, n times a plus, and when you
do so, you will very significantly decrease the amount of gradient or activation exploding,
explosions or disappearances. So that's called an LSTM cell, and an RNN which uses this,
instead of a matrix multiplication, is called an LSTM, and so you can replace �nn.rnn()�
with �nn.LSTM()�. Other than that we haven't really changed anything, except that LSTMs,
because they have more of these layers in them, we actually have to make our hidden
state have more layers in, as well, but other than that we can just replace LSTM, RNN with
LSTM, and we can call it just the same way as we did before. We can �detach()� just
like before, but there's now a list, so we have to detach all of them, and pop it through
our output layer, which is exactly as before. Our �reset()� is just as before, except
it's got to loop through each one, and we can fit it in exactly the same way as before,
and as you can see we end up with a much better result, which is great. We have� We have
two questions. Okay, perfect. �Could we somehow use regularization to try to make
the RNN parameters close to the identity matrix, or would that cause bad results because the
hidden layers want to deviate from the identity during training?� So we're actually about
to look at regularization. So we will take a look. The
identity matrix for those that don't know or don't remember, is the matrix where if
you multiply by it, you get exactly the same thing that you started with. So just like
if you multiply by one you get back the same number you started with. For linear algebra
if you multiply by the identity matrix you get the same matrix you started with, and
actually one quite popular approach to initializing the hidden to hidden activations is to initialize
with a identity matrix, which ensures that you start with something which doesn't have
gradient explosions, or activation explosions. There are... Yeah... Well I won't, and we're
about to have a look at some more regularization approaches so let's, let's wait until we do
that. All right, next question. �Is there a way to quickly check if the activations
are disappearing/exploding?� Absolutely just go ahead and calculate them and we'll
be looking at that a lot more detail in part two, but a really great exercise would be
to try to figure out how you can actually output the activations of each layer. And
it would certainly be very easy to do that in the, in the RNNs that we built ourselves
from scratch, because we can actually see the linear layers, and so you could just print
them out, or print out some statistics, or store them away, or, or something like that.
fastai has a class called ActivationStats(), which,
kind of, you can check out if you're interested. That's a really good way to specifically to
do this. Okay. So yeah... So regularization is important. We have potentially a lot of
parameters and a lot of layers. It would be really nice if we can do the same kind of
thing that we've done with, with our CNNs and so forth, which is to use more parameters,
but then use regularization to ensure that we don't overfit, and so we can certainly
do that with an LSTM as well, and perhaps the best way to do that is to use something
called dropout, and dropout is not just used for RNNs. Dropout is used all over the place,
but it works particularly well in RNNs. This is a picture from the dropout paper, and what
happens in dropout is... Here's a, here�s a, kind of a, picture of a, of three fully
connected layers. Not sorry, I guess it's two, one, two. Yeah, no three fully connected
layers. And so in these two fully connected layers at the start here, what we could do
is we could delete some of the activations at random, and so this has happened here but
X... This is what X means, it's like deleting those, those activations at random, and if
we do so you can see we end up with a lot less computation going on, and what dropout
does is each batch, each mini-batch, it randomly deletes a different set of activations from
whatever layers you ask for. That's what dropout does. So so basically the idea is that dropout
helps to generalize because if a particular activation was, kind of, effectively learning
some input, some, some particular piece of input, memorizing it, then sometimes it gets
randomly deleted, and so then suddenly it's not going to do anything useful at all. So
by randomly deleting activations it ensures that activations can't become over specialized
at doing just one thing, because then if it did, then the times they're randomly deleted,
it's, it's not going to work. So here is the entire implementation of a dropout layer.
You pass it some value p, which is the probability that an activation gets deleted. So we'll
store that away, and so then in the forward you're going to get your activations. Now,
if you're not training, so if you're doing validation, then we're not going to do dropout.
Okay? But if we are training, then we create a mask, and so the mask is Bernoulli random,
a Bernoulli random variable. So what does the know any random variable means? It means
it's a bunch of ones and zeros, where this is the probability that we get
a 1, which is 1 minus the probability we get a 0. And so then we just multiply that by
our input, so that's going to convert some of the inputs into zeroes, which is basically
deleting them. So you should check out some of the details, for example, about why we
do divide 1 minus p, which is described here, and we do we point out here that normally
and I would normally in the lesson show you an example of the, of what Bernoulli does,
but of course nowadays, you know, we're getting to the advanced classes, you're expected to
do it yourself. So be sure to create a little cell here, and make sure you actually create
a tensor, and then run Bernoulli underscore on it, and make sure you see exactly what
it's doing, so then you can understand this class. Now, of course we don't have to use
this class we made ourselves, we can just use nn.Dropout, but you can use this class
yourself, because it does the same thing. So again, you know, we're trying to make sure
that we know how to build stuff from scratch. This special �self.training� is set for
every module automatically by fastai to... Based on whether or not you're in the validation
part of your training loop, or the training part of your training loop. It's also part
of PyTorch and in PyTorch, if you're not using fastai, you have to call the �train()�
method on a module to set training to true and the �eval()� method to set it to false
for every module inside some other module. So that's one great approach to regularization.
Another approach, which I've only seen used in recurrent neural nets, is a activation
regularization and temporal activation regularization, which is very very similar to the question
that we were just asked. What happens with activation regularization is it looks very
similar to weight decay, but rather than adding some multiplier times the sum of squares of
the weights, we add some multiplier by the sum of squares of the activations. So in other
words we're basically saying, we're not just trying to decrease the weights, but decrease
the total activations, and then similarly we can also see what's the difference between
the activations from the previous time step to this time step, so take the difference
and then again, squared times some value. So the� These are two hyper parameters,
alpha and beta. The higher they are the more regularized your model, and so with TAR it's
going to say, no layer of the LSTM should too dramatically change the activations from
one time step to the next, and then for alpha it's saying, no layer of the LSTM should create
too large activations, and so they wouldn't actually create these large activations, or
large changes and unless the loss improved by enough to make it worth it. Okay, so this
then... I think just one more thing we need to know about, which is called weight tying,
and weight tying is a very minor change, and let's have a look at it here. So this is the
embedding we had before, this is the LSTM we had before, this is where we're going to
introduce dropout, this is the hidden to output linear layer we had before, but we're going
to add one more line of code, which is the hidden to output weights are actually equal
to the input to hidden weights. Now, this is not just setting them once this is actually
setting them so that they are a reference to the exact same object in memory, the exact
same tensor in memory. So the weights of the hidden to output layer, will always be identical
to the weights of the input to hidden layer, and this is called weight tying, and the reason
we do this is because conceptually in a language model predicting the next word is about kind
of converting activations into English words, whereas an embedding is about converting English
words to activations, and there's a reasonable hypothesis,
which would be that: �While those are basically exactly the same computation or at least the,
the reverse of it, so why shouldn't they use the same weights.� And it turns out lo and
behold, yes, if you use the same weights then actually it does work a little bit better.
So then here's our forward, which is to do the input to hidden, do the RNN, apply the
dropout, do the detach, and then apply the hidden to output, which is using exactly the
same weights as the input to hidden, and reset�s the same. We haven't created the RNN regularizer
from scratch here, but you can add it as a callback passing in your alpha and your beta.
If you call TextLearner(), instead of Learner(), it will add the ModelReseter and the RNNRegularizer()
for you. So that's what one of the things TextLearner() does. So this code is the same
as this code, and so we can then train the model again, and let�s also add weight decay.
And look at this! We're getting up close to 90% accuracy. So, we've covered a lot in this
lesson, but the amazing thing is that we've just replicated all of the pieces in an AWD-LSTM,
all of the pieces in this state-of-the-art recurrent neural net, which we've showed we
could use in the previous notebook to get, what was until very recently, state-of-the-art
results for text classification, and far more quickly, and with far less compute, and memory
than more modern, than the approaches in the last year or so, which have beaten that benchmark.
So this is a really efficient, really accurate approach, and it's still the, the state-of-the-art
in many, many academic situations, and it's still very widely used in industry, and so
it's pretty cool that we've actually seen how to, to write it from scratch. So the main
thing to mention in the further research is to have a look at the source code for AWD-LSTM
in fastai, and see if you can see how the things in AWD-LSTM map to, you know, what
those lines of code how they map to the concepts that we've seen in this chapter. Rachel, do
we have any questions? So, here we have come to the conclusion of our, what was originally
going to be seven lessons, and turned into eight lessons. I hope that you've got a lot
out of this. Thank you for staying with us. What a lot of folks people, people now do
when they finish, they're at least people who finish previous courses, is they go back
to lesson one, and try and repeat it, but doing a lot less looking at the notebooks,
a lot more doing stuff from scratch yourself, and going deeper into the assignments. So
that's one thing you could do next. Another thing you could do next would be to pick out
a Kaggle competition to enter, or pick a book that you want to read about deep learning,
or a paper, and team up with some friends to do, like a, paper reading group, or a book
reading group. You know, one of the most important things to keep the learning going is to get
together with, with other people on the learning journey. Another great way to do that, of
course, is through the forums. So if you haven't been using the forums much so far no problem,
but now might be a great time to get involved and find some projects that are going on,
that look interesting, and it's fine if you, you know, you don't have to be an expert.
Right? Obviously any of those projects, the people that are already doing it, are going
to know more about it than you do at this point, because they're already doing it, but
if you drop into a thread and say, �Hey I would love to learn more about this how
do I get started?�, or have a look at the wiki posts to find out, and try things out.
You can start getting involved in other people's projects, and help them out. So, yeah... And
of course don't forget about writing. So if you haven't tried writing a blog post yet,
maybe now's a great do that. Pick something that's interesting to you, especially if it's
something in your area of expertise at work, or a hobby, or something like that, or specific
to where you live, maybe you could try and build some kind of text classifier, or text
generator for particular kinds of texts, that are, that you know about. You know, that would
be that would be a super interesting thing to try out, and be sure to share it with the
folks on the forum. So there's a few ideas. So don't let this be the end of your learning
journey, you know, keep, keep going and then come back, and try part two. If it's not out
yet, obviously you have to wait until it is out, if it, but if it is out, you might want
to kind of spend a couple of months, you know, really experimenting with this, before you
move on to part two, to make sure that everything in part one feels pretty, pretty solid to
you. Well, thank you very much everybody for, for your time. We've really enjoyed doing
this course. It's been a tough course for us to teach, because with all this COVID-19
stuff going on at the same time, I'm really glad we've got through it. I'm particularly,
particularly grateful to Sylvain, who has been extraordinary in, in really making so
much of this happen, and particularly since I've been so busy with COVID-19 stuff, around
masks in particular, it's really... A lot thanks to Sylvain, that everything has come
together, and of course to Rachel, who's been here with me on, on every one of these lessons.
Thank you so much. And I'm looking forward to seeing you again in a future course. Thanks
everybody.
this is a um entirely
optional presentation which i'll call a
lesson zero which is all about um
how to fast ai
it's all about how to get the most out
of this course
how to make sure you finish it and how
to make sure you
feel like it's been a productive time
and the reason i'm doing this is because
a lot of people who take the course when
they get to the end of it
they say to me oh it wasn't until i got
to the end of the course that i realized
how i should have done the whole course
and now i'm going to go back and redo
the whole thing over again
and so i'm going to tell you about
what the messages i've heard are about
what people have found most
the best approaches to making the course
work
i'm also going to go through the
actual mechanics of how to get set up
with two systems google colab and aws
ec2 and i talk about why you might use
one versus the other
so a lot of people now as in many
hundreds of thousands
uh have gone through the fast ai
practical deep learning for coders
course
and um many many many of them have gone
on to create successful startups
to write research papers with high
impact factors
to create new products at their
companies
you know it's a pretty well proven
course at this time
but there's also a lot of people that
never finish the course
and so if you're watching this
it's because you've decided you do want
to learn deep learning
so i'm going to talk a bit about like
what's what's it going to take for you
to be one of the people that
that makes this into a great experience
um
when i talk about the course um i'm also
talking about the book so just to be
clear there's there's a book
that silva gujjar and i wrote
which you can either buy from amazon
and people like it happily
or believe it or not you can read the
whole thing for free
um so it's called fastbook it's a fast
book repo
honestly i make basically nothing from
the book so don't feel like you need to
buy it to say thank you or something buy
it if you want the book
if you're happy using notebooks use the
free one it's all good
um so the book was actually written as
jupiter notebooks
and was we wrote something to turn it
into a book book
now the book also by the way actually
looks great on kindle online as well
as paper i know often technical books
don't
this one actually does
and then the course goes through
half of the book okay and so
quite soon we'll do a part two which
we'll go through the other half of the
book plus some other new stuff
um but basically each lesson covers
a chapter or so of the book so if you're
doing this course
you'll be going through the book at
least in the notebooks and you might
want the paper one as well
so here is the main thing that you
should commit to right now which is to
finish the damn
course right and or finish at least half
of the book
um because everybody i think who joins
comes in thinking okay i'm going to do
this i'm going to
do deep learning but if you when i look
at our youtube analytics
a lot of people don't finish okay so
you just need to decide what day are you
going to
watch the course each week what day are
you going to do the assignment what day
like how are you going to structure your
time to finish the course and maybe
you're coming in deciding i i don't want
to finish it which is fine
right um you know and if that's your
intention up front no problem but if
your intention is
to be a really effective deep learning
practitioner
you need to finish the damn course okay
so so put it in your head
that that's your goal talk to your
friends or your spouse and tell them
that's my goal
get that social pressure that you're
gonna finish it
and you're not just going to finish the
course but try to finish
a project right so christine mcclavey is
one of our
fantastic alumni she's now at openai one
of the world's top
research organizations she built
a fantastic system for creating new
music
with deep learning she used to be a
pianist herself
and i remember this discussion i told
her um
focus on making one
project great and polishing it off and
finishing it
and and she did and and that project
has ended up creating music which the
bbc
orchestra played right and amongst other
things helped her
get this extremely exclusive job at open
ai
so this is a clip from a um podcast
with one of our students sanyon and
christine in which
christine is saying this is one of her
key
insights and so i'm going to be giving
you a few key insights
some of which are from me or some of
them are from me via students
but they're all like things i've heard a
bunch of times
and so this is one one example so finish
the course
and finish a project the project doesn't
have to be something no one's ever built
before
maybe it's just like oh i really love
that thing that person built
gosh it would be a real stretch if i
could build it too you know
great or it doesn't have to be world
changing you know so one of
our students built something uh for his
fiancee
which was a cousin recognizer he had i
think 14
cousins and so his fiancee could look at
could take a picture of one of the
cousins and it would tell them
which cousin it was right
in our first course
one of our students built the app for
the silicon valley tv show which did hot
dog or not hot dog
which was actually a huge smash hits
like
millions of downloads it was written
about in the media and it did exactly
one thing
just to tell you whether or not
something was a hot dog
anyway or it could you know um
solve medicine that would be fine too i
mean whatever
um so finishing the course
means being tenacious and um
one of the things i hear a lot is a lot
of the
um approaches people learn as they do
fast ai
around how to learn and how to study are
useful more generally and in fact
this is a quote from our book the number
one thing i see
the difference between successful deep
learning practitioners and
and not is tenacity okay and tenacity is
on the whole something you can choose
now something you can't choose is
whether you find yourself
in the middle of a global pandemic or
you know somebody in your family dies or
you come down with a terrible cold or
whatever like
obstacles happen right and so part of
being tenacious is being
understanding with yourself right and
saying okay
something's happened i can't do what i
hope to do right now but then
getting back to it right so part of
tenacity is
not about ignoring the bumps but keeping
going
after the bumps and maybe that's you
know quite often
i'll have a bump that's like a year long
right but if i've decided to finish
something
you know at the end of that year i'll go
back and finish it
um it's so sometimes that involves me
emailing somebody more than a year after
they've sent me something and saying
okay i'm
ready to reply now and they forgot that
they even sent me an email
um okay
so what i'm going to do now is i'm going
to share with you
a bunch of insights from this book
called metal learning
if you haven't seen it before that's
okay it came out yesterday
and it was written by a guy called
reddick who is one of the
top alumni of this course
and um it's a book well worth
reading because his journey is
extraordinary you know
this is a guy without a degree
who couldn't code just a few years ago
with a job that he found boring
and he set out to to learn
deep learning and um repeatedly failed
to do so
um but um radek is extremely tenacious
and each time he failed to do so he
tried again
and eventually he figured out a way to
do it and the way he did it
was very intensely based on
fast ai both the course and the
philosophy of learning
and uh he is a he is now a kaggle
competition winner
uh he was um the only
uh non-san francisco person at qai which
is one of the world's top
medical ai startups and now uh he works
at a new non-profit that is literally
trying to
translate animal language um
and uh is is so he's kind of a good
example like i always think it's good
idea to have a role model and in the
ai community there's a lot of role
models and so here's somebody who's like
both a role model for like trying
failing trying failing trying failing
and then you know
finding some success and so
um i'm going to show you some things
from from his book
um and a lot of his book is
him him taking stuff i say and kind of
casting it into what he took away from
it some of it's his ideas um
so one of the things we hear again and
again
from unsuccessful deep learning students
is they keep preparing to do deep
learning
and they keep preparing to do projects
so they study linear algebra
they study calculus they study c
plus they study all these different
things
they do a mooc and then another mooc and
then they read a book and then another
book
you know and at what point are they
actually going to start
doing something so the fast ai
philosophy is you start doing something
week one okay so week one
you need to actually train a model
okay which is not to say that
you're not going to learn theory
you will right as needed
in the context of getting stuff done
okay and so
if you do finish it right particularly
if you finish the full
two parts of the course right you'll
have
implemented basically all of fastai's
library just about from scratch you'll
know all about batch normalization
you'll have benchmark various matrix
multiplication approaches
you'll know how to write bare metal gpu
optimized code
you'll understand how to do back
propagation and the calculus of that
from scratch
you'll do all of that okay but it'll all
be
as you go along in the context of like
solving a particular problem or
understanding the next piece of the
puzzle
um so yeah really
just reading books and watching videos
um
is not going to get you there the thing
which is going to get you there
is writing code doing experiments and
training models
some of you might not be that great at
coding
um fine okay that's that's a perfectly
okay place to be
and but you guys are going to find it
the most challenging
because being good at coding is the
thing that lets you zip through quickly
so rather than think oh that's a shame
i'm not that good at coding yet this is
actually an opportunity
because now you have a really fun
project to learn to code in so a lot of
people
have become good coders by doing the
course because as you do the course
you'll learn about a lot of computer
science concepts
like object-oriented programming and
functional programming and
mapping over a list and list
comprehensions and
gpu acceleration and so on and so forth
right
so the thing is though if you're not
if you come across a computer science
concept or a programming idea or a piece
of syntax that you're not that familiar
with
that's a place it's worth pausing for a
moment okay and making sure that you
know
that you do understand how that code
works
because the code coding is the kind of
critical foundational skill
this is a pretty good course for getting
started with basic computer science it's
harvard cs50 course which everybody
at harvard does for computer science um
to get started
and that's all available for free online
um so i would
recommend well and so would radic start
there
um and so these quotes are all from
redex book by the way
and then the other piece so radek talks
about this four-legged table of the
things that are going to help you
do your deep learning experiments more
effectively and efficiently and these
are the ideas like knowing
the basic ideas around code knowing your
tools
so an editor jupiter notebook
knowing stuff like git like how to
save your work and and pull in other
people's work and so forth
and understanding kind of ssh and linux
like how to
access a server and manipulate it and do
stuff with it
so there's this great course called the
missing semester of your cs education
which was actually created i believe by
students at mit
who said oh everybody at mit is assuming
we already know this stuff but a lot of
us
don't right so there's nothing to be
ashamed of if you've never
used git or you've never used ssh you
know or whatever
they're just tools which at some point
in the journey most people just kind of
have to figure out all right so this is
actually a great time to do it
and this is a great course uh to use
to to help you get there and of course
again the main thing is to
to practice these tools so
that's the kind of foundation around
coding and your kind of development
environment
that the next big piece of advice um
which we talk about a lot in the course
and that redick talks about in his book
is um sharing your work communicating
your work
um and writing about your work um
this is something that a lot of people
feel very uncomfortable
like tweeting or blogging
or whatever right um it's like
who the hell am i to start writing about
deep learning
i've just started right well here's the
thing
no one is better placed than you to
write
for like what would you have wanted to
know six months ago right so you now
know more than
you did six months ago and you'll know
more in a week and more in a week or in
a week
and so if you're um got a background in
say the hospitality industry
you know you could probably write
something very interesting for your
colleagues in the whole hospitality
industry about ideas around
around deep learning for example uh or
if you teach in high school
you know you might have ideas that you
can write down about
what high school students might find
interesting or teachers might find
interesting
so you know everybody's got something to
say and the key thing is just to
to write it down because that is going
to help embed
your understanding a lot better and it's
going to start to build up your
portfolio okay
and so we'll talk more about that in a
moment but a lot of people
have found that this message of sharing
their work
has been a critical part of their
journey of
learning and of also building up their
personal brand that has ended up getting
them
a job okay
so what does it mean
to do a fast ai lesson so a fast ai
lesson is basically a chapter of the
book
or one video from the course
or both so what does it mean
to to do one of these lessons um
assuming you're doing the video then it
means okay obviously watching the video
so there's a couple of hours
right and then it means
running the notebook which we'll look at
in a moment
um when you run the notebook
you have the whole book with all of its
code and all of its outputs there you're
playing with it
um you should experiment
right you should you should try things
out
so if you wonder oh why is this
done before that we'll try removing it
try doing it in a different order if
you're wondering you know what would
happen if i did that but this
to this other image try it right the
more you can start to experiment
the more you're feeding your brain with
these kind of like
your own deep learning happening in your
brain input output patterns you try
something what happens you try something
what happens so after that
the next step is to try to reproduce
the notebook from scratch okay
um now and you're going to have to look
things up
obviously but the idea is can you
with a a fresh new notebook can you
um can you go back
and recreate some of those models
retrain them or redo some of that data
processing pipeline
so try to like type it in yourself you
know you can switch back to the answer
as much as you like but you're really
trying to start to
actually you know fill in your own write
write your own code
and then what you really the point you
really want to get to
is repeating some parts of the lesson
with a different data set which you
collect or
download now this
whole process often takes people
a number of times through the course
right so often
the first time through people might just
watch each lecture and try and
kind of run it and you know just get to
the end to get a kind of a general
sense of what's going on so people will
often kind of go through the whole thing
like three times
and then come back and try to go further
and further right
so don't worry if
you can't do all this right away um
certainly in lesson one that's going to
be challenging
just take it as far as you can right and
as you go along
try to push yourself to do more and more
and you can even go back to an earlier
notebook and see if you can understand
more and more of it
so let's take a look at what that looks
like
so here's the course okay
and
here's the lessons which you can watch
and then here are the places you can run
the notebooks so there's two
types of platform for running
the notebooks there are notebook servers
these are things that as soon as you
click into it
the actual environment we environment we
use jupiter notebook
will pop up and you can just start
running it pretty much straight away
so that is obviously the easiest
collab is free gradient
has a free tier and sagemaker
is not free
so we're going to look at colab today
the other option is to use a full linux
server and this is something where
you're going to have to
basically set up linux and install the
python
system and install notebooks and get the
code from github and run the server and
log into it with ssh and do all that
that's obviously a lot more work you
might want to skip it
for now in like lesson one um
but i would recommend at some point
um you go through this path
and the reason why is that in real life
at your workplace or if you do your own
startup or whatever
this is what you'll be doing you will be
interacting with
a linux server using ssh that's running
a gpu
and you'll want to understand how it all
works and once you're using your own
linux server
you'll suddenly learn about all these
productivity enhancing tips and tools
that make your life
easier so
um i'll be showing how to set up
aws ec2 that's the amazon
platform today um you'll find
google cloud looks very very similar
indeed
um
jarvis labs was created by a fast ai
alum
and this is probably at this stage the
best value of the full linux servers
um so that would certainly be also very
much worth
checking out one good thing about aws
so a couple of things aws is currently
the most popular
platform for cloud computing so it's
very likely that whatever company you're
at or end up
at is already using it um uh
they're also pretty generous with
credits for startups and students
so even although it can set you back you
know 60 or 70 cents an hour
you might well find you can get a few
hundred dollars worth of credits through
your
school um or even a few thousand dollars
worth of credits
um through their startup programs and so
forth
um so let's have a look at what
uh colab looks like um
so collab is uh it's it's wonderful
how easy it is to get started you
literally just click
on the chapter so let's do chapter one
and it pops up colab
um you can pay i think it's ten dollars
a month for collab pro
to get like longer sessions and more
likely that you'll get a better
gpu but for most people you'll find the
free version
is totally fine one of the biggest
problems with colab
is that it's not persistent which is to
say when i go to this
notebook it thinks it's never seen me
before
nothing's set up for me the way i want
it but we've set up the notebook so that
the very first cell actually installs
everything you need
so if i click this little run cell
button
here
it will run the cell although what i
will do
is i'm going to pop over to
collab here and let's also
read the steps here and actually it says
here before running anything
you should tell co-lab you're interested
in using a gpu
so if you find that um
when you run a cell in uh in the
from the course and it's going to take
like half an hour or an hour or more
it's very likely you forgot to use gpu
the gpu runs things many hundreds of
times faster
so all you do as it says here is go
runtime change runtime type
and say gpu okay
so now i can run this cell
and this is all python code
except lines that start with an
explanation mark
actually sent to a terminal okay so
paper is something that installs
python software and fastbook contains
all of the python software
necessarily necessary for the course and
so it's going to go away
and set it all up and so this is this
like
mildly annoying bit um
you can then connect
colab to google drive and that's going
to be how you can
save your notebooks and save your work
as you go
okay i'm not going to do that right now
but if you go to this
link that it says and it'll give you a
code and then that'll connect it up
to your google drive
and so at this point now everything from
the for the course is now available
and you can see the whole book
is here okay so here's the book
and um
you can open up sections to read them
okay
you can go to the table of contents
okay and so eventually we'll get to
this cell here which contains all the
code needed to
run a model so if i click run
here is where it goes now um
this is going to um it's amazing how
much this little bit of code is going to
do it's going to download
um tens of thousands of pictures of dogs
and cats
it's going to um uh use a simple rule to
to recognize the dogs from the cats
based on their file names basically the
way that this
this has been set up is that you can
tell from the file name whether it's a
dog or a cat
um it's then going to download something
called a pre-trained model
which is something that already knows
how to recognize uh various types of
images um it's then going to construct
it's going to then going to train that
model to make it particularly good at
recognizing dogs from cats
and then it's going to validate that
model to see how good it is at
recognizing dogs from cats
using a set of pictures that it hasn't
seen before and that's all
happening so so far it's already
downloaded the data set
it's already downloaded the pre-trained
model
and it's now busily going through the
first epoch which is to look at every
picture once
to try to learn how to recognize dogs
from cats
and and that's it the line starting with
a hash are just comments
um because this is also the source of an
actual book
there's a few like slightly weird
comments that you can ignore
they're just things that are used for
setting up references in the book
there's the caption so forth okay so
it's
now testing out i think that first epoch
okay so it's finished an epoch and so
far it's got a one percent error rate
so after 54 seconds it has learnt to
recognize dogs from cats with 99
accuracy and
so yeah we're going to let that finish
off
so that's how we get started with collab
okay and uh
there's nothing else to to set up um
now what you can do is you can
open
notebook and you can open a notebook
from github
and here is the fastbook repository
and you'll see in the fastbook
repository for every notebook there's a
second
copy inside the clean folder with the
same name there's also
so i was just looking at one intro
there's also a clean 01 intro
if i open that up
you'll see that it's got exactly the
same thing
as the last one i was just looking at
but
all the pros is now missing it's just
got headings
and code also all the outputs
are missing so the reason that we have
this
clean version is to help you
with these stages here is our suggestion
is once you've gone
through the lesson and you've run the
notebook
and you feel like okay i think i get it
is you open up this
clean version and before you run
each cell try to think okay why is this
cell here
what's it for what's it going to do
what's the output going to look like
right so once you remove all that
context this is a good test for you to
kind of
get your brain going to think what was
actually going on
so this is a kind of much more active
approach to to reading
and recall and so then once you've done
that
and you've finished going through
this at the bottom one thing that is
kept
is the questionnaire so at the end of
every chapter is a questionnaire
and so then at this point you should now
as much as you can without looking go
through
and try to answer each of those
questions
they all have answers in the notebook
in the book okay so you can you know if
you can't remember you can always look
it up
but um you know if you can't remember
that's a sign to you that like oh i'm
you know did i skip over that bit too
quickly like what what's
happened that i've not remembered and
then try to
remind yourself and then go back and
yeah finish the question there
okay so there's a lot of pieces
to help take this from a passive
i'm just watching a video i'm just
reading a book into a
participatory exercise that you're
a part of okay
so um as soon as you can
we want you to create something that's
yours
and so this is the easiest way to do
that is basically at the end of lesson
one once you're kind of
up and running try to do it with your
own data set
and um if you go to uh forums.fast.ai
which is something that you're going to
want to be deeply familiar with because
this is going to be full of people
just like you other people who want to
learn deep learning
okay and these people are all
asking questions and making comments and
you can see there's like a lot
going on all the time and so you can see
here's the part one course
topic
and you can see there's 1.4 thousand
topics there and each one is going to
have lots and lots of replies
um so this is where
amongst other things you'll find if you
search for it something called share
your work here
which has 2 000 replies and you can see
links to and pictures of lots of
examples
of things that other people have done
after the first week or two of the
course
and so hopefully that might help give
you some inspiration
okay and um it would be great
if you could reply and add you know a
picture or link to what you
build and you'll see you know everybody
is very
positive to each other on the forums in
general and in this topic in particular
nobody's going to go
oh my god i could have done that years
ago right
um people are going to be excited for
you
that you have now joined the ranks of
people that have built their first deep
learning model
and i will be excited for you
so as i said radek this is again from
his book um
expresses in his book a way of not doing
fast ai which i have heard
now probably hundreds of times um i
don't know why this is so common but
many many people do what reddit did
which was basically to learn
all these math things
right so he started with calculus
and then once he got to a certain point
in calculus he found that he had to
start understanding
real analysis and then as he started
understanding
um real analysis he had found he had to
learn set theory
you know and you get the idea right if
you want to learn all of math
that's going to take a while
there's a lot of gatekeeping out there
that
says like oh if you're going to be a
real deep learning practitioner
you have to finish you know a graduate
level course in linear algebra
here's the truth the actual linear
algebra you do
in in basically all deep learning
is matrix multiplication and if you've
forgotten what that is
that is multiplying things together and
then adding them up
okay so what you need to be able to do
is multiply things together
and add them up all right so if you can
do that
you're good to go so yeah don't
get you know you're not going to finish
it
if a you never start it because you keep
preparing or b
you keep thinking oh i wonder exactly
what's happening here and you go all the
way down to the bottom until you've
found yourself
in the midst of set theory right don't
worry
you'll get deeper and deeper over time
but
if you're learning mathematical theory
you're not coding you're not
experimenting you're not practicing
you're not actually building deep
learning models and if you're watching
this course
and your goal is not to build deep
learning models you're in the wrong
course
okay and if your goal is to build deep
learning models
then don't do this
so as radix says here
it's as you train actual models that
you're going to get
feedback right and the feedback that a
lot of people get
is oh my god i can already train useful
models
like a lot of people are surprised at
how early on they can actually
get astonishingly good results
okay so so you know jump in and be open
to surprising yourself that you
can do a bit more than you thought you
can't do everything right away
okay but but start that feedback loop of
figuring out what do you know what can
you do
what can you get working what can't you
get working
so one of the key things that you're
going to need to do if you're going to
finish
all of the course is become
even better developer than you are now
even better coder than you are now
wherever you're up to
and so to do this you need to
read code and write code
the ai source code is designed to be
extremely readable
so you can read that code you can
obviously read the code in the notebooks
um but yeah you want to be spending as
much time as possible reading
and writing code and particularly
reading and writing
deep learning code
all right how do you find out what's
going on in the world of deep learning
and how do you get yourself on the map
of people doing deep learning
um the probably the best answer is
twitter
for those of you whose only knowledge of
twitter is
uh the kardashians and donald trump this
might come as a surprise
but actually to create this slide
i opened twitter and i copied and pasted
the first three tweets that appeared on
my screen
so one of them uh somebody has a
discussion about
costs and impacts of different
approaches to labeling
uh this is a fast ai alum who's a
17 year old phd graduate he's doing well
who
shows how to mix pipe torch and fast ai
and then hillary mason who's a professor
i guess not a professor anymore but now
an industry
um talking about uh organizational
issues in
data science so you know there's a whole
world out there of um
machine learning um on twitter
and they're you know if you want to get
your work noticed that's a great place
to do it because really everybody
everybody's there okay and if you want
me to
highlight your work you know that's
where i can see it and i can
retweet it so yeah twitter is
a really good place to be
if you're just starting with twitter and
you don't know who to follow
go to my twitter go to my likes
and go through my likes and find tweets
that you think
you actually like that tweet too and
then follow the person who
did that tweet okay and pretty quickly
you'll have
a hundred people you're following okay
and then
you'll they'll retweet things and you'll
find other people you like and before
you know it
hopefully you've got a nice big lot of
interesting deep learning stuff to read
every day
at first you'll understand like one
percent of it um which is fine
but you know you're there you're in it
and it'll be all
washing over you and you'll start to
find the people who
write stuff you find engaging and
interesting and you'll also find the
people that
actually you don't and make sure you
unfollow them so that
you don't have your feed have stuff you
don't care about
um so then beyond twitter
you want to start blogging okay and
again blogging is not about
writing what you had for dinner okay
it's about
writing something that you of six months
ago would have found interesting
okay so you know more than you did six
months ago
so write that down um
we have something called fast pages that
makes it ridiculously easy to start a
blog
and so there's no reason for you not to
you know at least create a blog
there we go and one of the nice things
about fast pages is you can even turn
jupyter notebooks into blog posts so
it's great for kind of technical ones
so this is what a fast pages blog looks
like this is a fast pages blog about
fast pages
i had to write fast pages in order to
write the fast pages blog about fast
pages
um but basically and one of the other
nice things it's all into
it's all in github right so it's as as
you're blogging you're learning more
about git
it's all written with markdown which is
something that you're definitely going
to need to know anyway so
as you're blogging you'll be learning
about a lot of the tools you need to
learn about
anyway
um
so one interesting idea for things to
blog about
is um this example from aman arora who
is a aussie fastai alum who
is now working at weights and biases
which is one of the top ai startups in
the world
this is a really interesting kind of
blog post what amanda did was he took
a video that i did at the launch here of
the queensland ai hub
and he wrote down what i said um
and that's an example of
something that you could do if there are
videos out there that you liked
and nobody's turned it into a post
be the first to do so um because there's
all these benefits
um when somebody
sends me something saying i've written
up this talk you gave
i'm very grateful to that person because
now my talk is now available in a second
medium a lot of people prefer to read
rather than listen to a talk
um you know that person's taken the time
to do this they've given taken the time
to have me check
you know their work um and kind of
everybody
ends up winning from this so i've seen
with um aman's
post about my talk it's got attention
from people that
my talk didn't so for example i noticed
on my linkedin feed
the ceo of data61 which is uh
the csiro so the top data science body
in australia
highlighted it and said check out this
post from aurora
right it's like so this is like an
example of
the kind of stuff you can do it's like
try to be
helpful right and at the same time
you're also
learning so there's an example of an
interesting kind of blog post which
very few people are writing and so
there's a huge amount of opportunity
here for you to practice your
your writing
okay now um
what is the difference between machine
learning and other kinds of coding
um ezra dex says in in this chapter of
his book
um the key about machine learning is
that we can
generalize we can uh train a model with
one set of data
and apply it to a different set of data
and still get
good results and everything
just about that we're doing in this
course is
all about creating models that are going
to generalize well
and we're going to be learning about how
you can measure
how well your model generalizes
so answering these questions about
can we trust our model to be correct on
new data that we feed it is
absolutely critical to to every model
that you build whether it be in a kaggle
competition
or a little prototype or a production
model you're creating at work
um one of the most important things here
is creating a good validation set and
this is something that you're
you'll hear about in lesson one of the
course
um but you know i really wanted to
highlight it here as did redick in his
book
um it's it's a really important idea
is you need a good way to measure
whether your model is any good so you
need a data set that really represents
what kind of data is your model likely
to have to deal with in real life
and um my partner rachel wrote this
really great blog post on the fast.ai
blog about this actually interestingly
you know this was um
kind of came out of a um a lesson that i
did at the university of san francisco
and then rachel
turned it into a blog post and rachel's
blog post has ended up much more
influential than my
video ever was you know so this is
actually a good example of what i was
talking about
and she took it a lot further
okay the next key thing that radek
mentions
and i totally agree with is
it's hard to write correct machine
learning code
i always assume that every line of
machine learning code i
write is wrong and i'm normally correct
about that it normally is
wrong because there's lots of ways to be
wrong and
unlike creating a you know
a context management app on the web
whatever
it's much harder to see that you're
wrong you know you can't see that the
name didn't get stored in the database
or you can't see that the title isn't
centered right
often it's wrong that it's going to be
like half a percent less accurate
you know or your image is upside down
but it's kind of maybe you didn't even
look at it or got straight into
sent into the system and you end up with
something that can only recognize
upside-down images
or whatever so
whenever you're doing you know whenever
you're building a project make sure you
start with
a simple baseline right like create the
simplest possible model you can that's
that you know solves the problem so
simply that you
can't have made a mistake so often
that'll be like
just taking the average of the data or
if there's two groups take the average
of each of the two groups
um or you know something that something
really really simple
and then you can gradually build up from
there so another very common beginner
mistake with
projects remember we want you all doing
projects
is somebody in a project group will say
oh i read about this new
bayesian learning thing with these
clusters
and this you know advanced transformers
pipeline
and we could put all that together it's
going to be better than anything before
and they then spend months
creating this complex thing and at the
end
it doesn't work now why doesn't it work
well i don't know it's so big and so
complicated maybe it's a stupid idea
maybe there's a bug in one piece of it
maybe that one piece there shouldn't be
there but it should be somewhere else i
don't know
right that's not how anybody creates
successful machine learning projects
machine successful machine learning
projects are always built
in my experience by creating a simplest
possible solution that gets something
all the way from end to end first
and then very gradually it makes it
incrementally slightly better
okay so keep that in mind right you
might feel a bit silly
when you build that first model that
just takes the average of the data
right but that's how that's how the pros
do it
that's how everybody that actually gets
it to work does it
so often i've had you know silicon
valley startup hotshots come to me and
ask me to like
check out their amazing new startup and
i'll ask them
you know oh you reckon this can separate
um you know
sick people from well people or whatever
have you
taken the average of each of these two
groups and compared that to your model
for example and they'll say oh no and
then they try it and they find out their
model's worse
right so you you need to know whether
your model is actually
doing something useful
for projects one of the things you might
want to do
is join a kaggle competition um
that might be the last thing you see
yourself as doing
is being a cargo competitor but actually
this is one of the best possible
projects you can do because to enter a
kaggle competition
even to come last you have to go through
the entire process
of downloading a data set formatting it
into the right
method ready for a model getting it
through the model
saving the output getting it into the
correct submission format
and submitting it back to kaggle right
so getting a
um a model
actually up onto the kaggle leaderboard
is really going to test out your end to
end understanding right
and once you've done that you can start
to iterate you can start to make it
slightly better
slightly better slightly better
so although in a lot of ways kaggle is
not representative of the real world you
know you don't have to worry about
deployment you don't particularly have
to worry about kind of
inference speed stuff like that and a
lot of ways it is
closer to the real world than you might
expect and then it really does
force you to go through the whole
process and also to
think about engine about kind of
planning your project carefully
so um enter a competition with
your kind of goal that i i want to win
right now obviously on your first one
you're not going to win
but the whole point is it's a
competition so you're going to try to do
your best
right and so to do your best
join a competition that's early
right give yourself plenty of time um
and every single day try to make a small
improvement
um and then you'll find that but you
know if you keep
reading the forums on kaggle and keeping
trying a bit more every day
you'd be amazed at the end of the three
months how much you've learned how much
of the staff that at the start you
thought
this is i have no idea what's going on
and then you'll realize oh suddenly i've
i do know what's going on
and you might find you get in the top 50
which might be better than you expected
so that this is you know highly
recommended at some point
during this course is uh
have a real go at a kaggle competition
so at the end of all of this you might
be looking for a job
now this could mean a number of things a
lot of people just want to bring
some deep learning into their current
job
um and so you know that's if your
organization's already doing some deep
learning that
might be easier than if it's not if it's
not you might just have to start
prototyping some things and try to build
up some kind of you know proof of
concepts internally
or maybe you're going to try and go out
and get and
get you know get a new role as a
researcher or a data scientist or
whatever um
most people are not going to be able to
rely on
their you know stanford phd
to get them there right
most people are going to rely have to
rely on their portfolio
so your portfolio is going to be all the
stuff
you build along the way it's your
footprint on the deep learning community
and that footprint is going to include
you know think
things like your contributions to the
fast ai forums and your tweets
and your stuff on discord
i would say pretty much every one of the
fast ai
alumni that have come to my attention
as being thoughtful and effective
community members
all have very very very good jobs now
um and so like people really really
notice
this footprint right so your your blog
posts your
github projects these are the these are
the things that are going to get you a
job
um they
probably won't get you a job
at a big company a big old company
in a you know kind of
standard established i.t job right
that's going to go through hr and hr got
it like they're not going to understand
any of your github code or know any
about your community impact
they're just going to know about
credentials right and you'll come up
against somebody with a stanford phd and
they'll get the job
right but startups
particularly startups from other people
who've got similar backgrounds of which
there are many
are going to appreciate you or companies
that
don't really have an established ai
group yet
or the startup you built yourself
well certainly appreciate you right um
so it's um
the the more you've got a portfolio and
that you can show that you've really
built stuff
um the better and so start early
um
another reason to
finish
this first course is that it's going to
allow you
to do the second course and if you're
doing this live
um part two we're going to be doing
actually a whole new
part two um towards you know
basically shortly after this is finished
right so if you if you finish this and
do a good job of it
then you could actually be one of the
first
to do part two now
um
we've seen um how
easy colab is to get started we've also
talked about some of the downsides of it
right it's kind of ephemeral
you start from scratch every time you've
got this kind of hacky stuff
of saving notebooks into your google
drive blah blah blah
aws on the other hand is going to give
you and google cloud and java slabs and
so forth are going to give you a real
linux server okay and
it's going to cost you javascript is the
cheapest about 40 cents
aws i think about 60 cents
u.s per hour it's not going to send you
broke but it's you know it's not nothing
but it's a good idea to to try it if you
can and i'm going to show you how to get
started there
and what we might do michael is i'll
do some q a while things are running
maybe so i'm going to head over to aws
ec2
okay
so one of the tricky things about aws is
they've got hundreds of products this is
amazon web services and they all have
names that are totally meaningless
okay so you just have to know ec2 is the
name of the thing that you go to
to rent a computer okay so they don't
call it
amazon computer rental they call it ec2
so the first thing you need to do is you
need to sign up
to aws
and one of the things that uh they get
is a lot of fraud
um so a lot of people try to use their
gpus to mine bitcoin
so you have to ask them uh to give you
permission to use their gpus
and that's called requesting a service
limit increase
um so you'll need to follow the steps
here
to ask them for a limit increase if you
write these exact words
with this exact formatting uh it might
come through a little bit quicker
if you're from a country where there's
a lot of fraud um you might
not even get this permission um
um maybe jarvis labs is going to be
easier i'm not sure javascript even has
the fraud check
so um anyway there's quite a few places
you can you can
try to get a an instance so if aws has a
problem with your quota try somewhere
else but generally speaking
most people should get a response pretty
quickly saying you've now got approved
so for you doing this course if you're
going to try out
aws ec2 i suggest you log in
and request this service limit increase
right away so that you know by the time
you come back tomorrow the next day
it'll be done and so what i'm currently
doing is i'm on course fast ai
and i've got linux servers aws ec2 and
we're following through that project
process
okay now um
to log in to your server you're going to
need to use something called ssh
secure shell so this is something where
on your computer screen
that server's computer screen
effectively is going to appear and the
stuff you
type is actually running on that remote
server
not on your computer um nowadays pretty
much nobody uses usernames and passwords
for ssh instead we use
something called public key cryptography
which is where you basically have a
secret number
which only you know and then there's
another
public number that you tell other people
and basically
there's a really cool math trick which
allows
um people to check whether you have the
secret number without actually anybody
without actually telling them the secret
number
and the process so that's called um so
that's what
an ssh key is so there's this thing
called a public key
and that's the number that your the code
that you're going to give to anybody you
want to be able to log into
and then there's your private key which
you're going to keep for yourself
so you're going to need a terminal
so on windows in the store there's
something called the windows terminal
which microsoft provides for free which
is pretty good
mac has a terminal that comes with it
linux has a terminal that comes with it
so i'm using windows but it'll basically
look the same
for everybody now on
windows you need a ubuntu linux
shell not a normal windows shell
so to do that you need something called
wsl
windows subsystem for linux and that
will give you a
full ubuntu system on your windows
computer it's again it's free
it only takes a couple minutes to set up
so there's a link to how to do it here
so once you've done it um whatever
whether you're on mac
or linux or windows it's going to look
basically the same
right and so you'll create your ssh key
by following the instructions
in the documentation which is basically
you run
ssh keygen and it's just going to go
through
and create
these two files so you just run it it
creates these two files
and so this is the one that we have to
give amazon this is the one that we're
going to keep for ourselves
so following along the documentation
here
it says to click on services
ec2
find key pairs
here okay and then we'll go
here import key pair
and whatever aws
and this is where we're going to find
the
that id rsa pub that we just created
and you can see this here it is right
it's just a big long code and it's fine
you can
all look at this this is public not
secret this is a cool thing right
there's no
passwords and i say import and so now
we have an ssh key
and we can use that to log in
okay so this is just all this is here's
all those steps
so renting a server
in aws speak is called launching an
instance so to launch an instance
we'll scroll back up to the top to
instances
and we will say launch instance
okay and it'll say okay what
kind of thing do you want to run amazon
linux or windows or red hat or whatever
i strongly strongly suggest you use
ubuntu and the latest version which is
currently 20. so i'm just going to say
select
okay and then it'll say okay what kind
of server do you want um
for playing around there's actually one
that you can get for free
now it doesn't do it's pretty it's kind
of slow right but for
learning about ssh and linux and stuff
this is actually a great one to use
um it's no good for deep learning it
doesn't have a gpu
so if i go to g4dn
that's the cheapest kind of good gpus we
can get
and i'll get the smallest one there g4dn
x large and then i'll say next
next um so how big a hard drive do i
want
um i'd normally say about 100 gig
launch and launch
and so now it's going to say okay when
you log into this which key pair are you
going to use
okay so you just select the one that you
just imported
and say yep i i know that i have that
and then launch
and you'll see dale says this has now
been initiated and it's got a code so
this is the thing that i've just
launched if i click on it
here it shows me here's my instance
okay um
so as you if you haven't done much with
servers and linux and ssh and stuff
there's going to be this whole world of
new
stuff for you to learn about but this is
an opportunity it's not a problem so if
you're not familiar with things like ip
addresses
that's cool there's lots of tutorials
around at the moment but for now
just know this is the unique address
like a street address that your new
computer has and so we're going to
connect to it
so this button here will click we'll
copy that address
okay so we can then go to our terminal
and we can type ssh
and paste in the address and then the
only other thing i do need to do
is i need to say provide a username
and aws always uses the username ubuntu
for all of its ubuntu images so you say
ubuntu at and then the ip
and so if i now press enter
we're in okay so now everything i type
here
is actually being typed on that remote
computer
so for example to list the contents of a
directory i type ls
okay so the thing i'm actually typing
into here is bash
a bash shell so bash is something other
of these things you need to be familiar
with
and you can learn about it in that
missing semester mit course i mentioned
you know it takes takes a few weeks to
get somewhat comfortable with bash
it's a very different feel to using a
gui if you're more familiar with
explorer or finder or whatever but
you'll find it you'll be much more
productive soon enough because you can
replicate things quickly you can script
things you can
copy and paste things and so forth
anyway so here's my
here's my computer
it's going to sit here running until you
tell it not to
even if you turn your computer off your
server is still running and that means
you're still paying for it
okay so one of the things i guarantee
you're going to learn the hard way by
wasting money
is that you're going to forget to turn
it off okay so to turn it off
you're just going to go stop instance
okay so you make sure you
you do that
all right let's see how we're going here
so we've launched our instance
and we ssh into it
okay so um
keeping a linux server up to date and
running used to be kind of annoying
but luckily
i've created something called fast setup
for you which makes it easy
and all you need to do is copy this
and paste it into your terminal and this
is one of the really cool things about
linux and using bash is like in windows
or
with mac finder you'd have pages and
pages of click
this and drag that and scroll here but
i've just scripted the whole thing so
i'm just going to go ahead and paste it
over here
and it's off okay now what this is going
to do
is it's going to fully set up this linux
server it's going to make it
automatically update with the latest
software it's going to
configure it all correctly
and so forth and it's going to ask a
minimum number of questions so i'm just
going to show you the questions it's
going to ask you it's going to ask for a
host name
so a host name is just a more convenient
way to access a server
and so you can basically write anything
you like as long as it's got at least
two dots in it
so i'm going to call this
dot coursetest.fast.ai for example okay
and then it asks for an email address
now the email address
um is basically just goes well it's
where it's going to send
kind of error locks and stuff too so
maybe we'll say
info at fast.ai
okay do you want to set a password
probably do so hit enter for yes
so i'm going to put in a password
it asks you to type it again
okay reboot automatically when required
i'll say yes
and that's it okay so that's all the
information
that it needed so behind the scenes
what's actually happening here
is it's grabbed the latest
get repo from fast setup and it's
running this thing called ubuntu initial
and you know this is something you can
check out if you're interested it's
basically
uh 125 lines of bash script
which is going to set up your firewall
for you set up
ssh security for you set up your swap
file for you set up your ssh
configuration for you
install all the software you need for
you
set up your logging and upgrades for you
set up your password and host name for
you okay
so it's going to do all that
and you know this is the kind of thing
that if you you know from time to time
you can just
might think oh i'm interested in how x
works
and since everything is open source you
can just go in and see how x works and
at first
none of this might make any sense and so
you go oh all right let's pick something
and learn about it
enable firewall ufw oh what's ufw
copy paste ufw
probably not united farm workers
uncomplicated firewall did jeremy
mentioned firewall okay what the hell's
a firewall and you know you could start
reading right um and then you could be
like oh
maybe firewall tutorial
often adding tutorial can be helpful
okay so you know you can start to just
jump in here and there okay don't get
too distracted we want to spend as much
time as possible
training models but this is how we learn
about our tools
okay so this is now going and
downloading the latest version of all
the software that it's going to need
from linux so it may be good time for
questions if we have any michael
what's your current opinion regarding
swift and julia as replacements for
python
so swift is basically out now so
google has basically archived the swift
for tensorflow project
so you can safely ignore that um
yeah julia's julia's interesting
you know i think it's a lovely language
nothing has the ecosystem that python
does
so
you know if you use julia you're going
to have to figure out a lot more stuff
on your own and you'll find a lot more
hard edges
but i do think at some point python is
going to have to be replaced and
julia seems like one of if not the most
likely thing to replace it
or maybe it won't be replaced by julia
maybe it will be replaced by something
else that's kind of python
like jax which actually takes python and
compiles it using something called xla
into a
much faster thing than python otherwise
would be
okay do you think that deep learning or
more traditional ml or stats
approaches are more useful for
traditional industry applications right
now
um so before i answer that question i'm
going to press
y which is going to reboot our computer
now that it's all
updated and obviously when we reboot
that that computer running at the aws
data center
um it closes the connection because it's
busy rebooting
okay so we'll give it a couple of
minutes um
there's not a single good answer to that
question um
um and you don't really need to answer
that question because basically any time
you want to
[Music]
try any kind of machine learning model
on a problem you should try a few
different
algorithms and switching from a random
forest to a gradient boosting machine to
logistic regression to deep learning is
you know an extra half hour so um
you should just try a few different
approaches
i find personally for me deep learning
is
increasingly turning out to be
the easiest thing to get started with
and gives me the best results
um for most projects i seem to do
nowadays um but you know have a look at
like cargo competitions from time to
time there are still things where
gradient boosting machines work better
or very often
people use both and ensemble them but
yeah it's not a question that you
actually
need to answer you want to get to a
point where
it just takes you a few minutes to try
another algorithm out
and so you don't need to be wedded to
one or the other
so i'm just going to see if i can i
don't know how long it's going to take
to reboot so i'm just going to i just
pressed up arrow to get back my last ssh
command and i'll press
enter we'll see if we're back we're back
okay so this is finished rebooting
oh actually this time it says to do
something slightly different which is to
add
this minus l here this is the thing
that's going to let us connect
to jupyter notebook so i'm going to type
excerpt
to exit from the server and this time
i'm going to add the
extra bit of the command
there we go okay and all right so the
next thing is we're going to install
something called mini conda
mini conda is a very nice
distribution of python the programming
language
a lot of people have bad experiences
of their computers getting really
confused with python packages and things
conflicting and all kinds of stuff like
that
that's because pretty much all the major
operating systems now
come with a version of python that is
used by your computer for
you know important operating system
tasks you should
not be using that python to train your
machine learning models
leave that python alone right you should
always install mini conda and which is
going to give you your own version of
python which has nothing to do with your
operating
system as you can play around with as
you like
it's really easy just you can delete the
whole folder and
create it again in like three minutes
you can create new environments which is
like little testing grounds you can try
different things
so this is yeah very strong
recommendation is
to make sure that you install even if
you're just playing around on windows or
a mac
not on a server install mini conda it's
cross-platform you can use it everywhere
and use
that python okay so
minicon is now installed so we now have
our own python setup
so the last setup step is we have to
install
drivers for the gpu and ubuntu actually
comes with something that figures out
for you what the best drivers are
for your device so this is just what
this set here is
and so i'm going to look down look here
it says recommended
okay so here's the driver i want okay
but what i actually recommend is you use
that but also the one add the dash
server to the end
that's going to make like not install
the stuff for playing computer games and
whatever
okay so let's go ahead and
run these lines of code this is the bit
here see this is 460.
depending on your graphics card when you
run this you might have some different
number
okay but since i wrote this today it's
still 460.
so we'll go ahead and do that and this
is going to
help go ahead and install this
oh um sudo sudo
is a special thing you can add to the
front of a command
that runs it as an administrator okay
so some things you know
by default commands you run basically
can't break your system
right um where else things like
installing new software
um you have to tell it to run it as an
administrator so and when you do that
it'll ask you for your password and this
is the password that you put in just
just a moment ago in the setup
there we go okay
is there a section of the course that
people skip over too quickly
um yes
um part two
yeah not enough people do part two
and the difference between part one and
part two is
the difference between um
being a pretty handy practitioner you
know who can
who can do some pretty good work as long
as it's in reasonably well established
kinds of areas
um and versus being somebody who
understands how everything's put
together
you could you know if you're told to
create a deep learning model
on in a domain that's like there are no
published models you'll be able to
create one
um if you understand how to create
models which combine
multiple different data types um
you know you're it it's a it's
yeah it's it's a really great thing to
to finish and yeah not enough people
realize how much is is there
and just the later lessons in in general
you know it can like after you've done
three lessons
you you are pretty handy and you'll feel
pretty handy
right but it's pretty easy to stop there
because it feels like okay i get it you
know i can train a model
i get what's going on um
and to be fair it does very dramatically
kind of scale up in terms of intensity
after that because in lesson four
you'll have to write your own optimizer
from scratch and you'll be getting into
the calculus and stuff
um but you know it
it it is a big difference in terms of
what what you can do and what you
understand
so i think in general you know not
enough people are
getting deeper into the lessons okay
so um this is now finished installing
the nvidia drivers
normally at this point people say to
reboot but there's actually a magic
thing you can do which means you don't
have to reboot and
the nvidia provides something called
nvidia smi
which will tell you about your installed
gpus
and so if you run it and it pops up
anything at all other than an error it
means that you are
you successfully have your gpus
installed so in this case
we have a tesla t4
um it's currently 36 centigrade in there
and the most important thing to know
about is that it has 15 gigabytes of
memory
of which we're using nothing at all and
there are
no processors currently running on the
gpu so if you're finding something's
going very slowly
and you're wondering maybe it's not
using the gpu you can always run
nvidia smi and if it says no running
process is found
you're not using the gpu okay
okay so um one more setup step
which is we have to install all of the
software all of the python libraries
needed so
pytorch fast ai um
jupiter notebook and so forth and so
i've created a package which has that
whole lot it's called fastbook
if you're if you've used anaconda or
mini conda before
you might be surprised it says mamba
rather than conda
you should definitely use mamba and not
conda it's way way way faster
so anytime you see something saying
conda install
you should instead type member install
it's way faster
okay so off it goes member is now going
to install
all of the we've got all this python
software getting installed
for us
pytorch is well over a gigabyte
so this is going to take a few minutes
just because it has to download that
that whole thing and yeah that can take
a while
so while this is going do we got any
more questions michael
do you recommend any software for
experiment tracking
so the most popular experiment tracking
software would be
tensorboard and weights and biases
experiment tracking software is stuff
which will basically
um you can use a fasta callback and you
basically will say train
whilst tracking with tensorboard or
train whilst tracking with weights and
biases
and what it'll do is it will
kind of create a little database showing
you
all the training results from all the
different experiments you've run
um and create some little graphs of them
and so forth
um personally
i don't use any experiment tracking
software and the reason i don't
is i found that many many many people
just about everybody i know who uses
them finds it incredibly distracting
so the trick to training models is don't
watch them train so if you've done an ec
programming it's like don't watch it
compile
right go and do something else
preferably set up your next experiment
okay experiment tracking software just
makes it so
tempting to look at all the pretty
graphs in my opinion
um so i would suggest get it running
leave come back when it's done and
there should be a bit of reason you are
running that experiment so
check whatever that reason was right
um having said that
if you're really sure you need the
services of experiment tracking software
for what you're doing
um and there are some things that
genuinely need it uh then i think
weights and bias is
is the best at the moment
i think it's really great and
furthermore they've hired lots of fast
ai alumni and they're super nice people
so i definitely recommend that
the um so that's all the installation
so the last step is just to grab um the
book
the notebooks um and so you use
something called getclone
to grab a repository of code and this is
going to grab the fastbook repository
paste so you can see it's saying cloning
this repository
and so you'll now find that there is a
fastbook
directory so you can cd into it
and there is our book okay um okay
so i think something um on anaconda is
going slowly so we're not going to wait
for it to download
but so i want to show the very last step
but the very last step is to run jupyter
notebook
and then you'll be able to click on the
url that pops up and it'll bring up
something that basically looks
just like we saw in collab but the nice
thing is everything you
save like everything you do
will be remembered so all of your
experiments are going to be there
the data sets you download are still
there so on and so forth
so that's that's that
so when you're done it'll remind you
here
to as i mentioned before stop your
instance
so you can either choose stop in this
menu
or you can choose
stop here or personally what i quite
like to do
is to run sudo remember sudo is this
thing that runs as an administrator
shutdown halt now
and so that shuts it down from here
without having to go into the
aws gui
and there we go okay so
if we look back at the um
ec2 here in a moment this will switch
from running state to stopped state
okay so i think that's
everything michael is there anything
else we need to cover
okay okay great um
all right well thank you everybody for
listening in to um
lesson zero and um i look forward to
hearing how you go with lesson one and
seeing
your
projects that you create and don't
forget to get involved
in the forums if you do get stuck with
something the first thing to do is to
search the forums
because out of the hundreds of thousands
of people that have done this before
somebody's probably got stuck in the
same way before so hopefully they can
answer your question
otherwise feel free to ask your own
questions and hopefully somebody
will answer for you all right thanks
everybody bye