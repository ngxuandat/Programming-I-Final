Good afternoon everyone and Welcome to MIT  6.S191 -- Introduction to Deep Learning. My  
name is Alexander Amini and I'm so excited to  be your instructor this year along with Ava  
Soleimany in this new virtual format. 6.S191 is  a two-week bootcamp on everything deep learning  
and we'll cover a ton of material in  only two weeks so I think it's really  
important for us to dive right in with  these lectures but before we do that I  
do want to motivate exactly why I think  this is such an awesome field to study  
and when we taught this class last year I decided  to try introducing the class very differently and  
instead of me telling the class how great  6.S191 is I wanted to let someone else do  
that instead so actually I want to start this year  by showing you how we introduced 6s191 last year.
[Obama] Hi everybody and welcome to MIT  6.S191 the official introductory course on  
deep learning taught here at MIT. Deep learning  is revolutionizing so many things from robotics  
to medicine and everything in between you'll  learn the fundamentals of this field and how you  
can build some of these incredible algorithms.  In fact, this entire speech and video  
are not real and were created using deep learning  and artificial intelligence and in this class  
you'll learn how. It has been an honor to speak  with you today and I hope you enjoy the course.
So in case you couldn't tell that was  actually not a real video or audio  
and the audio you actually heard was purposely  degraded a bit more to even make it more obvious  
that this was not real and avoid some potential  misuse even with the purposely degraded audio  
that intro went somewhat viral last year  after the course and we got some really  
great and interesting feedback and to be  honest after last year and when we did  
when we did this i thought it was going to  be really hard for us to top it this year  
but actually i was wrong because the one thing  i love about this field is that it's moving so  
incredibly fast that even within the past year  the state of the art has significantly advanced  
and the video you saw that we used last year used  deep learning but it was not a particularly easy  
video to create it required a full video of obama  speaking and it used this to intelligently stitch  
together parts of the scene to make it look and  appear like he was mouthing the words that i said  
and to see the behind the scenes here now  you can see the same video with my voice.
[Alexander] Hi everybody and welcome to mit 6.S191  
the official introductory course on  deep learning taught here at MIT.
Now it's actually possible to use just a single  static image not the full video to achieve  
the exact same thing and now you can actually  see eight more examples of obama now just  
created using just a single static image no more  full dynamic videos but we can achieve the same  
incredible realism and result using deep learning  now of course there's nothing restricting us to  
one person this method generalizes to different  faces and there's nothing restricting us even  
to humans anymore or individuals that  the algorithm has ever seen before
[Alexander] Hi everybody and welcome to mit  
6.S191 the official introductory course  on deep learning taught here at MIT.
The ability to generate these types of  dynamic moving videos from only a single  
image is remarkable to me and it's a testament  to the true power of deep learning in this class  
you're going to actually not only learn about  the technical basis of this technology but also  
some of the very important and very important  ethical and societal implications of this work  
as well now I hope this was a really great way  to get you excited about this course and 6.S191  
and with that let's get started we can actually  start by taking a step back and asking ourselves  
what is deep learning defining deep learning  in the context of intelligence intelligence is  
actually the ability to process information such  that it can be used to inform a future decision  
now the field of artificial intelligence or AI  is a science that actually focuses on building  
algorithms to do exactly this to build algorithms  to process information such that they can inform  
future predictions now machine learning you  can think of this as just a subset of AI  
that actually focuses on teaching an algorithm to  learn from experiences without being explicitly  
programmed now deep learning takes this idea even  further and it's a subset of machine learning that  
focuses on using neural networks to automatically  extract useful patterns in raw data and then using  
these patterns or features to learn to perform  that task and that's exactly what this class is  
about this class is about teaching algorithms  how to learn a task directly from raw data  
and we want to provide you with a solid  foundation both technically and practically  
for you to understand under the hood how these  algorithms are built and how they can learn
so this course is split between technical  lectures as well as project software labs we'll  
cover the foundation starting today with neural  networks which are really the building blocks  
of everything that we'll see in this  course and this year we also have two  
brand new really exciting hot topic lectures  focusing on uncertainty and probabilistic deep  
learning as well as algorithmic bias and fairness  finally we'll conclude with some really exciting  
guest lectures and student project presentations  as part of a final project competition that all of  
you will be eligible to win some really exciting  prizes now a bit of logistics before we dive into  
the technical side of the lecture for those of you  taking this course for credit you will have two  
options to fulfill your credit requirement the  first option will be to actually work in teams  
of or teams of up to four or individually  to develop a cool new deep learning idea  
now doing so will make you eligible to win some of  the prizes that you can see on the right hand side  
and we realize that in the context of this  class which is only two weeks that's an  
extremely short amount of time to come up  with an impressive project or research idea  
so we're not going to be judging you on the  novelty of that idea but rather we're not  
going to be judging you on the results of that  idea but rather the novelty of the idea your  
thinking process and how you how impactful this  idea can be but not on the results themselves  
on the last day of class you will actually  give a three-minute presentation to a group  
of judges who will then award the winners and the  prizes now again three minutes is extremely short  
to actually present your ideas and present your  project but i do believe that there's an art to  
presenting and conveying your ideas concisely  and clearly in such a short amount of time  
so we will be holding you strictly to that  to that strict deadline the second option  
to fulfill your grade requirement is to write a  one-page review on a deep learning paper here the  
grade is based more on the clarity of the writing  and the technical communication of the main ideas  
this will be due on thursday the last thursday  of the class and you can pick whatever deep  
learning paper you would like if you would  like some pointers we have provided some  
guide papers that can help you get started if  you would just like to use one of those for  
your review in addition to the final project  prizes we'll also be awarding this year three  
lab prizes one associated to each of the software  labs that students will complete again completion  
of the software labs is not required for grade of  this course but it will make you eligible for some  
of these cool prices so please we encourage  everyone to compete for these prizes and  
get the opportunity to win them all please  post the piazza if you have any questions  
visit the course website for announcements  and digital recordings of the lectures etc  
and please email us if you have any  questions also there are software labs  
and office hours right after each of these  technical lectures held in gather town so  
please drop by in gather town to ask any questions  about the software labs specifically on those or  
more generally about past software labs or  about the lecture that occurred that day
now this team all this course has a incredible  group of TA's and teaching assistants that  
you can reach out to at any time  in case you have any issues or  
questions about the material that you're learning
and finally we want to give a  huge thanks to all of our sponsors  
who without their help this class would  not be possible this is the fourth year  
that we're teaching this class and each year it  just keeps getting bigger and bigger and bigger  
and we really give a huge shout out to our  sponsors for helping us make this happen each year  
and especially this year in light of the virtual  format so now let's start with the fun stuff  
let's start by asking ourselves a question about  why why do we all care about deep learning and  
specifically why do we care right now understand  that it's important to actually understand first  
why is deep learning or how is deep learning  different from traditional machine learning  
now traditionally machine learning algorithms  define a set of features in their data usually  
these are features that are handcrafted or hand  engineered and as a result they tend to be pretty  
brittle in practice when they're deployed the  key idea of deep learning is to learn these  
features directly from data in a hierarchical  manner that is can we learn if we want to learn  
how to detect a face for example can we learn  to first start by detecting edges in the image  
composing these edges together to detect  mid-level features such as a eye or a nose  
or a mouth and then going deeper and composing  these features into structural facial features  
so that we can recognize this face this is this  hierarchical way of thinking is really core  
to deep learning as core to everything  that we're going to learn in this class  
actually the fundamental building blocks though  of deep learning and neural networks have actually  
existed for decades so one interesting thing to  consider is why are we studying this now now is an  
incredibly amazing time to study these algorithms  and for one reason is because data has become  
much more pervasive these models are extremely  hungry for data and at the moment we're living  
in an era where we have more data than ever  before secondly these algorithms are massively  
parallelizable so they can benefit tremendously  from modern gpu hardware that simply did not exist  
when these algorithms were developed and finally  due to open source toolboxes like tensorflow  
building and deploying these models  has become extremely streamlined
so let's start actually with  the fundamental building block  
of deep learning and of every neural network that  is just a single neuron also known as a perceptron  
so we're going to walk through exactly what is  a perceptron how it's defined and we're going to  
build our way up to deeper neural networks all the  way from there so let's start we're really at the  
basic building block the idea of a perceptron  or a single neuron is actually very simple so  
i think it's really important for all of you  to understand this at its core let's start by  
actually talking about the forward propagation  of information through this single neuron we  
can define a set of inputs x i through x m  which you can see on the left hand side and  
each of these inputs or each of these numbers  are multiplied by their corresponding weight  
and then added together we take this single number  the result of that addition and pass it through  
what's called a nonlinear activation function  to produce our final output y we can actually  
actually this is not entirely correct because one  thing i forgot to mention is that we also have  
what's called a bias term in here which allows  you to shift your activation function left or  
right now on the right hand side of this diagram  you can actually see this concept illustrated or  
written out mathematically as a single equation  you can actually rewrite this in terms of linear  
algebra matrix multiplications and dot products  to to represent this a bit more concisely  
so let's do that let's now do that with x  capital x which is a vector of our inputs  
x1 through xm and capital w which is a vector  of our weights w1 through wm so each of these  
are vectors of length m and the output is very  simply obtained by taking their dot product  
adding a bias which in this case is  w0 and then applying a non-linearity g
one thing is that i haven't i've been mentioning  it a couple of times this non-linearity g what  
exactly is it because i've mentioned it now a  couple of times well it is a non-linear function  
one common example of this nonlinear activation  function is what is known as the sigmoid function  
defined here on the right in fact there  are many types of nonlinear functions  
you can see three more examples here including the  sigmoid function and throughout this presentation  
you'll actually see these tensorflow code  blocks which will actually illustrate  
uh how we can take some of the topics that we're  learning in this class and actually practically  
use them using the tensorflow software library now  the sigmoid activation function which i presented  
on the previous slide is very popular since it's  a function that gives outputs it takes as input  
any real number any activation value and it  outputs a number always between 0 and 1. so  
this makes it really really suitable for problems  and probability because probabilities also have to  
be between 0 and 1 so this makes them very well  suited for those types of problems in modern deep  
neural networks the relu activation function which  you can see on the right is also extremely popular  
because of its simplicity in this case it's a  piecewise linear function it is zero before when  
it's uh in the negative regime and it is strictly  the identity function in the positive regime
but one really important question that i  hope that you're asking yourselves right now  
is why do we even need activation functions  and i think actually throughout this course  
i do want to say that no matter what  i say in the course i hope that always  
you're questioning why this is a necessary step  and why do we need each of these steps because  
often these are the questions that can lead to  really amazing research breakthroughs so why do  
we need activation functions now the point of  an activation function is to actually introduce  
non-linearities into our network because these are  non-linear functions and it allows us to actually  
deal with non-linear data this is extremely  important in real life especially because in  
the real world data is almost always non-linear  imagine i told you to separate here the green  
points from the red points but all you could  use is a single straight line you might think  
this is easy with multiple lines or curved lines  but you can only use a single straight line and  
that's what using a neural network with a linear  activation function would be like that makes the  
problem really hard because no matter how deep the  neural network is you'll only be able to produce  
a single line decision boundary and you're  only able to separate your space with one line  
now using non-linear activation functions  allows your neural network to approximate  
arbitrarily complex functions and that's what  makes neural networks extraordinarily powerful
let's understand this with a simple example so  that we can build up our intuition even further  
imagine i give you this trained network now with  weights on the left hand side 3 and negative 2.  
this network only has two inputs x1 and x2  if we want to get the output of it we simply  
do the same story as i said before first take  a dot product of our inputs with our weights  
add the bias and apply a non-linearity  but let's take a look at what's inside  
of that non-linearity it's simply a  weighted combination of our inputs  
and the in the form of a two-dimensional line  because in this case we only have two inputs  
so if we want to compute this output it's the  same story as before we take a dot product of x  
and w we add our bias and apply our non-linearity  what about what's inside of this nonlinearity g  
well this is just a 2d line in fact since it's  just a two dimensional line we can even plot it  
in two-dimensional space this is called  the feature space the input space  
in this case the feature space and the input  space are equal because we only have one neuron  
so in this plot let me describe what you're seeing  so on the two axes you're seeing our two inputs so  
on one axis is x1 one of the inputs on the other  axis is x2 our other input and we can plot the  
line here our decision boundary of this trained  neural network that i gave you as a line in this  
space now this line corresponds to actually all  of the decisions that this neural network can make  
because if i give you a new data point for example  here i'm giving you negative 1 2. this point lies  
somewhere in this space specifically at x one  equal to negative one and x two equal to two  
that's just a point in the space i want you to  compute its weighted combination and i i can  
actually follow the perceptron equation to get  the answer so here we can see that if we plug it  
into the perceptron equation we get 1 plus minus  3 minus 4 and the result would be minus 6. we  
plug that into our nonlinear activation function  g and we get a final output of 0.002 now in fact  
remember that the sigmoid function actually  divides the space into two parts of either  
because it outputs everything between zero and  one it's dividing it between a point at 0.5 and  
greater than 0.5 and less than 0.5 when the input  is less than 0 and greater than 0.5 that's when  
the input is positive we can illustrate the space  actually but this feature space when we're dealing  
with a small dimensional data like in this case  we only have two dimensions but soon we'll start  
to talk about problems where we have thousands or  millions or in some cases even billions of inpu  
of uh weights in our neural network and then  drawing these types of plots becomes extremely  
challenging and not really possible anymore but  at least when we're in this regime of small number  
of inputs and small number of weights we can make  these plots to really understand the entire space  
and for any new input that we obtain for example  an input right here we can see exactly that this  
point is going to be having an activation function  less than zero and its output will be less than  
0.5 the magnitude of that actually is computed  by plugging it into the perceptron equation so  
we can't avoid that but we can immediately get an  answer on the decision boundary depending on which  
side of this hyperplane that we lie on when we  plug it in so now that we have an idea of how to  
build a perceptron let's start by building neural  networks and seeing how they all come together
so let's revisit that diagram of the perceptron  that i showed you before if there's only a few  
things that you get from this class i really  want everyone to take away how a perceptron works  
and there's three steps remember them always  the dot product you take a dot product of your  
inputs and your weights you add a bias and you  apply your non-linearity there's three steps  
let's simplify this diagram a little bit let's  clean up some of the arrows and remove the bias  
and we can actually see now that every line here  has its own associated weight to it and i'll  
remove the bias term like i said for simplicity  note that z here is the result of that dot  
product plus bias before we apply the activation  function though g the final output though is is  
simply y which is equal to the activation  function of z which is our activation value
now if we want to define a multi-output neural  network we can simply add another perceptron to  
this picture so instead of having one perceptron  now we have two perceptrons and two outputs each  
one is a normal perceptron exactly like we saw  before taking its inputs from each of the x i x  
ones through x m's taking the dot product adding a  bias and that's it now we have two outputs each of  
those perceptrons though will have a different set  of weights remember that we'll get back to that
if we want it so actually one thing to keep  in mind here is because all the inputs are  
densely connected every input has a connection to  the weights of every perceptron these are often  
called dense layers or sometimes fully connected  layers now we're through this class you're going  
to get a lot of experience actually coding up  and practically creating some of these algorithms  
using a software toolbox called tensorflow  so now that we have the understanding of how  
a single perceptron works and how a dense  layer works this is a stack of perceptrons  
let's try and see how we can actually build up  a dense layer like this all the way from scratch  
to do that we can actually start by initializing  the two components of our dense layer  
which are the weights and the biases now that we  have these two parameters of our neural network  
of our dense layer we can actually define the  forward propagation of information just like  
we saw it and learn about already that forward  propagation of information is simply the dot  
product or the matrix multiplication of our  inputs with our weights at a bias that gives  
us our activation function here and then we  apply this non-linearity to compute the output
now tensorflow has actually implemented  this dense layer for us so we don't need  
to do that from scratch instead we  can just call it like shown here so  
to create a dense layer with two outputs  we can specify this units equal to two
now let's take a look at what's called a single  layered neural network this is one we have a  
single hidden layer between our inputs and our  outputs this layer is called the hidden layer  
because unlike an input layer and an output layer  the states of this hidden layer are typically  
unobserved they're hidden to some extent they're  not strictly enforced either and since we have  
this transformation now from the input layer to  the hidden layer and from the hidden layer to the  
output layer each of these layers are going to  have their own specified weight matrices we'll  
call w1 the weight matrices for the first layer  and w2 the weight matrix for the second layer
if we take a zoomed in look at one of the neurons  in this hidden layer let's take for example z2 for  
example this is the exact same perceptron that we  saw before we can compute its output again using  
the exact same story taking all of its inputs x1  through xm applying a dot product with the weights  
adding a bias and that gives us z2 if we  look at a different neuron let's suppose z3  
we'll get a different value here because the  weights leading to z3 are probably different than  
those leading to z2 now this picture looks a bit  messy so let's try and clean things up a bit more
from now on i'll just use this symbol here to  denote what we call this dense layer or fully  
connected layers and here you can actually  see an example of how we can create this  
exact neural network again using tensorflow  with the predefined dense layer notation  
here we're creating a sequential model where  we can stack layers on top of each other  
first layer with n neurons and the second  layer with two neurons the output layer
and if we want to create a deep neural network  all we have to do is keep stacking these layers  
to create more and more hierarchical models  ones where the final output is computed  
by going deeper and deeper into the network and  to implement this in tensorflow again it's very  
similar as we saw before again using the tf keras  sequential call we can stack each of these dense  
layers on top of each other each one specified  by the number of neurons in that dense layer  
n1 and 2 but with the last output layer fixed to  two outputs if that's how many outputs we have
okay so that's awesome now we have an idea of not  only how to build up a neural network directly  
from a perceptron but how to compose them together  to form complex deep neural networks let's take a  
look at how we can actually apply them to a very  real problem that i believe all of you should  
care very deeply about here's a problem that we  want to build an ai system to learn to answer  
will i pass this class and we can start with  a simple two feature model one feature let's  
say is the number of lectures that you attend as  part of this class and the second feature is the  
number of hours that you spend working on your  final project you do have some training data  
from all of the past participants of success 191  and we can plot this data on this feature space  
like this the green points here actually indicate  students so each point is one student that has  
passed the class and the red points  are students that have failed the pass  
failed the class you can see their where they are  in this feature space depends on the actual number  
of hours that they attended the lecture the number  of lectures they attended and the number of hours  
they spent on the final project and then there's  you you spent you have attended four lectures  
and you have spent five hours on your  final project and you want to understand  
uh will you or how can you build a neural network  given everyone else in this class will you pass  
or fail uh this class based on the training data  that you see so let's do it we have now all of the  
uh to do this now so let's build a neural  network with two inputs x1 and x2 with x1 being  
the number of lectures that we attend x2 is the  number of hours you spend on your final project  
we'll have one hidden layer with three units and  we'll feed those into a final probability output  
by passing this class and we can see that  the probability that we pass is 0.1 or 10  
that's not great but the reason is because  that this model uh was never actually trained  
it's basically just a a baby it's never seen any  data even though you have seen the data it hasn't  
seen any data and more importantly you haven't  told the model how to interpret this data it needs  
to learn about this problem first it knows nothing  about this class or final projects or any of that  
so one of the most important things to do  this is actually you have to tell the model  
when it's able when it is making bad predictions  in order for it to be able to correct itself  
now the loss of a neural network actually defines  exactly this it defines how wrong a prediction was  
so it takes as input the predicted outputs  and the ground truth outputs now if those  
two things are very far apart from each other  then the loss will be very large on the other  
hand the closer these two things are from each  other the smaller the loss and the more accurate  
the loss the model will be so we always  want to minimize the loss we want to incur  
that we want to predict something that's  as close as possible to the ground truth
now let's assume we have not just the data  from one student but as we have in this case  
the data from many students we now care about  not just how the model did on predicting just  
one prediction but how it did on average  across all of these students this is what  
we call the empirical loss and it's  simply just the mean or the average  
of every loss from each individual  example or each individual student
when training a neural network we  want to find a network that minimizes  
the empirical loss between our  predictions and the true outputs
now if we look at the problem of binary  classification where the neural network  
like we want to do in this case is supposed to  answer either yes or no one or zero we can use  
what is called a soft max cross entropy loss now  the soft max cross entropy loss is actually built  
is actually written out here and it's  defined by actually what's called the  
cross entropy between two probability  distributions it measures how far apart  
the ground truth probability distribution is  from the predicted probability distribution  
let's suppose instead of predicting binary  outputs will i pass this class or will i not  
pass this class instead you want to predict the  final grade as a real number not a probability  
or as a percentage we want the the grade that you  will get in this class now in this case because  
the type of the output is different we also need  to use a different loss here because our outputs  
are no longer 0 1 but they can be any real number  they're just the grade that you're going to get on  
on the final class so for example here since this  is a continuous variable the grade we want to use  
what's called the mean squared error this measures  just the the squared error the squared difference  
between our ground truth and our predictions  again averaged over the entire data set  
okay great so now we've seen two loss functions  one for classification binary outputs as well as  
regression continuous outputs and the problem now  i think that we need to start asking ourselves is  
how can we take that loss function we've seen our  loss function we've seen our network now we have  
to actually understand how can we put those two  things together how can we use our loss function  
to train the weights of our neural network  such that it can actually learn that problem  
well what we want to do is actually  find the weights of the neural network  
that will minimize the loss of our  data set that essentially means  
that we want to find the ws in our neural network  that minimize j of w jfw is our empirical cost  
function that we saw in the previous slides that  average loss over each data point in the data set
now remember that w capital w is simply  a collection of all of the weights in our  
neural network not just from one layer  but from every single layer so that's  
w0 from the zeroth layer to the first layer  to the second layer all concatenate into one  
in this optimization problem we want to optimize  all of the w's to minimize this empirical loss
now remember our loss function is just a  simple function of our weights if we have  
only two weights we can actually plot this entire  lost landscape over this grid of weight so on the  
one axis on the bottom you can see weight number  one and the other one you can see weight zero  
there's only two weights in this neural network  very simple neural network so we can actually plot  
for every w0 and w1 what is the loss what is the  error that we'd expect to see and obtain from this  
neural network now the whole process of training a  neural network optimizing it is to find the lowest  
point in this lost landscape that will tell us our  optimal w0 and w1 now how can we do that the first  
thing we have to do is pick a point so let's pick  any w0 w1 starting from this point we can compute  
the gradient of the landscape at that point  now the gradient tells us the direction of  
highest or steepest ascent okay  so that tells us which way is up  
okay if we compute the gradient of our  loss with respect to our weights that's  
the derivative our gradient for the loss  with respect to the weights that tells  
us the direction of which way is up on that  lost landscape from where we stand right now  
instead of going up though we want to find the  lowest loss so let's take the the negative of our  
gradient and take a small step in that direction  okay and this will move us a little bit closer  
to the lowest point and we just keep repeating  this now we compute the gradient at this point  
and repeat the process until we converge  and we will converge to a local minimum  
we don't know if it will converge to a global  minimum but at least we know that it should  
in theory converge to a local minimum now we can  summarize this algorithm as follows this algorithm  
is also known as gradient descent so we start by  initializing all of our weights randomly and we  
start and we loop until convergence we start  from one of those weights our initial point  
we compute the gradient that tells us which way is  up so we take a step in the opposite direction we  
take a small step here small is computed by  multiplying our gradient by this factor eta  
and we'll learn more about this factor later  this factor is called the learning rate  
we'll learn more about that later now again in  tensorflow we can actually see this pseudocode  
of grading descent algorithm written out in  code we can randomize all of our weights that  
in that basically initializes our search our  optimization process at some point in space  
and then we keep looping over and over and over  again and we compute the loss we compute the  
gradient and we take a small step of our weights  in the direction of that gradient but now let's  
take a look at this term here this is the how  we actually compute the gradient this explains  
how the loss is changing with respect to  the weight but i never actually told you  
how we compute this so let's  talk about this process  
which is actually extremely important in training  neural networks it's known as backpropagation
so how does backpropagation work  how do we compute this gradient  
let's start with a very simple neural network  this is probably the simplest neural network  
in existence it only has one input one hidden  neuron and one output computing the gradient of  
our loss j of w with respect to one of the weights  in this case just w2 for example tells us how much  
a small change in w2 is going to affect our loss  j so if we move around j infinitesimally small how  
will that affect our loss that's what the gradient  is going to tell us of derivative of j of w 2.  
so if we write out this derivative we can actually  apply the chain rule to actually compute it  
so what does that look like specifically  we can decompose that derivative into the  
derivative of j d d w over d y multiplied by  derivative of our output with respect to w2  
now the question here is with the second part  if we want to compute now not the derivative  
of our loss with respect to w2 but now the  loss with respect to w1 we can do the same  
story as before we can apply the chain rule now  recursively so now we have to apply the chain  
rule again to this second part now the second  part is expanded even further so the derivative  
of our output with respect to z1 which is the  activation function of this first hidden unit and  
we can back propagate this information now you can  see starting from our loss all the way through w2  
and then recursively applying this chain rule  again to get to w1 and this allows us to see  
both the gradient at both w2 and w1 so  in this case just to reiterate once again  
this is telling us this dj dw1 is telling us how  a small change in our weight is going to affect  
our loss so we can see if we increase our weight a  small amount it will increase our loss that means  
we will want to decrease the weight to decrease  our loss that's what the gradient tells us which  
direction we need to step in order to decrease  or increase our loss function now we showed this  
here for just two weights in our neural network  because we only have two weights but imagine  
we have a very deep neural network one with more  than just two layers of or one layer rather of of  
hidden units we can just repeat this this process  of applying recursively applying the chain rule  
to determine how every single way in the model  needs to change to impact that loss but really  
all this boils down to just recursively applying  this chain rule formulation that you can see here
and that's the back propagation algorithm in  theory it sounds very simple it's just a very  
very basic extension on derivatives and the chain  rule but now let's actually touch on some insights  
from training these networks in practice that make  this process much more complicated in practice  
and why using back propagation as we saw there  is not always so easy now in practice training  
neural networks and optimization of networks can  be extremely difficult and it's actually extremely  
computationally intensive here's the visualization  of what a lost landscape of a real neural network  
can look like visualized on just two dimensions  now you can see here that the loss is extremely  
non-convex meaning that it has many many local  minimum that can make using an algorithm like  
gradient descent very very challenging because  gradient descent is always going to step  
closest to the first local minimum but it can  always get stuck there so finding how to get  
to the global minima or a really good solution for  your neural network can often be very sensitive to  
your hyperparameters such as where the optimizer  starts in this lost landscape if it starts in a  
potentially bad part of the landscape it can very  easily get stuck in one of these local minimum
now recall the equation that we talked about for  gradient descent this was the equation i showed  
you your next weight update is going to be your  current weights minus a small amount called the  
learning rate multiplied by the gradient so we  have this minus sign because we want to step in  
the opposite direction and we multiply it by the  gradient or we multiply by the small number called  
here called eta which is what we call the learning  rate how fast do we want to do the learning  
now it determines actually not just how fast  to do the learning that's maybe not the best  
way to say it but it tells us how large should  each step we take in practice be with regards  
to that gradient so the gradient tells us the  direction but it doesn't necessarily tell us  
the magnitude of the direction so eta can tell  us actually a scale of how much we want to trust  
that gradient and step in the direction of that  gradient in practice setting even eta this one  
parameters one number can be extremely difficult  and i want to give you a quick example of why  
so if you have a very non-convex loc or lost  landscape where you have local minima if you  
set the learning rate too low then the model  can get stuck in these local minima it can  
never escape them because it gets it actually does  optimize itself but it optimizes it to a very sm  
to a non-optimal minima and it can converge very  slowly as well on the other hand if we increase  
our learning rate too much then we can actually  overshoot our our minima and actually diverge  
and and lose control and basically uh explode the  training process completely one of the challenges  
is actually how to pre how to use stable learning  rates that are large enough to avoid the local  
minima but small enough so that they don't  diverge and convert or that they don't diverge  
completely so they're small enough to actually  converge to that global spot once they reach it
so how can we actually set this learning  rate well one option which is actually  
somewhat popular in practice is to actually  just try a lot of different learning rates  
and that actually works it is a feasible approach  but let's see if we can do something a little bit  
smarter than that more intelligent what if we  could say instead how can we build an adaptive  
learning rate that actually looks at its lost  landscape and adapts itself to account for what  
it sees in the landscape there are actually  many types of optimizers that do exactly this  
this means that the learning rates are no longer  fixed they can increase or decrease depending on  
how large the gradient is in that location and how  fast we want and how fast we're actually learning
and many other options that could be also with  regards to the size of the weights at that point  
the magnitudes etc in fact these have been widely  explored and published as part of tensorflow as  
well and during your labs we encourage each of  you to really try out each of these different  
types of optimizers and experiment with  their performance in different types of  
problems so that you can gain very important  intuition about when to use different types of  
optimizers are what their advantages are and  disadvantages in certain applications as well  
so let's try and put all of this together so  here we can see a full loop of using tensorflow  
to define your model on the first line define  your optimizer here you can replace this with  
any optimizer that you want here i'm just using  stochastic gradient descent like we saw before  
and feeding it through the model we loop  forever we're doing this forward prediction  
we predict using our model we compute the  loss with our prediction this is exactly  
the loss is telling us again how incorrect our  prediction is with respect to the ground truth y  
we compute the gradient of our loss with respect  to each of the weights in our neural network and  
finally we apply those gradients using our  optimizer to step and update our weights  
this is really taking everything that we've  learned in the class in the lecture so far  
and applying it into one one whole  piece of code written in tensorflow  
so i want to continue this talk and really talk  about tips for training these networks in practice  
now that we can focus on this very powerful  idea of batching your data into mini batches  
so before we saw it with gradient descent that  we have the following algorithm this gradient  
that we saw to compute using back propagation can  be actually very intensive to compute especially  
if it's computed over your entire training set so  this is a summation over every single data point  
in the entire data set in most real-life  applications it is simply not feasible to  
compute this on every single iteration in  your optimization loop alternatively let's  
consider a different variant of this algorithm  called stochastic gradient descent so instead  
of computing the gradient over our entire data  set let's just pick a single point compute the  
gradient of that single point with respect to the  weights and then update all of our weights based  
on that gradient so this has some advantages this  is very easy to compute because it's only using  
one data point now it's very fast but it's also  very noisy because it's only from one data point  
instead there's a middle ground instead of  computing this noisy gradient of a single point  
let's get a better estimate of our gradient by  using a batch of b data points so now let's pick  
a batch of b data points and we'll compute the  gradient estimation estimate simply as the average  
over this batch so since b here is usually not  that large on the order of tens or hundreds of  
samples this is much much faster to compute than  regular gradient descent and it's also much much  
more accurate than just purely stochastic gradient  descent that only uses a single example now this  
increases the gradient accuracy estimation which  also allows us to converge much more smoothly  
it also means that we can trust our gradient more  than in stochastic gradient descent so that we can  
actually increase our learning rate a bit more  as well mini-batching also leads to massively  
parallelizable computation we can split up the  batches on separate workers and separate machines  
and thus achieve even more parallelization and  speed increases on our gpus now the last topic  
i want to talk about is that of overfitting this  is also known as the problem of generalization and  
is one of the most fundamental problems in all  of machine learning and not just deep learning
now overfitting like i said is critical to  understand so i really want to make sure that  
this is a clear concept in everyone's mind ideally  in machine learning we want to learn a model  
that accurately describes our test data not the  training data even though we're optimizing this  
model based on the training data what we really  want is for it to perform well on the test data  
so said differently we want  to build representations  
that can learn from our training data but  still generalize well to unseen test data  
now assume you want to build a line to describe  these points underfitting means that the model  
does simply not have enough capacity to  represent these points so no matter how good  
we try to fit this model it simply does not  have the capacity to represent this type of data  
on the far right hand side we can see the  extreme other extreme where here the model  
is too complex it has too many parameters  and it does not generalize well to new data  
in the middle though we can see what's called  the ideal fit it's not overfitting it's not  
underfitting but it has a medium number of  parameters and it's able to fit in a generalizable  
way to the output and is able to generalize well  to brand new data when it sees it at test time  
now to address this problem let's talk about  
regularization how can we make sure that our  models do not end up over fit because neural  
networks do have a ton of parameters how  can we enforce some form of regularization  
to them now what is regularization regularization  is a technique that constrains our optimization  
problems such that we can discourage these complex  models from actually being learned and overfit  
right so again why do we need it we need it so  that our model can generalize to this unseen  
data set and in neural networks we have many  techniques for actually imposing regularization  
onto the model one very common technique and very  simple to understand is called dropout this is one  
of the most popular forms of regularization  in deep learning and it's very simple let's  
revisit this picture of a neural network this is  a two layered neural network two hidden layers  
and in dropout during training all we simply  do is randomly set some of the activations here  
to zero with some probability so what we can do is  let's say we pick our probability to be 50 or 0.5  
we can drop randomly for each of the activations  50 of those neurons this is extremely powerful as  
it lowers the capacity of our neural network so  that they have to learn to perform better on test  
sets because sometimes on training sets it just  simply cannot rely on some of those parameters  
so it has to be able to be resilient to  that kind of dropout it also means that  
they're easier to train because at least on every  forward passive iterations we're training only 50  
of the weights and only 50 of the gradients so  that also cuts our uh gradient computation time  
down in by a factor of two so because now  we only have to compute half the number of  
neuron gradients now on every iteration we dropped  out on the previous iteration fifty percent of  
neurons but on the next iteration we're going  to drop out a different set of fifty 50 of the  
neurons a different set of neurons and this gives  the network it basically forces the network to  
learn how to take different pathways to get to its  answer and it can't rely on any one pathway too  
strongly and overfit to that pathway this is a way  to really force it to generalize to this new data  
the second regularization technique that  we'll talk about is this notion of early  
stopping and again here the idea is very basic  it's it's basically let's stop training once  
we realize that our our loss is increasing on a  held out validation or let's call it a test set  
so when we start training we all know the  definition of overfitting is when our model  
starts to perform worse on the test set so if we  set aside some of this training data to be quote  
unquote test data we can monitor how our network  is learning on this data and simply just stop  
before it has a chance to overfit so on the x-axis  you can see the number of training iterations and  
on the y-axis you can see the loss that we  get after training that number of iterations  
so as we continue to train in the beginning both  lines continue to decrease this is as we'd expect  
and this is excellent since it  means our model is getting stronger  
eventually though the network's testing  loss plateaus and starts to increase  
note that the training accuracy will always  continue to go to go down as long as the  
network has the capacity to memorize the data and  this pattern continues for the rest of training  
so it's important here to actually focus on this  point here this is the point where we need to stop  
training and after this point assuming  that our test set is a valid representation  
of the true test set the accuracy of the model  will only get worse so we can stop training here  
take this model and this should be the model that  we actually use when we deploy into the real world  
anything any model taken from the left hand  side is going to be underfit it's not going  
to be utilizing the full capacity of the  network and anything taken from the right  
hand side is over fit and actually performing  worse than it needs to on that held out test set
so i'll conclude this lecture by summarizing  three key points that we've covered so far  
we started about the fundamental building  blocks of neural networks the perceptron  
we learned about stacking and composing  these perceptrons together to form complex  
hierarchical neural networks and how to  mathematically optimize these models with  
back propagation and finally we address the  practical side of these models that you'll find  
useful for the labs today including adaptive  learning rates batching and regularization  
so thank you for attending the first  lecture in 6s191 thank you very much

Hi everyone, my name is Ava. I'm a lecturer and  organizer for 6.S191 and welcome to lecture 2  
which is going to focus on deep sequence modeling.  So in the first lecture with Alexander we learned  
about the essentials of neural networks and built  up that understanding moving from perceptrons to  
feed-forward models. Next we're going to turn our  attention to applying neural networks to problems  
which involve sequential processing of  data and we'll see why these sorts of tasks  
require a different type of network  architecture from what we've seen so far.  
To build up an understanding of these types of  models we're going to walk through this step  
by step starting from intuition about how these  networks work and building that up using where  
we left off with perceptrons and feed-forward  models as introduced in the first lecture. So  
let's dive right in. First, I'd like to motivate  what we mean in terms of sequential modeling and  
sequential processing by beginning with a very  simple intuitive example. Let's suppose we have  
this picture here of a ball and our task is  to predict where it's going to travel to next.  
Without any prior information about the  ball's history or understanding of the  
dynamics of its motion any guess on its  next position is going to be exactly that  
just to guess. But instead if in addition to the  current location of the ball I also gave you its  
previous locations now our problem becomes much  easier and I think we can all agree that we have  
a sense of where the ball is going to travel to  next. So hopefully this this intuitive example  
gives you a sense of what we mean in terms of  sequential modeling and sequential prediction  
and the truth is that sequential data and these  types of problems are really all around us for  
example audio like the waveform from my speech  can be split up into a sequence of sound waves and  
text can be split up into a sequence of characters  or also words when here each of these individual  
characters or each of the individual words can  be thought of as a time step in our sequence  
now beyond these two examples there are many  more cases in which sequential processing  
can be useful from medical signals to EKGs to  prediction of stock prices to genomic or genetic  
data and beyond so now that we've gotten a  sense of what sequential data looks like let's  
think about some concrete applications in which  sequence modeling plays out in the real world  
in the first lecture Alexander introduced  feed-forward models that sort of operate  
in this one-to-one manner going from a fixed  and static input to a fixed and static output  
for example he gave this this  use case of binary classification  
where we were trying to build a model that  given a single input of a student in this class  
could be trained to predict whether or not that  student was going to pass or not and in this type  
of example there's no time component there's no  inherent notion of sequence or of sequential data  
when we consider sequence modeling we now expand  the range of possibilities to situations that can  
involve temporal inputs and also potentially  sequential outputs as well so for example  
let's consider the case where we have a language  processing problem where there's a sentence as  
input to our model and that defines a sequence  where the words in the sentence are the individual  
time steps in that sequence and at the end our  task is to predict one output which is going to  
be the sentiment or feeling associated with that  sequence input and you can think of this problem  
as a having a sequence input single output  or as sort of a many-to-one sequence problem  
we can also consider the converse case where now  our input does not have that time dimension so  
for example when we're considering a static image  and our task is now to produce a sequence of input  
of outputs for example a sentence caption  that describes the content in this image  
and you can think of this as a one  to many sequence modeling problem  
finally we can also consider this situation  of many to many where we're now translating  
from a sequence to another sequence and perhaps  one of the most well-known examples of this  
type of application is in machine translation  where the goal is to train a model to translate  
sentences from one language to another all right  so this hopefully gives you a concrete sense of  
use cases and applications where sequence modeling  becomes important now i'd like to move forward  
to understand how we can actually build neural  networks to tackle these sorts of problems and  
sometimes it can be a bit challenging to sort of  wrap your head around how we can add this temporal  
dimension to our models so to address this and  to build up really a strong intuition i want to  
start from the very fundamentals and we'll  do that by first revisiting the perceptron  
and we're going to go step by step to develop a  really solid understanding of what changes need  
to be made to our neural network architecture  in order to be able to handle sequential data  
all right so let's recall and revisit the  perceptron which we studied in lecture one  
we defined the set of inputs right  which we can call x1 through xn  
and each of these numbers are going  to be multiplied by a weight matrix  
and then they're going to all be added together  to form this internal state of the perceptron  
which we'll say is z and then this value z is  passed through a non-linear activation function  
to produce a predictive output y hat and remember  that with the perceptron you can have multiple  
inputs coming in and since you know in this  lecture overall we're considering sequence  
modeling i'd like you to think of these inputs  as being from a single time step in your sequence  
we also saw how we could extend from the single  perceptron to now a layer of perceptrons to yield  
multi-dimensional outputs so for example here  we have a single layer of perceptrons in green  
taking three inputs in blue and predicting four  outputs shown in purple but once again does this  
have a notion of time or of sequence no it doesn't  because again our inputs and our outputs you can  
think of as being from a fixed time step in our  sequence so let's simplify this diagram right  
and to do that we'll collapse that hidden layer  down to this green box and our input and output  
vectors will be as depicted here and again our  inputs x are going to be some vectors of length  
m and our outputs are going to be of length n but  still we're still considering the input at just  
a specific time denoted here by t which is nothing  different from what we saw in the first lecture  
and even with this this simplified representation  of a feed forward network we could naively already  
try to feed a sequence into this model by just  applying that same model over and over again once  
for each time step in our sequence to get a sense  of this and how we could handle these individual  
inputs across different time step let's first just  rotate the same diagram from the previous slide  
so now again we have an input vector x of t  from some time step t we feed it into our neural  
network and then get an output vector at that time  step but since we're interested in sequential data  
let's assume we don't just have a single time step  right we have multiple individual time steps which  
start from let's say time zero the first time step  in our sequence and we could take that input at  
that time step treat it as this isolated point  in time pass it into the model and generate a  
predictive output and we could do that for the  next time step again treating it as something  
isolated and same for the next and to emphasize  here all of these models depicted here are just  
replicas of each other right with different  inputs at each of these different time steps  
but we we know sort of that our output and we  know from the first lecture that our output vector  
y hat at a particular time sub t is just going  to be a function of the input at that time step  
but let's take a step back here for a  minute if we're considering sequential data  
it's probably very likely that the output or the  label at a later time step is going to somehow  
depend on the inputs at prior time steps so what  we're missing here by treating these individual  
time steps as individual isolated time steps is  this relationship that's inherent to sequence  
data between inputs earlier on in the sequence  to what we predict later on in the sequence so  
how could we address this what we really  need is a way to relate the computations  
and the operations that the network is doing at  a particular time step to both the prior history  
of its computation from prior time steps as well  as the input at that time step and finally to have  
a sense of forward looking right to be able to  pass that information the current information onto  
future time steps so let's try to do exactly that  what we'll consider is linking the information and  
the computation of the network at different time  steps to each other specifically we're going to  
introduce this internal memory or cell state which  we denote here as h of t and this is going to be  
this memory that's going to be maintained by the  neurons and the network itself and this state  
can be passed on time step to time step across  time and the key idea here is that by having  
this recurrence relation we're capturing some  notion of memory of what the sequence looks like  
what this means is now the network's output  predictions and its computations are not only  
a function of the input at a particular time  step but also the past memory of cell state  
denoted by h that is to say that our output  depends on both our current inputs as well as  
the past computations and the past learning that  occurred and we can define this relationship via  
these functions that map inputs to output and  these functions are standard neural network  
operations that alexander introduced in the first  lecture so once again our output our prediction is  
going to depend not only on the current input at  a particular time step but also on the past memory
and because as you see in this relation here our  output is now a function of both the current input  
and the past memory at a previous  time step this means we can describe  
these neurons via a recurrence  relation which means that the  
we have the cell state that depends on the current  input and again on the prior in prior cell states  
and the depiction on the right of this line  shows these individual time steps being sort  
of unrolled across time but we could also  depict the same relationship by this cycle  
and this is shown on the loop on the left of the  slide which shows this concept of a recurrence  
relation and it's exactly this idea of recurrence  that provides the intuition and the key operations  
behind recurrent neural networks or rnns and  we're going to continue for the remainder of  
this lecture to build up from this foundation and  build up our understanding of the mathematics of  
these recurrence relations and the  operations that define rnn behavior  
all right so let's formalize this a little  bit more the key idea here as i mentioned and  
hopefully that you take away from this lecture  is that these rnns maintain this internal state  
h of t which is updated at each time step as  the sequence is processed and this is done  
by this recurrence relation which specifically  defines how the state is updated at the time step  
specifically we define this internal cell state  h of t and that internal cell state is going to  
be a function of that is going to be defined by  a function that can be parametrized by a set of  
weights w which are what we're actually trying to  learn over the course of training such a network  
and that function f of w is going to take as input  both the input at the current time step x of t as  
well as the prior state h of t minus 1. and how  do we actually find and define this function again  
it's going to be parametrized by a set of weights  that are going to be specifically what's learned  
over the course of training the model and a key  key feature of rnns is that they use this very  
same function and this very same set of parameters  at every time step of processing the sequence  
and of course the weights are going to  change over time over the course of training  
and later on we'll see exactly how but at  each iteration of training that same set of  
weights is going to be applied to each of  the individual time steps in the sequence  
all right so now let's let's step through the  algorithm for updating rnns to get a better sense  
of how these networks work we're going to  begin by initializing our network which i'm  
just abstracting away here in this code  block as in the pseudo code block as rnn  
and we're also going to initialize a hidden state  as well as a sentence and let's say ours our task  
here is to predict the network the next word  in the sentence the rnn algorithm is as follows  
we're going to loop through the words in the in  this sentence and at each step we're going to  
feed both the current word and the previous  hidden state into our rnn and this is going  
to generate a prediction for the next word as  well as an update to the hidden state itself  
and finally when we've done when we've when we're  done processing these four words in this sentence  
we can generate our prediction  for what the next word actually is  
by considering the rnn's output after all the  individual words have been fed through the model
all right so as you may have realized the  rnn computation includes both this internal  
cell state update to hft as well as the output  prediction itself so now we're going to concretely  
walk through how each of these computations is  defined all right going from bottom to top right  
we're going to consider our input vector x of t  and we're next going to apply a function to update  
the hidden state and this function is a standard  neural network operation just like we saw in the  
first lecture and again because this internal  cell state h of t is going to depend on both the  
input x of t as well as the prior cell state h of  t minus one we're going to multiply each of these  
individual terms by their respective weight  matrices and we're going to add the result  
and then apply a non-linear activation function  which in this case is going to be a hyperbolic  
tangent to the sum of these two terms to  actually update the value of the hidden state  
and then to generate our output at a given time  step we take that internal hidden state multiply  
it by a separate weight matrix which inherently  produces a modified version of this internal state  
and this actually forms our output prediction  so this gives you the mathematics behind  
how the rnn can actually update its hidden  state and also to produce a predictive output
all right so so far we've seen rnn's being  depicted as having these internal loops that  
feedback on themselves and we've also seen how we  can represent this loop as being unrolled across  
time where we can start from a first time step  and continue to unroll the network across time  
up until time set t and within this diagram  we can also make explicit the weight matrices  
starting from the weight matrix that defines how  the inputs at each time step are being transformed  
in the hidden state computation as  well as the weight matrices that define  
the relationship between the prior hidden  state and the current hidden state and finally  
the weight matrix that transforms the hidden  state to the output at a particular time step  
and again to re-emphasize in all of these  cases for all of these weight matrices  
we're going to be reusing the same weight matrix  matrices at every time step in our sequence  
now when we make a forward pass through the  network we're going to generate outputs at  
each of those individual time steps and from those  individual outputs we can derive a value for the  
loss and then we can sum all of these losses  from the individual time steps together  
to determine the total loss  which will be ultimately what is  
used to train our rnn and we'll get to  exactly how we achieve this in a few slides
all right so now this this gives you a an  intuition and mathematical foundation for  
how we actually can make a forward  pass a forward step through our rnn  
let's now walk through an example of how we can  implement an rnn from scratch using tensorflow  
we're going to define the rnn using a layer so  we can build it up from as inheriting from the  
layer class that alexander introduced in the first  lecture we can also initialize the weight matrices  
and also finally initialize the hidden state of  the rnn to all zeros our next step is going to be  
to define what we call the call function  and this function is really important  
because it describes exactly how we can make a  forward pass through the network given a input  
our first step in this forward pass is  going to be to update the hidden state  
according to that same exact equation  we saw earlier where the hidden state  
and the from the previous time step and an  input x are multiplied by their relative  
relevant wave matrices are summed and then  passed through a non-linear activation function  
we next compute the output by transforming  this hidden state via multiplication by a  
separate weight matrix and at each time step  we're going to return both the current output as  
well as the hidden state so this gives a sense  of breaks down how we define the forward pass  
through an rnn in code using tensorflow but  conveniently tensorflow has already implemented  
these types of rnn cells for us which you can  use via the simple rnn layer and you're going  
to get some practice doing exactly this and using  the rnns later on in today's lab all right so to  
recap now that we're at this point in this lecture  where we've built up our understanding of rnn's  
and their mathematical basis i'd like to turn back  to those applications of sequence modeling that we  
discussed earlier on and hopefully now you've  gotten a sense of why rnns can be particularly  
suited for handling sequential data again with  feedforward or traditional neural networks  
we're operating in this one-to-one manner  going from a static input to a static output  
in contrast with sequences we can go from a  sequential input where we have many time steps  
defined sequentially over time feed them into  a recurrent neural network and generate a  
a single output like a classification of  sentiment or emotion associated with a sentence  
we can also move from a static input for example  an image to a sequential output going from one to  
many and finally we can go from sequential input  to sequential output many to many and two examples  
of this are in machine translation and also in  music generation and with the latter with music  
generation you'll actually get the chance  to implement an rnn that does exactly this  
in later on in today's lab beyond this we can  extend recurrent neural networks to many other  
applications in which sequential processing  and sequential modeling may be useful
to to really appreciate why recurrent  neural networks are so powerful  
i'd like to sort of consider a concrete  set of what i like to call design criteria  
that we need to be keeping in mind when thinking  about sequence modeling problems specifically we  
need to be able to ensure that our recurrent  neural network or any machine learning model  
that we may be interested in will be equipped to  handle variable length sequences because not all  
sentences not all sequences are going to have  the same length so we need to have the ability  
to handle this variability we also need to have  this critical property of being able to track  
long-term dependencies in the data and to  have a notion of memory and associated with  
that is also the ability to have this sense  of order and have a sense of how things that  
occur previously or earlier on in the sequence  affect what's going to happen or occur later on  
and to do this we can achieve both points two and  three by using weight sharing and actually sharing  
the values of the way matrices across the entire  sequence and we'll see i'm telling you now and  
we'll see that recurrent neural networks do indeed  meet all these sequence modeling design criteria  
all right so to understand these criteria  concretely i'd like to consider a very concrete  
sequence modeling problem which is going to be the  following given some series of words in a sentence  
our task is going to be to predict the most likely  next word to occur in that sentence all right  
so let's suppose we have this sentence as an  example this morning i took my cat for a walk  
and our task is let's say we're given these  words this morning i took my cat for a and  
we want to predict the next word in the c in the  sentence walk and our goal is going to be to try  
to build a recurring neural network to do exactly  this what's our first step to tackle this problem  
well the first consideration before we even get  started with training our model is how we can  
actually represent language to a neural network so  let's suppose we have a model where we input the  
word deep and we want to use the neural network to  predict the next word learning what could be the  
issue here in terms of how we are passing in these  in this input to our network remember that neural  
networks are functional operators they execute  functional mathematical operations on their inputs  
and generate numerical outputs as a result so  they can't really interpret and operate on words  
if they're just passed in as words so what we  have here is is just simply not going to work  
instead neural networks require numerical inputs  that can be a vector or an array of numbers such  
that the model can operate on them to generate  a vector or array of numbers as the output  
so this is going to work for us but  operating just on words simply is not
all right so now we know that we need to have  a way to transform language into this vector  
or array based representation how exactly are we  going to go about this the solution we're going  
to consider is this concept of embedding which is  this idea of transforming a set of identifiers for  
objects effectively indices into a vector of fixed  size that captures the the content of the input so  
to think through how we could actually go about  doing this for language data let's again turn  
back to that example sentence that we've been  considering this morning i took my cat for a walk  
we want to be able to map any word that  appears or could appear in our body of language  
to a fixed size vector so our first step  is going to be generate to generate a  
vocabulary which is going to consist of  all unique words in our set of language  
we can then index these individual  words by mapping individual unique words  
to unique indices and these indices can  then be mapped to a vector embedding  
one way we could do this is by generating sparse  and binary vectors that are going to have a length  
that's equal to the number of unique words in  our vocabulary such that we can then indicate  
the nature of a particular word by encoding this  in the corresponding index so for example for the  
word cat we could encode this at the second index  in this sparse binary vector and this is a very  
common way of embedding and encoding language  data and it's called a one hot encoding and  
very likely you're going to encounter this in your  journey through machine learning and deep learning  
another way we could build up these embeddings  is by actually learning them so the idea here  
is to take our index mapping and feed that index  mapping into a model like a neural network model  
such that we can then transform that index mapping  
across all the words of our vocabulary to  a to a vector of a lower dimensional space  
where the values of that vector are learned  such that words that are similar to each other  
are have similar embeddings and an example  that demonstrates this concept is shown here  
all right so these are two distinct ways in which  we can encode language data and transform language  
data into a vector representation that's going  to be suitable for input to a neural network  
now that we've now that we've built up this  way to encode language data and to actually get  
started with feeding it into our recurrent neural  network model let's go back to that set of design  
criteria where the first capability we desired is  this ability to handle variable sequence lengths  
and again let's consider this task of trying to  predict the next word in a sentence we could have  
very short sentences right where driving words  driving the meaning of our prediction are going  
to be very close to each other but we could  also have a longer sequence or an even longer  
sequence where the information that's needed  to predict the next word occurs much earlier on  
and the key requirement for our recurrent  neural network model is the ability to  
handle these inputs of varying length feed forward  networks are not able to do this because they have  
inputs of fixed dimensionality and then those  fixed dimensionality inputs are passed into the  
next layer in contrast rnns are able to handle  variable sequence lengths and that's because  
those differences in sequence lengths are just  differences in the number of time steps that  
are going to be input and processed by the rnn  so rnns meet this first first design criterion  
our second criterion is the ability to effectively  capture and model long-term dependencies in data  
and this is really exemplified in examples like  this one where we clearly need information from  
much earlier in the sequence or the sentence to  accurately make our prediction rnns are able to  
achieve this because they have this way of  updating their internal cell state via the  
recurrence relation we previously discussed  which fundamentally incorporates information  
from the past state into the cell state update  so this criteria is also met next we need to  
be able to capture differences in sequence order  which could result in differences in the overall  
meaning or property of a sequence for example in  this case where we have two sentences that have  
opposite semantic meaning but have the same words  with the same counts just in a different order  
and once again the cell state maintained by an  rnn depends on its past history which helps us  
capture these sorts of differences because we  are maintaining information about past history  
and also reusing the same weight matrices across  each of the individual time steps in our sequence  
so hopefully going through this example of  predicting the next word in a sentence with a very  
particularly common type of  sequential data being language data  
this shows how it shows you how sequential data  more broadly can be represented and encoded for  
input to rnns and how rnns can achieve these  set the set of sequence modeling design criteria  
all right so now we at this stage in the lecture  we've built up our intuition and our understanding  
of how recurrent neural networks work how they  operate and what it means to model sequences  
now we can discuss the algorithm for how we can  actually train recurrent neural networks and it's  
a twist on the back propagation algorithm that  was introduced in lecture one it's called back  
propagation through time so to get there let's  first again take a step back to our first lecture  
and recall how we can actually train feed-forward  models using the back propagation algorithm  
we first take a set of inputs and make a forward  pass through the network going from input to  
output and then to train the model we back  propagate net gradients back through the network  
and we take the derivative of the loss with  respect to each weight parameter in our network  
and then adjust the parameters the weights  in our model in order to minimize that loss
for rnns as we walked through earlier are  forward pass through the network consists  
of going forward across time and updating the cell  state based on the input as well as the previous  
state generating an output and fundamentally  computing the loss values at the individual  
time steps in our sequence and finally summing  those individual losses to get the total loss  
instead of back propagating errors through a  single feed-forward network at a single time  
step in rnns those errors are going to  be back propagated from the overall loss  
through each individual time step and then  across the time steps all the way from where  
we are currently in the sequence back to the  beginning and this is the reason why it's called  
back propagation through time because as  you can see all of the errors are going  
to be flowing back in time from the most recent  time step to the very beginning of the sequence  
now if we expand this out and take a closer  look at how gradients can actually flow across  
this chain of repeating recurrent neural network  module we can see that between each time step we  
have to perform this matrix multiplication  that involves the weight matrix w h of h  
and so computing the gradient with respect to the  initial cell state h of 0 is going to involve many  
factors of this weight matrix and also repeated  computation of the gradients with respect to the  
this weight matrix this can be problematic for a  couple of reasons the first being that if we have  
many values in this series this chain of matrix  multiplications where the gradient values are  
less or greater than 1 or the weight values are  greater than 1 we can run into a problem that's  
called the exploding gradient problem where our  gradients are going to become extremely large and  
we can't really optimize and the solution here is  to do what is called gradient clipping effectively  
scaling back the values of particularly  large gradients to try to mitigate this  
we can also have the opposite problem where now  our weight values or our gradients are very very  
small and this can lead to what is  called the vanishing gradient problem  
when gradients become increasingly smaller and  smaller and smaller such that we can no longer  
effectively train the network and today we're  going to discuss three ways in which we can  
address this vanishing gradient problem first by  cleverly choosing our activation function also by  
smartly initially initializing our weight matrices  and finally we can we'll discuss how we can make  
some changes to the network architecture itself  to alleviate this vanishing gradient problem  
all right in order to get into that you'll need  some intuition about why vanishing gradients could  
be a problem let's imagine you keeps multiplying  a small number something in between 0 and 1 by  
another small number over time that number  is going to keep shrinking and shrinking and  
eventually it's going to vanish and what this  means when this occurs for gradients is that  
it's going to be harder and harder and harder  to propagate errors from our loss function  
back into the distant past because we have this  problem of the gradients becoming smaller and  
smaller and smaller and ultimately what this  will lead to is we're going to end up biasing  
the weights and the parameters of our network  to capture shorter term dependencies in the data  
rather than longer term dependencies to see why  this could be a problem let's again consider  
this example of training a language model to  predict the next word in a sentence of words  
and let's say we're given this phrase the clouds  are in the blank in this case it's it's pretty  
obvious what the next word is likely going to be  right sky because there's not that much of a gap  
in the sequence between the relevant information  the word cloud and the place where our prediction  
is actually going to be needed and so  an rnn could be equipped to handle that  
but let's say now we have this other sentence  i grew up in france and i speak fluent blank  
where now there's more context that's needed from  earlier in the sentence to make that prediction  
and in many cases that's going to be exactly  the problem where we have this large gap  
between what's relevant and the point where we may  need to make a prediction and as this gap grows  
standard rnns become increasingly unable to  connect the relevant information and that's  
because of this vanishing grading  problem so it relates back to this  
need to be able to effectively model and  capture long-term dependencies in data  
how can we get around this the first trick  we're going to consider is pretty simple we  
can smartly select the activation function our  networks use specifically what is commonly done  
is to use a relu activation function where the  derivative of the of this activation function  
is greater than one for all instances in which x  is greater than zero and this helps the value of  
the gradient of our of our with respect to  our loss function to actually shrink when um  
the values of its input are greater than zero  another thing we can do is to be smart in how  
we actually initialize the parameters in our  network and we can specifically initialize the  
weights to the identity matrix to be able to try  to prevent them from shrinking to zero completely  
and very rapidly during back propagation our final  solution and the one that we're going to spend the  
most time discussing and it's also the most robust  is to introduce and to use a sort of more complex  
recurrent unit that can more effectively  track long-term dependencies in the data  
by intuitively you can think of it as controlling  what information is passed through and and what  
information is used to update the actual cell  state specifically we're going to use what is  
called gated cells and today we're going to focus  on one particular type of gated cell which is  
definitely the most common and most broadly used  in recurrent neural networks and that's called  
the long short term memory unit or lstm and what's  cool about lstms is that networks that are built  
using lstms are particularly well suited at better  maintaining long-term dependencies in the data and  
tracking information across multiple time steps  to try to overcome this vanishing gradient problem  
and more importantly to more effectively model  sequential data all right so lstms are really the  
the workhorse of the deep learning community  for most sequential modeling tasks so let's  
discuss a bit about how lcms work and my goal for  this part of the lecture is to provide you with  
the intuition about the fundamental operations of  lstms abstracting away a little bit of the math  
because it can get again a little confusing to  wrap your mind around but hopefully i hope to  
provide you with the intuitive understanding  about how these networks work all right  
so to understand the key operations that make  lstm special let's first go back to the general  
structure of an rnn and here i'm depicting  it slightly differently but the concept is  
exactly that from what i introduced previously  where we build up our recurrent neural network  
via this repeating module linked across time and  what you're looking at here is a representation  
of an rnn that shows a illustration of those  operations that define the state and output  
update functions so here we've simplified  it down where these black lines effectively  
capture weight matrix multiplications  and the yellow rectangles such as the  
tan h depicted here show the non-linear the  application of a non-linear activation function  
so here in this diagram this repeating module  of the rnn contains a single computation neural  
network computation node which is consisting of  a tan h activation function layer so here again  
we perform this update to the internal cell  state h of t which is going to depend on the  
previous cell state h of t minus 1 as well as  the current input x of t and at each time step  
we're also going to generate an output  prediction y of t a transformation of the state  
lstms also have this chain like structure but  the internal repeating module that recurrent unit  
is slightly more complex in the lcm this  repeating recurrent unit contains these  
different interacting layers again which are  defined by standard neural network operations  
like sigmoid and 10h non-linear activation  functions weight matrix multiplications but  
what's cool and what's neat about these different  interacting layers is that they can effectively  
control the flow of information through the  lstm cell and we're going to walk through how  
these updates actually enable the lstms to track  and store information throughout many time steps  
and here you can see how we can  define a lstm layer using tensorflow  
all right so the key idea behind lstms is  that they can effectively selectively add  
or remove information to the internal cell state  using these structures that are abstracted away  
calling by being called gates and these gates  consist of a standard neural network layer like a  
sigmoid shown here and a pointwise multiplication  so let's let's take a moment to think about what a  
gate like this could be doing in this case because  we have the sigmoid activation function this is  
going to force anything that passes through that  gate to be between 0 and 1. so you can effectively  
think of this as modulating and capturing how  much of the input should be passed through  
between between nothing 0 or everything one  which effectively gates the flow of information  
lstms use this type of operation to process  information by first forgetting irrelevant  
by first forgetting irrelevant history secondly  by storing the most relevant new information  
fi thirdly by updating their internal cell state  and then generating a output the first step is to  
forget irrelevant parts of the previous state  and this is achieved by taking the previous state  
and passing it through one of these sigmoid gates  which again you can think of it as as modulating  
how much should be passed in or kept out the  next step is to determine what part of the new  
information and what part of the old information  is relevant and to store this into the cell state
and what is really critical about lstms is that  they maintain the separate value of the cell state  
c of t in addition to what we introduced  previously h of t and c of t is what is  
going to be selectively updated  by via these gatewise operations  
finally we can return an output from our  lstm and so there is a a interacting layer  
an output gate that can control what information  that's encoded in the cell state is ultimately  
outputted and sent to the network as input  in the following time step so this operation  
controls both the value of the output y of t as  well as the um the cell state that's passed on  
time step to time step in the form of h of t the  key takeaway that i want you to have about lstms  
in this from this lecture is that they  can regulate information flow in storage  
and by doing this they can effectively better  capture longer term dependencies and also help  
us train the networks overall and overcome the  vanishing gradient problem and the key way that  
they help during training is that all of these  different gating mechanisms actually work to allow  
for what i like to call the uninterrupted  flow of gradient computation over time  
and this is done by maintaining the  separate cell state c of t across which  
the actual gradient computation so taking the  derivative with respect to the weights updating
derivative of the loss with respect to the  weights and shifting the weights in response  
occurs with respect to this uh separate separately  maintained cell state c of t and what this  
ultimately allows for is that we can mitigate  the vanishing gradient problem that's seen with  
traditional rnns so to recap on the key concepts  behind lstms lstms maintain a separate cell state  
from what is outputted that's c of t they use  these gates to control the flow of information  
by forgetting uh irrelevant information from  past history storing relevant information  
from the current input updating their cell state  and outputting the prediction at each time step  
and it's really this maintenance of the separate  cell state cft which allows for back propagation  
through time with uninterrupted gradient  flow and more efficient and more effective  
training and so for these reasons lscms are very  very commonly used as sort of the backbone rnn in  
modern deep learning all right so now that we've  gone through the fundamental workings of rnn's  
been introduced to the back propagation through  time algorithm and also considered the lstm  
architecture i'd like to consider a few very  concrete practical examples of how recurrent  
neural networks can be deployed for sequential  modeling including an example that you'll get  
experience with in today's lab and that's the  the task and the question of music generation  
or music prediction so let's suppose you're trying  to build up a recurrent neural network that can  
take sequences of musical notes and actually from  that sequence predict what is the most likely next  
note to occur and not only do you want to predict  what's the most likely next note to occur we want  
to actually take this trained model and use  it to generate brand new musical sequences  
that have never been heard before and we can  do this by basically seeding a trained rnn  
model with a first note and then um iteratively  building up the sequence over time to generate  
a new song and indeed this is one of the one of  the most exciting and powerful applications of  
recurrent neural networks and to motivate this  which is going to be the topic of your lab today  
i i'm going to introduce a really fun and  interesting historical example and it turns out  
that one of the most famous classical composers  franz schubert had a famous symphony which was  
called the unfinished symphony and the symphony  is is described exactly like that it's unfinished  
it was actually left at two movements rather than  four and schubert did not was not able to finish  
composing the symphony symphony before he died  and recently they there was a neural network based  
algorithm that was trained and put to the test  to actually finish this symphony and compose  
two new movements and this was done by training  the the model of recurrent neural network model on  
schubert's body of work and then testing  it by tasking with tasking the model with  
trying to generate the new composition  given given the score of the previous two  
movements of this unfinished symphony so  let's listen in to see what the result was
i'd like to continue because i'm actually  enjoying listening to that music but we also  
have to go on with the lecture so pretty awesome  right and i hope you agree that i think you know  
it's really exciting to see neural networks being  put to the test here but also at least for me this  
sparks some questioning and understanding  about sort of what is the line between  
artificial intelligence and human creativity and  you'll get a chance to explore this in today's lab  
another cool example is beyond music generation  is one in language processing where we can go from  
an input sequence like a sentence to a single  output where we can train an rnn to take this  
to train an rnn to let's say produce a a  prediction of emotion or sentiment associated with  
a particular sentence either positive or negative  and this is effectively a classification task  
much like what we saw in the first lecture  except again we're operating over a sequence  
where we have this time component so because  this is a classification problem we can train  
these networks using a cross-entropy loss and one  application where that we may be interested in is  
classifying the sentiments associated with  tweets so for example this tweet we could  
train an rnn to predict that this first tweet  about our class 6s191 has a positive sentiment  
but that this other tweet about the  weather actually has a negative sentiment  
all right so the next example i'll talk about  is one of the most powerful applications of  
recurrent neural networks and it's the backbone  of things like google translate and that's  
this idea of machine translation where our goal is  to input a sentence in one language and train an  
rnn to output a sentence in another language and  this is can be done by having an encoder component  
which effectively encodes the original sentence  into some state vector and a decoder component  
which decodes that state vector into  the target language the new language  
but it's it's quite remarkable that and it's quite  remarkable that using the foundations and the  
concepts that we learned today about sequential  modeling and about recurrent neural networks  
we can tackle this very complex problem of machine  translation but what could be some potential  
issues with using this approach using rnns or  lstams the first issue is that we have this  
encoding bottleneck we need to which means that  we need to encode a lot of content for example a  
long body of text of many different words into the  single memory state vector which is a compressed  
version of all the information necessary to  complete the translation and it's this state  
vector that's ultimately going to be passed on  and decoded to actually achieve the translation  
and by by forcing this this compression we could  actually lose some important information by  
imposing um this extreme encoding bottleneck which  is definitely a problem another limitation is that  
the recurrent neural networks that we learned  about today are not really that efficient as they  
require sequential processing of information  which is the point that i've sort of been  
driving home all along and it's the sequential  nature that makes these recurrent neural networks  
relatively inefficient on modern gpu hardware  because it's difficult to parallelize them  
and furthermore besides this problem of speed  
we need to be able to to to train the  rnn we need to go from the decoded output  
all the way back to the original input  which will involve going through order t or  
number t iterations of the of the network where t  is the number of time steps that we feed into our  
sequence and so what this means in practice is  that back propagation through time is actually  
very very expensive especially when considering  large bodies of text that need to be translated
that's depicted here finally  and perhaps most importantly  
is this fact that traditional rnns have limited  memory capacity and we saw that recurrent neural  
networks suffered from this vanishing gradient  problem lestm's helped us a bit but still they  
both of these architectures are not super  effective at handling very long temporal  
dependencies that could be found in large bodies  of text that need to be translated so how can we  
build an architecture that could be aware of these  dependencies that may occur in larger sequences or  
bodies of text to overcome these these limitations  a method that's called attention was developed  
and instead the way it works is that instead of  the decoder component having access to only the  
final encoded state that state vector passed  from encoder to decoder instead the decoder  
now has access to the states after each of the  time steps in the original sentence and it's  
the weighting of these vectors that is actually  learned by the network over the course of training  
and this is a really interesting idea because  what this attention module actually does is to  
learn from the input which points and which states  to attend to and it makes it very efficient and  
capable of capturing long-term dependencies  as easily as it does short-term dependencies  
and that's because to train such a network  it only requires a single pass through this  
attention module no back propagation through  time and what you can think of these attention  
mechanisms as providing is this sort of learnable  memory access and indeed this system is called  
attention because when the network is actually  learning the weighting it's learning to place  
its attention on different parts of the input  sequence to effectively capture a sort of  
accessible memory across the entirety of the  original sequence it's a really really really  
powerful idea and indeed it's the basis of  a new class and then rapidly emerging class  
of models that are extremely powerful for large  scale sequential modeling problems and that that  
class of models is called transformers  which you may have heard about as well
this application and this consideration of  attention is very important not only in language  
modeling but in other applications as well so for  example if we're considering autonomous vehicles  
at any moment in time an autonomous vehicle like  this one needs to have a understanding of not  
only where every object is in its environment  but also where a particular object may move in  
the future so here's an example of a self-driving  car and the the red boxes on the right hand side  
depict a cyclist and as you can see the  cyclist is approaching a stopped vehicle
shown here in purple and the car the self-driving  car can be able to recognize that the the cyclist  
is now going to merge in front of the car and  before it does so the self-driving car pulls back  
and stops and so this is an example of trajectory  prediction and forecasting in which it's clear  
that we need to be able to attend to and make  effective predictions about where dynamic objects  
in a scene may move to in the future another  powerful example of sequential modeling is in  
environmental modeling and climate pattern  analysis and prediction so here we can visualize  
the predicted patterns for different environmental  markers such as winds and humidity and  
it's an extremely important and powerful  application for sequence modeling and for  
recurrent neural networks because effectively  predicting the future behavior of such markers  
could aid in projecting and planning for  long-term climate impact all right so hopefully  
over the course of this lecture you've gotten  a sense of how recurrent neural networks work  
and why they're so powerful for processing  sequential data we saw how we can model sequences  
by this defined recurrence relation and how  we could train them using the back propagation  
through time algorithm we then explored  a bit about how gated cells like lstms  
could help us model long-term dependencies in  data and also discussed applications of rnns  
to music generation machine translation and beyond  so with that we're going to transition now to the  
lab sessions where you're going to have a chance  to begin to implement recurrent neural networks  
on your own using tensorflow and we encourage  you to come to the class and the lab office hour  
gather town session to discuss the labs answer ask  your questions about both lab content as well as  
content from the lectures and we look  forward to seeing you there thank you
Hi Everyone and welcome back to MIT 6.S191! Today  we're going to be talking about one of my favorite  
topics in this course and that's how we can give  machines a sense of vision now vision is one of  
the most important human senses I believe sighted  people rely on vision quite a lot from everything  
from navigating in the world to recognizing  and manipulating objects to interpreting facial  
expressions and understanding very complex human  emotions i think it's safe to say that vision is  
a huge part of everyday human life and today we're  going to learn about how we can use deep learning  
to build very powerful computer vision  systems and actually predict what is where  
by only looking and specifically looking at only  raw visual inputs i like to think that this is a  
very super simple definition of what vision at its  core really means but actually vision is so much  
more than simply understanding what an image  is of it means not just what the image is of  
but also understanding where the objects  in the scene are and really predicting  
and anticipating forward in the future what's  going to happen next take this scene for example  
we can build computer vision algorithms that  can identify objects in the scene such as this  
yellow taxi or maybe even this white truck on the  side of the road but what we need to understand  
on a different level is what is actually going  to be required to achieve true vision where are  
all of these objects going uh for that we should  actually focus probably more on the yellow taxi  
than on the white truck because there are  some subtle cues in this image that you can  
probably pick up on that lead us to believe that  probably this white truck is parked on the side  
of the road it's stationary and probably won't  be moving in the future at least for the time  
that we're observing the scene the yellow taxi on  the other hand is even though it's also not moving  
it is much more likely to be stationary as a  result of the pedestrians that are crossing in  
front of it and that's something that is very  subtle but can actually be reasoned about very  
effectively by our brains and humans take this for  granted but this is an extraordinarily challenging  
problem in the real world and since in the  real world building true vision algorithms  
can require reasoning about all of these different  components not just in the foreground but also  
there are some very important cues that we can  pick up in the background like this light this  
uh road light as well as some obstacles in the  far distance as well and building these vision  
algorithms really does require an understanding  of all of these very subtle details now  
deep learning is bringing forward an incredible  revolution or evolution as well of computer vision  
algorithms and applications ranging from allowing  robots to use visual cues to perform things like  
navigation and these algorithms that you're going  to learn about today in this class have become so  
mainstreamed and so compressed that they are all  fitting and running in each of our pockets in our  
telephones for processing photos and videos  and detecting faces for greater convenience  
we're also seeing some extraordinarily exciting  applications of vision in biology and medicine  
for picking up on extremely subtle cues and  detecting things like cancer as well as in the  
field of autonomous driving and finally in a  few slides i'll share a very inspiring story  
of how the algorithms that you're going to learn  about today are also being used for accessibility  
to aid the visually impaired now deep learning  has taken computer vision especially computer  
vision by storm because of its ability to learn  directly from the raw image inputs and learn to do  
feature extraction only through observation of a  ton of data now one example of that that is really  
prevalent in the computer vision field is of  facial detection and facial recognition on the  
top left or on the left hand side you can actually  see an icon of a human eye which pictorially i'm  
using to represent images that we perceive and  we can also pass through a neural network for  
predicting these facial features now deep learning  has transformed this field because it allows  
the creator of the machine learning or the deep  learning algorithm to easily swap out the end task  
given enough data to learn this neural network  in the middle between the vision and the task  
and try and solve it so here we're performing an  end task of facial detection but just equivalently  
that end task could be in the context of  autonomous driving here where we take an image as  
an input which you can see actually in the bottom  right hand corner and we try to directly learn  
the steering control for the output and actually  learn directly from this one observation of the  
scene where the car should control so what is  the steering wheel that should execute and this  
is done completely end to end the entire control  system here of this vehicle is a single neural  
network learned entirely from data now this is  very very different than the majority of other  
self-driving car companies like you'll see with  waymo and tesla et cetera and we'll talk more  
about this later but i actually wanted to share  this one clip with you because this is one of the  
autonomous vehicles that we've been building  in our lab and here in csail that i'm part of  
and we'll see more about that  later in the lecture as well  
we're seeing like i mentioned a lot of  applications in medicine and healthcare  
where we can take these raw images and scans  of patients and learn to detect things like  
breast cancer skin cancer and now most recently  taking scans of patients lungs to detect covid19
finally i want to share this inspiring story  of how computer vision is being used to help  
the visually impaired so in this project actually  researchers built a deep learning enabled device  
that can detect a trail for running and provide  audible feedback to the visually impaired user  
such that they can run and now to demonstrate  this let me just share this very brief video  
the machine learning algorithm that we have  detects the line and can tell whether the  
line is to the runners left right or center we  can then send signals to the runner that guides  
them left and right based on their positioning  the first time we went out we didn't even know  
if sound would be enough to guide me so it's a  sort of that beta testing process that you go  
through from human eyes it's very obvious it's  very obvious to recognize the line teaching a  
machine learning model to do that is not that  easy you step left and right as you're running  
so there's like a shake to the line left and  right as soon as you start going outdoors  
now the light is a lot more variable tree shadows  falling leaves and also the line on the ground  
can be very narrow and there may be only a few  pixels for the computer vision model to recognize
there was no tether there was no stick there  was no furry dog it was just being with yourself
ah that's the first time i run loading in decades
so these are often tasks that  we as humans take for granted  
but for a computer it's really remarkable  to see how deep learning is being applied  
uh to some of these problems focused on really  doing good and just helping people here in this  
case the visually impaired a man who has never run  without his uh guide dog before is now able to run  
independently through the through the trails with  the aid of this computer vision system and like  
i said we often take these tasks for granted but  because it's so easy for each sighted individual  
for us to do them routinely but we can actually  train computers to do them as well and in order  
to do that though we need to ask ourselves  some very foundational questions specifically  
stemming from how we can build a computer that  can quote unquote c and specifically how does a  
computer process an image let's use an image as  our base example of site to a computer so far  
so to a computer images are just  numbers there are two dimensional  
lists of numbers suppose we have a picture here  this is of abraham lincoln it's just made up of  
what are called pixels each of those numbers  can be represented by what's called a pixel  
now a pixel is simply a number like i said here  represented by a range of either zero to one or  
in 0 to 255 and since this is a grayscale image  each of these pixels is just one number if you  
have a color image you would represent it by  three numbers a red a green and a blue channel rgb  
now what does the computer see so we can represent  this image as a two-dimensional matrix of these  
numbers one number for each pixel in the image and  this is it this is how a computer sees an image  
like i said if we have a rgb image not a  a grayscale image we can represent this  
by a three-dimensional array now we have  three two-dimensional arrays stacked on  
top of each other one of those two dimensional  arrays corresponds to the red channel one for  
the green one for the blue representing this  rgb image and now we have a way to represent  
images to computers and we can start to think  about what types of computer vision algorithms  
we can perform with this so there are very there  are two very common types of learning tasks and  
those are like we saw in the first and the second  classes those are one regression and those are  
also classification tasks in regression tasks our  output takes the form of a continuous value and  
in classification it takes a single class label so  let's consider first the problem of classification  
we want to predict a label for each image so  for example let's say we have a database of all  
u.s precedents and we want to build  a classification pipeline to tell us  
which precedent this image is of so we feed  this image that we can see on the left hand  
side to our model and we wanted to output the  probability that this image is of any of these  
particular precedents that this database consists  of in order to classify these images correctly  
though our pipeline needs to be able to tell what  is actually unique about a picture of abraham  
lincoln versus a picture of any other president  like george washington or jefferson or obama
another way i think about this uh  these differences between these  
images and the image classification pipeline  is at a high level in terms of the features  
that are really characteristics of that particular  class so for example what are the features  
that define abraham lincoln now classification  is simply done by detecting the features in that  
given image so if the features for a particular  class are present in the image then we can predict  
with pretty high confidence that that class is  occurring with a high probability so if we're  
building an image classic classification pipeline  our model needs to know what are the features are  
what they are and two it needs to be able to  detect those features in a brand new image so  
for example if we want to detect human faces some  features that we might want to be able to identify  
would be noses eyes and mouths whereas like if  we want to detect cars we might be looking at  
certain things in the image like wheels license  plates and headlights and the same for houses and  
doors and windows and steps these are all examples  of features for the larger object categories
now one way to do this and solve this problem is  actually to leverage knowledge about a particular  
field say let's say human faces so if we want  to detect human faces we could manually define  
in images what we believe those features  are and actually use the results of our  
detection algorithm for classification but there's  actually a huge problem to this type of approach  
and that is that images are just 3d arrays  of numbers of brightness values and that each  
image can have a ton of variation and this  includes things like occlusions in the scene  
there could also be variations in illumination  the lighting conditions as well as you could  
even think of intra class variation variation  within the same class of images our classification  
pipeline whatever we're building really  needs to be invariant to all of these types  
of variations but it still needs to be  sensitive to picking out the different  
inter-class variations so being able to  distinguish a feature that is unique to this class  
in comparison to features or variations of  that feature that are present within the class
now even though our pipeline could be used  could use features that we as humans define  
that is if a human was to come into  this problem knowing something about  
the problem a priori they could define  or manually extract and break down  
what features they want to detect for this  specific task even if we could do that  
due to the incredible variability of the scene  of image data in general the detection of these  
features is still an extremely challenging problem  in practice because your detection algorithm needs  
to be invariant to all of these different  variations so instead of actually manually  
defining these how can we do better and what we  actually want to do is be able to extract features  
and detect their presence in images automatically  in a hierarchical fashion and this should remind  
you back to the first lecture when we talked about  hierarchy being a core component of deep learning  
and we can use neural network-based approaches  to learn these visual features directly from  
data and to learn a hierarchy of features  to construct a representation of the image  
internal to our network so again like we saw in  the first lecture we can detect these low-level  
features and composing them together to  build these mid-level features and then  
in later layers these higher level features  to really perform the task of interest  
so neural networks will allow us to learn these  hierarchies of visual features from data if we  
construct them cleverly so this will require us to  use some different architectures than what we have  
seen so far in the class namely architectures  from the first lecture with feedforward  
dense layers and in the second lecture recurrent  layers for handling sequential data this lecture  
will focus on yet another type of way that we  can extract features specifically focusing on  
the visual domain so let's recap what we learned  in lecture one so in lecture one we learned about  
these fully connected neural networks also called  dense neural networks where you can have multiple  
hidden layers stacked on top of each other and  each neuron in each hidden layer is connected to  
every neuron in the previous layer now let's  say we want to use a fully connected network  
to perform image classification and we're going  to try and motivate the the use of something  
better than this by first starting with what we  already know and we'll see the limitations of this  
so in this case remember our input is  this two-dimensional image it's a vector  
a two-dimensional vector but it can be collapsed  into a one-dimensional vector if you just stack  
all of those dimensions on top of each other  of pixel values and what we're going to do is  
feed in that vector of pixel values to our hidden  layer connected to all neurons in the next layer  
now here you should already appreciate something  and that is that all spatial information that we  
had in this image is automatically gone it's  lost because now since we have flattened this  
two-dimensional image into one dimension we have  now basically removed any spatial information  
that we previously had by the next layer and our  network now has to relearn all of that uh very  
important spatial information for example that  one pixel is closer to the its neighboring pixel  
that's something very important in our input but  it's lost immediately in a fully connected layer  
so the question is how can we build some structure  into our model so that in order so that we can  
actually inform the learning process and provide  some prior information to the model and help  
it learn this very complicated and large input  image so to do this let's keep our representation  
of our image our 2d image as an array a  two-dimensional array of pixel values let's not  
collapse it down into one dimension now one  way that we can use the spatial structure  
would be to actually connect patches of our input  not the whole input but just patches of the input  
two neurons in the hidden layer so before  everything was connected from the input layer to  
the hidden layer but now we're just gonna connect  only things that are within a single patch to the  
next neuron in the next layer now that is really  to say that each neuron only sees so if we look  
at this output neuron this neuron is only going to  see the values coming from the patch that precedes  
it this will not only reduce the number of weights  in our model but it's also going to allow us  
to leverage the fact that in an image spatially  close pixels are likely to be somewhat related and  
correlated to each other and that's a fact that  we should really take into account so notice how  
the only that only a small region of the input  layer influences this output neuron and that's  
because of this spatially connected idea that  we want to preserve as part of this architecture  
so to define connections across the whole input  now we can apply the same principle of connecting  
patches in our input layer to single neurons in  the subsequent layer and we can basically do this  
by sliding that patch across the input image and  for each time we slide it we're going to have a  
new output neuron in the subsequent layer now this  way we can actually take into account some of the  
spatial structure that i'm talking about inherent  to our input but remember that our ultimate task  
is not only to preserve spatial structure but  to actually learn the visual features and we  
do this by weighting the connections between  the patches and the neurons so we can detect  
particular features so that each patch is going  to try to perform that detection of the feature  
so now we ask ourselves how can we rate this  patch such that we can detect those features well  
in practice there's an operation called a  convolution and we'll first think about this  
at a high level suppose we have a 4x4 patch  or a filter which will consist of 16 weights  
we're going to apply this same filter to by four  patches in the input and use the result of that  
operation to define the state of the neuron in  the next layer so the neuron in the next layer  
the output that single neuron is going to be  defined by applying this patch with a filter  
with of equal size and learned weights  we're then going to shift that patch  
over let's say in this case by two pixels we  have here to grab the next patch and thereby  
compute the next output neuron now this is how we  can think about convolutions at a very high level  
but you're probably wondering here well how does  the convolution operator actually allow us to  
extract features and i want to make this really  concrete by walking through a very simple example
so suppose we want to classify the letter x  in a set of black and white images of letters  
where black is equal to negative one  and white is equal to positive one  
now to classify it's clearly not possible to  simply compare the two images the two matrices  
on top of each other and say are they equal  because we also want to be classifying this x  
uh no matter if it has some slight deformations  if it's shifted or if it's uh enlarged rotated  
or deformed we need we want to build a classifier  that's a little bit robust to all of these changes  
so how can we do that we want to detect the  features that define an x so instead we want  
our model to basically compare images of a  piece of an x piece by piece and the really  
important pieces that it should look for are  exactly what we've been calling the features  
if our model can find those important features  those rough features that define the x in the same  
positions roughly the same positions then it can  get a lot better at understanding the similarity  
between different examples of x even in  the presence of these types of deformities
so let's suppose each feature is like a mini  image it's a patch right it's also a small  
array a small two-dimensional array of values and  we'll use these filters to pick up on the features  
common to the x's in the case of this x  for example the filters we might want to  
pay attention to might represent things like the  diagonal lines on the edge as well as the crossing  
points you can see in the second patch here so  we'll probably want to capture these features  
in the arms and the center of the x in order  to detect all of these different variations  
so note that these smaller matrices  of filters like we can see on the  
the top row here these represent the filters of  weights that we're going to use as part of our  
convolution operation in order to detect the  corresponding features in the input image  
so all that's left for us to define is actually  how this convolution operation actually  
looks like and how it's able to pick up on these  features given each of these in this case three  
filters so how can it detect given a filter where  this filter is occurring or where this feature is  
occurring rather in this image and that is  exactly what the operation of convolution is  
all about convolution the idea of convolution  is to preserve the spatial relationship between  
pixels by learning image features in small little  patches of image data now to do this we need to  
perform an element-wise multiplication between  the filter matrix and the patch of the input image  
of same dimension so if we have a patch  of 3x3 we're going to compare that to an  
input filter or our filter which is also of  size 3x3 with learned weights so in this case  
our filter which you can see on the top left all  of its entries are of either positive one or one  
or negative one and when we multiply this filter  by the corresponding green input image patch  
and we element wise multiply  we can actually see the result  
in this matrix so multiplying all of the positive  ones by positive ones we'll get a positive one  
multiplying a negative one by a negative one will  also get a positive one so the result of all of  
our element-wise multiplications is going to be  a three by three matrix of all ones now the next  
step in as part of the convolution operation is  to add all of those element-wise multiplications  
together so the result here after we add those  outputs is going to be 9. so what this means now  
actually so actually before we  get to that let me start with  
another very brief example suppose we want  to compute the convolution now not of a  
very large image but this is just of a five by  five image our filter here is three by three so  
we can slide this three by three filter over the  entirety of our input image and performing this  
element-wise multiplication and then adding  the outputs let's see what this looks like so  
let's start by sliding this filter over the top  left hand side of our input we can element wise  
multiply the entries of this patch of this filter  with this patch and then add them together and  
for this part this three by three filter is  placed on the top left corner of this image  
element-wise multiply add and we get this  resulting output of this neuron to be four
and we can slide this filter over one one spot by  one spot to the next patch and repeat the results  
in the second entry now would be corresponding  to the activation of this filter applied to  
this part of the image in this case three and  we can continue this over the entirety of our  
image until the end when we have completely filled  up this activation or feature map and this feature  
map really tells us where in the input image was  activated by this filter so for example wherever  
we see this pattern conveyed in the original input  image that's where this feature map is going to  
have the highest value and that's where we need  to actually activate maximally now that we've  
gone through the mechanism of the convolution  operation let's see how different filters can be  
used to produce feature maps so picture a woman  of a woman a picture this picture of a woman's  
face this woman's name is lena and the output of  applying these three convolutional filters so you  
can see the three filters that we're considering  on the bottom right hand corner of each image  
by simply changing the weights of these filters  each filter here has a different weight we can  
learn to detect very different features in  that image so we can learn to sharpen the  
image by applying this very specific type of  sharpening filter we can learn to detect edges  
or we can learn to detect very strong edges in  this image simply by modifying these filters  
so these filters are not learned filters these  are constructed filters and there's been a ton  
of research historically about developing hand  engineering these filters but what convolutional  
neural networks learn to want to do is actually  to learn the weights defining these filters so  
the network will learn what kind of features it  needs to detect in the image doesn't need to do  
edge detection or strong edge detection or does  it need to detect certain types of edges curves  
certain types of geometric objects etc  what are the features that it needs to  
extract from this image and by learning the  convolutional filters it's able to do that  
so i hope now you can actually appreciate  how convolution allows us to capitalize on  
very important spatial structure and to use sets  of weights to extract very local features in the  
image and to very easily detect different features  by simply using different sets of weights and  
different filters now these concepts of preserving  spatial structure and local feature extraction  
using the convolutional operation are actually  core to the convolutional neural networks that  
are used for computer vision tasks and that's  exactly what i want to dive into next now that  
we've gotten the operation the mathematical  foundation of convolutions under our belts  
we can start to think about how we can utilize  this operation this operation of convolutions  
to actually build neural networks for computer  vision tasks and tie this whole thing in to this  
paradigm of learning that we've been exposed to  in the first couple lectures now these networks  
aptly are named convolutional neural networks  very appropriately and first we'll take a look  
at a cnn or convolutional neural network designed  specifically for the task of image classification  
so how can you use cnns for classification let's  consider a simple cnn designed for the goal here  
to learn features directly from the image  data and we can use these learned features  
to map these onto a classification task for  these images now there are three main components  
and operations that are core to a cnn the first  part is what we've already gotten some exposure to  
in the first part of this lecture and that  is the convolution operation and that allows  
us like we saw earlier to generate these  feature maps and detect features in our image  
the second part is applying a non-linearity  and we saw the importance of nonlinearities  
in the first and the second lecture in order to  help us deal with these features that we extract  
being highly non-linear thirdly we need to  apply some sort of pooling operation this is  
another word for a down sampling operation  and this allows us to scale down the size of  
each feature map now the computation of a class  of scores which is what we're doing when we define  
an image classification task is actually  performed using these features that we obtain  
through convolution non-linearity and pooling  and then passing those learned features into a  
fully connected network or a dense layer like we  learned about in the first part of the class in  
the first lecture and we can train this model end  to end from image input to class prediction output  
using fully connected layers and convolutional  layers end to end where we learn as part of the  
convolutional layers the sets of weights of the  filters for each convolutional layer and as well  
as the weights that define these fully connected  layers that actually perform our classification  
task in the end and we'll go through each one of  these operations in a bit more detail to really  
break down the basics and the architecture  of these convolutional neural networks
so first we'll consider the convolution  operation of a cnn and as before each neuron  
in the hidden layer will compute a weighted  sum of each of its inputs like we saw in the  
dense layers we'll also need to add on a bias  to allow us to shift the activation function  
and apply and activate it with some non-linearity  so that we can handle non-linear data  
relationships now what's really special here is  that the local connectivity is preserved each  
neuron in the hidden layer you can see in the  middle only sees a very specific patch of its  
inputs it does not see the entire input neurons  like it would have if it was a fully connected  
layer but no in this case each neuron output  observes only a very local connected patch as  
input we take a weighted sum of those patches we  compute that weighted sum we apply a bias and we  
apply and activate it with a non-linear  activation function and that's the  
feature map that we're left with at the end of a  convolutional layer we can now define this actual  
operation more concretely using a mathematical  equation here we're left with a 4x4 filter matrix  
and for each neuron in the hidden layer its  inputs are those neurons in the patch from the  
previous layer we apply this set of weights wi  j in this case like i said it's a four by four  
filter and we do this element-wise multiplication  of every element in w multiplied by the  
corresponding elements in the input x we add the  bias and we activate it with this non-linearity  
remember our element-wise multiplication  and addition is exactly that convolutional  
operation that we talked about earlier so if you  look up the definition of what convolution means  
it is actually that exactly it's element-wise  multiplication and then a summation of all of  
the results and this actually defines also how  convolutional layers are connected to these ideas  
but with this single convolutional layer we  can how can we have multiple filters so all  
we saw in the previous slide is how we can take  this input image and learn a single feature map  
but in reality there are many types of features  in our image how can we use convolutional layers  
to learn a stack or many different types of  features that could be useful for performing  
our type of task how can we use this to do  multiple feature extraction now the output layer  
is still convolution but now it has a volume  dimension where the height and the width are  
spatial dimensions dependent upon  the dimensions of the input layer  
the dimensions of the filter  the stride how how much we're  
skipping on each each time that we apply the  filter but we also need to think about the  
the connections of the neurons in these layers  in terms of their what's called receptive field  
the locations of their input in the in the  in the model in in the path of the model that  
they're connected to now these parameters actually  define the spatial arrangement of how the neurons  
are connected in the convolutional layers and how  those connections are really defined so the output  
of a convolutional layer in this case will have  this volume dimension so instead of having one  
filter map that we slide along our image now we're  going to have a volume of filters each filter  
is going to be slid across the image and compute  this convolution operation piece by piece for each  
filter the result of each convolution operation  defines the feature map that that convolution that  
that filter will activate maximally so now we're  well on our way to actually defining what a cnn is  
and the next step would actually be to apply that  non-linearity after each convolution operation we  
need to actually apply this non-linear activation  function to the output volume of that layer and  
this is very very similar like i said in the  first and we saw also in the second lecture  
and we do this because image data is highly  nonlinear a common example in the image domain  
is to use an activation function of relu which  is the rectified linear unit this is a pixel-wise  
operation that replaces all negative values with  zero and keeps all positive values with whatever  
their value was we can think of this really as a  thresholding operation so anything less than zero  
gets thresholded to zero negative values indicate  negative detection of a convolution but this  
nonlinearity actually kind of uh clamps that to  some sense and that is a nonlinear operation so  
it does satisfy our ability to learn non-linear  dynamics as part of our neural network model  
so the next operation in convolutional  neural networks is that of pooling  
pooling is an operation that is commonly used to  reduce the dimensionality of our inputs and of  
our feature maps while still preserving spatial  invariants now a common technique and a common  
type of pooling that is commonly used in practice  is called max pooling as shown in this example  
max pooling is actually super simple and intuitive  uh it's simply taking the maximum over these two  
by two filters in our patches and sliding that  patch over our input very similar to convolutions  
but now instead of applying a element-wise  multiplication and summation we're just simply  
going to take the maximum of that patch so in  this case as we feed over this two by two patch of  
filters and striding that patch by a factor of two  across the image we can actually take the maximum  
of those two by two pixels in our input and that  gets propagated and activated to the next neuron  
now i encourage all of you to really think  about some other ways that we can perform  
this type of pooling while still making sure that  we downsample and preserve spatial invariants  
taking the maximum over that patch is  one idea a very common alternative is  
also taking the average that's called mean pooling  taking the average you can think of actually  
represents a very smooth way to perform the  pooling operation because you're not just taking  
a maximum which can be subject to maybe outliers  but you're averaging it or also so you will get a  
smoother result in your output layer but they  both have their advantages and disadvantages
so these are three operations three  key operations of a convolutional  
neural network and i think now we're actually  ready to really put all of these together and  
start to construct our first convolutional  neural network end to end and with cnns  
just to remind you once again we can layer  these operations the whole point of this  
is that we want to learn this hierarchy  of features present in the image data  
starting from the low-level features composing  those together to mid-level features and then  
again to high-level features that can be used  to accomplish our task now a cnn built for image  
classification can be broken down into two parts  first the feature learning part where we actually  
try to learn the features in our input image  that can be used to perform our specific task  
that feature learning part is actually done  through those pieces that we've been seeing so far  
in this lecture the convolution the non-linearity  and the pooling to preserve the spatial invariance
now the second part the convolutional layers and  pooling provide output those the output excuse me  
of the first part is those high-level features of  the input now the second part is actually using  
those features to perform our classification  or whatever our task is in this case  
the task is to output the class probabilities that  are present in the input image so we feed those  
outputted features into a fully connected or dense  neural network to perform the classification we  
can do this now and we don't mind about losing  spatial invariance because we've already down  
sampled our image so much that it's not really  even an image anymore it's actually closer to a  
vector of numbers and we can directly apply our  dense neural network to that vector of numbers  
it's also much lower dimensional now and we  can output a class of probabilities using a  
function called the softmax whose output actually  represents a categorical probability distribution  
it's summed uh equal to one so it does  make it a proper categorical distribution  
and it is each element in this is strictly between  zero and one so it's all positive and it does sum  
to one so it makes it very well suited for the  second part if your task is image classification  
so now let's put this all together what does a  end-to-end convolutional neural network look like  
we start by defining our feature extraction head  which starts with a convolutional layer with 32  
feature maps a filter size of 3x3 pixels and we  downsample this using a max pooling operation  
with a pooling size of 2 and a stride of 2. this  is very exactly the same as what we saw when we  
were first introducing the convolution operation  next we feed these 32 feature maps into the next  
set of the convolutional convolutional and pooling  layers now we're increasing this from 32 feature  
maps to 64 feature maps and still down scaling our  image as a result so we're down scaling the image  
but we're increasing the amount of features  that we're detecting and that allows us to  
actually expand ourselves in this dimensional  space while down sampling the spatial information  
the irrelevant spatial information now finally now  that we've done this feature extraction through  
only two convolutional layers in this case we can  flatten all of this information down into a single  
vector and feed it into our dense layers and  predict these final 10 outputs and note here that  
we're using the activation function of softmax  to make sure that these outputs are a categorical  
distribution okay awesome so so far we've talked  about how we can use cnns for image classification  
tasks this architecture is actually so powerful  because it extends to a number of different tasks  
not just image classification and the reason for  that is that you can really take this feature  
extraction head this feature learning part and  you can put onto the second part so many different  
end networks whatever and network you'd like to  use you can really think of this first part as  
a feature learning part and the second part as  your task learning part now what that task is  
is entirely up to you and what you desire so and  that's that's really what makes these networks  
incredibly powerful so for example we may want  to look at different image classification domains  
we can introduce new architectures for  specifically things like image and object  
detection semantic segmentation and even  things like image captioning you can use  
this as an input to some of the sequential  networks that we saw in lecture two even
so let's look at and dive a bit deeper into  each of these different types of tasks that  
we could use are convolutional neural networks  for in the case of classification for example  
there is a significant impact in medicine and  healthcare when deep learning models are actually  
being applied to the analysis of entire inputs of  medical image scans now this is an example of a  
paper that was published in nature for actually  demonstrating that a cnn can outperform expert  
radiologists at detecting breast cancer directly  from mammogram images instead of giving a binary  
prediction of what an output is though cancer or  not cancer or what type of objects for example in  
this image we may say that this image is an image  of a taxi we may want to ask our neural network to  
do something a bit more fine resolution and tell  us for this image can you predict what the objects  
are and actually draw a bounding box localize this  image or localize this object within our image  
this is a much harder problem since there may  be many objects in our scene and they may be  
overlapping with each other partially occluded  etc so not only do we want to localize the object  
we want to also perform classification on that  object so it's actually harder than simply the  
classification task because we still have to  do classification but we also have to detect  
where all of these objects are in addition  to classifying each of those objects  
now our network needs to also be flexible and  actually and be able to infer not just potentially  
one object but a dynamic number of objects in the  scene now if we if we have a scene that only has  
one taxi it should output a bounding box over just  that single taxi and the bounding box should tell  
us the xy position of one of the corners and maybe  the height and the width of that bounding box as  
well that defines our bounding box on the other  hand if our scene contains many different types  
of objects potentially even of different types of  classes we want our network to be able to output  
many different outputs as well and be flexible  to that type of differences in our input even  
with one single network so our network should not  be constrained to only outputting a single output  
or a certain number of outputs it needs to have a  flexible range of how we can dynamically infer the  
objects in the scene so what is one maybe naive  solution to tackle this very complicated problem  
and how can cnns be used to do that so what we  can do is start with this image and let's consider  
the simplest way possible to do this  problem we can start by placing a random box  
over this image somewhere in the image it has  some random location it also has a random size  
and we can take that box and feed it through  our normal image classification network like  
we saw earlier in the lecture this is  just taking a single image or it's now  
a sub image but it's still a single image and it  feeds that through our network now that network is  
tasked to predict what is the what is the class  of this image it's not doing object detection  
and it predicts that it has some class if there is  no class of this box then it simply can ignore it  
and we repeat this process then we pick another  box in the scene and we pass that through the  
network to predict its class and we can keep doing  this with different boxes in the scene and keep  
doing it and over time we can basically have many  different class predictions of all of these boxes  
as they're passed through our classification  network in some sense if each of these boxes give  
us a prediction class we can pick the boxes that  do have a class in them and use those as a box  
where an object is found if no object is found we  can simply discard it and move on to the next box  
so what's the problem with this well one is that  there are way too many inputs the this basically  
results in boxes and considering a number of  boxes that have way too many scales way too  
many positions too many sizes we can't possibly  iterate over our image in all of these dimensions  
and and and have this as a naive solute and have  this as a solution to our object detection problem  
so we need to do better than that so instead of  picking random boxes or iterating over all of the  
boxes in our image let's use a simple heuristic  method to identify some places in the image  
that might contain meaningful objects  and use these to feed through our model  
but still even with this uh extraction of region  proposals the the rest of the store is the exact  
same we extract the region of proposal and we feed  it through the rest of our network we warp it to  
be the correct size and then we feed it to our  classification network if there's nothing in that  
box we discard it if there is then we keep it and  say that that box actually contained this image  
but still this has two very important problems  that we have to consider one is that it's still  
super super slow we have to feed in each region  independently to the model so if we extract  
in this case 2000 regions we have here we have to  feed this we have to run this network 2 000 times  
to get the answer just for the single image it  also tends to be very brittle because in practice  
how are we doing this region proposal well  it's entirely heuristic based it's not being  
learned with a neural network and it's also  even more importantly perhaps perhaps it's  
detached from the feature extraction part so  our feature extraction is learning one piece  
but our region proposal piece of the network or  of this architecture is completely detached so  
the model cannot learn to predict regions  that may be specific to a given task that  
makes it very brittle for some applications  now many variants have been proposed to  
actually tackle and tackle some of these issues  and advance this forward to accomplish object  
detection but i'd like to touch on one extremely  quickly just to point you on in this direction  
for those of you who are interested and that's the  faster rcnn method to actually learn these region  
proposals the idea here is instead of feeding  in this image to a heuristic based feedback or  
region proposal network or method we can have a  part of our network that is trained to identify  
the proposal regions of our model of our image  and that allows us to directly understand or  
identify these regions in our original image where  there are candidate patches that we should explore  
for our classification and our for our object  detection now each of these regions then are  
processed with their own feature extractor as  part of our neural network and individuals or  
in their cnn heads then after these features for  each of these proposals are extracted we can do  
a normal classification over each of these  individual regions very similar as before  
but now the huge advantage of this is that it only  requires a single forward pass through the model  
we only feed in this image once we have a region  proposal network that extracts the regions  
and all of these regions are fed on to perform  classification on the rest of the image  
so it's super super fast compared to the previous  method so in classification we predict one class  
for an entire image of the model in object  detection we predict bounding boxes over all  
of the objects in order to localize them and  identify them we can go even further than this  
and in this idea we're still using cnns to predict  this predict this output as well but instead of  
predicting bounding boxes which are rather coarse  we can task our network to also here predict  
an entire image as well now one example  of this would be for semantic segmentation  
where the input is an rgb an image just a normal  rgb image and the output would be pixel-wise  
probabilities for every single pixel what is  the probability that it belongs to a given class  
so here you can see an example of this image  of some two cows on the on some grass being  
fed into the neural network and the neural  network actually predicts a brand new image  
but now this image is not an rgb image it's a  semantic segmentation image it has a probability  
for every single pixel it's doing a classification  problem and it's learning to classify every single  
pixel depending on what class it thinks it is  and here we can actually see how the cow pixels  
are being classified separately from the grass  pixels and sky pixels and this output is actually  
created using an up sampling operation not a down  sampling operation but up sampling to allow the  
convolutional decoder to actually increase its  spatial dimension now these layers are the analog  
you could say of the normal convolutional layers  that we learned about earlier in the lecture  
they're also already implemented in tensorflow so  it's very easy to just drop these into your model  
and allow your model to learn how to actually  predict full images in addition or instead of  
single class probabilities this semantic  segmentation idea is extremely powerful  
because it can be also applied to many  different applications in healthcare as well  
especially for segmenting for example  cancerous regions on medical scans or  
even identifying parts of the blood that are  infected with diseases like in this case malaria
let's see one final example here of how we can  use convolutional feature extraction to perform  
yet another task this task is different from  the first three that we saw with classification  
object detection and semantic segmentation now  we're going to consider the task of continuous  
robotic control here for self-driving cars  and navigating directly from raw vision data  
specifically this model is going to take as  input as you can see on the top left hand side  
the raw perception from the vehicle this is  coming for example from a camera on the car and  
it's also going to see a noisy representation of  street view maps something that you might see for  
example from google maps on your smartphone and it  will be tasked not to predict the classification  
problem or object detection but rather learn  a full probability distribution over the space  
of all possible control commands that this  vehicle could take in this given situation  
now how does it do that actually this entire model  is actually using everything that we learned about  
in this lecture today it can be trained end to  end by passing each of these cameras through their  
dedicated convolutional feature extractors and  then basically extracting all of those features  
and then concatenating them flattening them down  and then concatenating them into a single feature  
extraction vector so once we have this entire  representation of all of the features extracted  
from all of our cameras and our maps we can  actually use this representation to predict the  
full control parameters on top of a deterministic  control given to the desired destination of the  
vehicle this probabilistic control is very  powerful because here we're actually learning  
to just optimize a probability distribution over  where the vehicle should steer at any given time  
you can actually see this probability distribution  visualized on this map and it's optimized simply  
by the negative log likelihood which is the  negative log likelihood of this distribution which  
is a normal a mixture of normal distributions and  this is nearly identical to how you operate in  
classification as well in that domain you  try to minimize the cross-entropy loss  
which is also a negative log likelihood  optim or probability function  
so keep in mind here that this is composed of  the convolutional layers to actually perform this  
feature extraction these are exactly the same  as what we learned about in this lecture today  
as well as these flattening pooling layers and  concatenation layers to really produce this single  
representation and feature vector of our inputs  and finally it predicts these outputs in this case  
a continuous representation of control that this  vehicle should take so this is really powerful  
because a human can actually enter the car input  a desired destination and the end to end cnn will  
output the control commands to actuate the vehicle  towards that destination note here that the  
vehicle is able to successfully recognize when it  approaches the intersections and take the correct  
control commands to actually navigate that  vehicle through these brand new environments  
that it has never seen before and never  driven before in its training data set  
and the impact of cnns has been very wide  reaching beyond these examples as well that  
i've explained here today it has touched so many  different fields in computer vision especially  
and i'd like to really conclude this lecture  today by taking a look at what we've covered we've  
really covered a ton of material today we covered  the foundations of computer vision how images are  
represented as an array of brightness values and  how we can use convolutions and how they work  
we saw that we can build up these convolutions  into the basic architecture defining convolutional  
neural networks and discussed how  cnns can be used for classification  
finally we talked about a lot of the extensions  and applications of how you can use these basic  
convolutional neural network architectures as  a feature extraction module and then use this  
to perform your task at hand and a bit about  how we can actually visualize the behavior  
of our neural network and actually understand a  bit about what it's doing under the hood through  
ways of some of these semantic segmentation maps  and really getting a more fine-grained perspective  
of the the very high resolution classification  of these input images that it's seeing  
and with that i would like to conclude this  lecture and point everyone to the next lab  
that will be upcoming today this will be a  lab specifically focused on computer vision  
you'll get very familiar with a lot of the  algorithms that we've been talking about today  
starting with building your first convolutional  neural networks and then building this up to build  
some facial detection systems and learn how we  can use unsupervised generative models like we're  
going to see in the next lecture to actually  make sure that these computer vision facial  
classification algorithms are fair and unbiased  so stay tuned for the next lecture as well on  
unsupervised generative modeling to get more  details on how to do a second part thank you
Hi everyone and welcome to lecture 4 of MIT  6.S191! In today's lecture we're going to be  
talking about how we can use deep learning and  neural networks to build systems that not only  
look for patterns in data but actually can go a  step beyond this to generate brand new synthetic  
examples based on those learned patterns and  this i think is an incredibly powerful idea  
and it's a particular subfield of deep  learning that has enjoyed a lot of success and  
and gotten a lot of interest in the past couple  of years but i think there's still tremendous  
tremendous potential of this field of degenerative  modeling in the future and in the years to come  
particularly as we see these types of models and  the types of problems that they tackle becoming  
more and more relevant in a variety of application  areas all right so to get started i'd like to  
consider a quick question for each of you here we  have three photos of faces and i want you all to  
take a moment look at these faces study them and  think about which of these faces you think is real  
is it the face on the left is it the face in the  center is it the face on the right which of these  
is real well in truth each of these faces are  not real they're all fake these are all images  
that were synthetically generated by a deep  neural network none of these people actually  
exist in the real world and hopefully i think you  all have appreciated the realism of each of these  
synthetic images and this to me highlights the  incredible power of deep generative modeling and  
not only does it highlight the power of these  types of algorithms and these types of models  
but it raises a lot of questions about how we can  consider the fair use and the ethical use of such  
algorithms as they are being deployed in the  real world so by setting this up and motivating  
in this way i first i now like to take a step  back and consider fundamentally what is the type  
of learning that can occur when we are training  neural networks to perform tasks such as these so  
so far in this course we've been considering  what we call supervised learning problems  
instances in which we are given a set of data and  a set of labels associated with that data and we  
our goal is to learn a functional mapping that  moves from data to labels and those labels can  
be class labels or continuous values and in  this course we've been concerned primarily with  
developing these functional mappings that  can be described by deep neural networks  
but at their core these mappings could be  anything you know any sort of statistical function  
the topic of today's lecture is going to focus on  what we call unsupervised learning which is a new  
class of learning problems and in contrast to  supervised settings where we're given data and  
labels in unsupervised learning we're given only  data no labels and our goal is to train a machine  
learning or deep learning model to understand  or build up a representation of the hidden and  
underlying structure in that data and what this  can do is it can allow sort of an insight into the  
foundational structure of the data and then in  turn we can use this understanding to actually  
generate synthetic examples and unsupervised  learning beyond this domain of deep generative  
modeling also extends to other types of problems  and example applications which you may be  
familiar with such as clustering algorithms or  dimensionality reduction algorithms generative  
modeling is one example of unsupervised learning  and our goal in this case is to take as input  
examples from a training set and learn a model  that represents the distribution of the data that  
is input to that model and this can be achieved  in two principle ways the first is through what  
is called density estimation where let's say we  are given a set of data samples and they fall  
according to some density the task for building  a deep generative model applied to these samples  
is to learn the underlying probability density  function that describes how and where these data  
fall along this distribution and we can not only  just estimate the density of such a probability  
density function but actually use this information  to generate new synthetic samples where again  
we are considering some input examples that fall  and are drawn from some training data distribution  
and after building up a model using that data  our goal is now to generate synthetic examples  
that can be described as falling within  the data distribution modeled by our model  
so the key the key idea in both these  instances is this question of how can we learn  
a probability distribution using our  model which we call p model of x that is  
so similar to the true data distribution  which we call p data of x this will not only  
enable us to effectively estimate these  probability density functions but also generate  
new synthetic samples that are realistic and match  the distribution of the data we're considering  
so this this i think summarizes concretely what  are the key principles behind generative modeling  
but to understand how generative modeling may  be informative and also impactful let's take  
this this idea a step further and consider what  could be potential impactful applications and  
real world use cases of generative modeling what  generative models enable us as the users to do is  
to automatically uncover the underlying structure  and features in a data set the reason this can be  
really important and really powerful is often we  do not know how those features are distributed  
within a particular data set of interest so  let's say we're trying to build up a facial  
detection classifier and we're given a data set  of faces which for which we may not know the exact  
distribution of these faces with respect to key  features like skin tone or pose or clothing items  
and without going through our data set and  manually ex inspecting each of these instances  
our training data may actually be very biased with  respect to some of these features without us even  
knowing it and as you'll see in this lecture and  in today's lab what we can actually do is train  
generative models that can automatically learn  the landscape of the features in a data set like  
these like that of faces and by doing so actually  uncover the regions of the training distribution  
that are underrepresented and over represented  with respect to particular features such as skin  
tone and the reason why this is so powerful is we  can actually now use this information to actually  
adjust how the data is sampled during training  to ultimately build up a more fair and more  
representative data set that then will  lead to a more fair and unbiased model  
and you'll get practice doing exactly this and  implementing this idea in today's lab exercise  
another great example in use case where generative  models are exceptionally powerful is this broad  
class of problems that can be considered outlier  or anomaly detection one example is in the case of  
self-driving cars where it's going to be really  critical to ensure that an autonomous vehicle  
governed and operated by a deep neural network  is able to handle all all of the cases that it  
may encounter on the road not just you know the  straight freeway driving that is going to be the  
majority of the training data and the majority  of the time the car experiences on the road so  
generative models can actually be used to detect  outliers within training distributions and use  
this to again improve the training process so  that the resulting model can be better equipped to  
handle these edge cases and rare events all right  so hopefully that motivates why and how generative  
models can be exceptionally powerful and useful  for a variety of real world applications to dive  
into the bulk of the technical content for today's  lecture we're going to discuss two classes of what  
we call latent variable models specifically we'll  look at autoencoders and generative adversarial  
networks organs but before we get into that i'd  like to first begin by discussing why these are  
called latent variable models and what we  actually mean when we use this word latent  
and to do so i think really the best example that  i've personally come across for understanding what  
a latent variable is is this story that is from  plato's work the republic and this story is called  
the myth of the cave or the parable of the cave  and the story is as follows in this myth there  
are a group of prisoners and these prisoners are  constrained as part of their prison punishment to  
face a wall and the only things that they can  see on this wall are the shadows of particular  
objects that are being passed in front of a fire  that's behind them so behind their heads and out  
of their line of sight and the prisoners the only  thing they're really observing are these shadows  
on the wall and so to them that's what they can  see that's what they can measure and that's what  
they can give names to that's really their reality  these are their observed variables but they can't  
actually directly observe or measure the physical  objects themselves that are actually casting these  
shadows so those objects are effectively  what we can analyze like latent variables  
they're the variables that are not directly  observable but they're the true explanatory  
factors that are creating the observable  variables which in this case the prisoners  
are seeing like the shadows cast on the wall  and so our question in in generative modeling  
broadly is to find ways of actually learning these  underlying and hidden latent variables in the data  
even when we're only given the observations that  are made and it's this is an extremely extremely  
complex problem that is very well suited to  learning by neural networks because of their power  
to handle multi-dimensional data sets and to learn  combinations of non-linear functions that can  
approximate really complex data distributions all  right so we'll first begin by discussing a simple  
and foundational generative model which tries  to build up this latent variable representation  
by actually self-encoding the input and  these models are known as auto encoders  
what an auto encoder is is it's an approach  for learning a lower dimensional latent space  
from raw data to understand how it works  what we do is we feed in as input raw data  
for example this image of a two that's going to be  passed through many successive deep neural network  
layers and at the output of that succession  of neural network layers what we're going to  
generate is a low dimensional latent space a  feature representation and that's really the  
goal that we're we're trying to predict and so we  can call this portion of the network an encoder  
since it's mapping the data x into a encoded  vector of latent variables z so let's consider  
this this latent space z if you've noticed  i've represented z as having a smaller size  
a smaller dimensionality as the input x why would  it be important to ensure the low dimensionality  
of this latent space z having a low dimensional  latent space means that we are able to compress  
the data which in the case of image data can be  you know on the order of many many many dimensions  
we can compress the data into a small latent  vector where we can learn a very compact and rich  
feature representation so how can we actually  train this model are we going to have are we going  
to be able to supervise for the particular  latent variables that we're interested in  
well remember that this is an unsupervised problem  where we have training data but no labels for  
the latent space z so in order to actually train  such a model what we can do is learn a decoder  
network and build up a decoder network that is  used to actually reconstruct the original image  
starting from this lower dimensional latent  space and again this decoder portion of our auto  
encoder network is going to be a series of layers  neural network layers like convolutional layers  
that's going to then take this hidden latent  vector and map it back up to the input space  
and we call our reconstructed output x hat  because it's our prediction and it's an imperfect  
reconstruction of our input x and the way that we  can actually train this network is by looking at  
the original input x and our reconstructed output  x hat and simply comparing the two and minimizing  
the distance between these two images so for  example we could consider the mean squared error  
which in the case of images means effectively  subtracting one image from another and squaring  
the difference right which is effectively  the pixel wise difference between the input  
and reconstruction measuring how faithful our  reconstruction is to the original input and again  
notice that by using this reconstruction loss  this difference between the reconstructed output  
and our original input we do not require any  labels for our data beyond the data itself right  
so we can simplify this diagram just a little bit  by abstracting away these individual layers in the  
encoder and decoder components and again note once  again that this loss function does not require any  
labels it is just using the raw data to supervise  itself on the output and this is a truly powerful  
idea and a transformative idea because it enables  the model to learn a quantity the latent variables  
z that we're fundamentally interested in but we  cannot simply observe or cannot readily model  
and when we constrain this this latent space to  a lower dimensionality that affects the degree  
to which and the faithfulness to which  we can actually reconstruct the input  
and what this you the way you  can think of this is as imposing  
a sort of information bottleneck during  the model's training and learning process  
and effectively what this bottleneck does is it's  a form of compression right we're taking the input  
data compressing it down to a much smaller latent  space and then building back up a reconstruction  
and in practice what this results in is that the  lower the dimensionality of your latent space  
the poorer and worse quality reconstruction  you're going to get out all right so in summary  
these autoencoder structures use this sort of  bottlenecking hidden layer to learn a compressed  
latent representation of the data and we can  self-supervise the training of this network  
by using what we call a reconstruction loss  that forces the forces the autoencoder network  
to encode as much information about the data as  possible into a lower dimensional latent space  
while still being able to build up faithful  reconstructions so the way i like to think of  
this is automatically encoding information from  the data into a lower dimensional latent space
let's now expand upon this idea a bit more  and introduce this concept and architecture of  
variational auto encoders or vaes so as  we just saw traditional auto encoders  
go from input to reconstructed output and if  we pay closer attention to this latent layer  
denoted here in orange what you can hopefully  realize is that this is just a normal layer  
in a neural network just like any other layer  it's deterministic if you're going to feed in  
a particular input to this network you're going  to get the same output so long as the weights are  
the same so effectively a traditional auto  encoder learns this deterministic encoding  
which allows for reconstruction and  reproduction of the input in contrast  
variational auto encoders impose a stochastic  or variational twist on this architecture  
and the idea behind doing so is to generate  smoother representations of the input data  
and improve the quality of the of not only of  reconstructions but also to actually generate  
new images that are similar to the input data set  but not direct reconstructions of the input data  
and the way this is achieved is  that variational autoencoders  
replace that deterministic layer z  with a stochastic sampling operation  
what this means is that instead of learning the  latent variables z directly for each variable  
the variational autoencoder learns a mean and  a variance associated with that latent variable  
and what those means and variances do is that  they parameterize a probability distribution for  
that latent variable so what we've done in going  from an autoencoder to a variational autoencoder  
is going from a vector of latent variable z  to learning a vector of means mu and a vector  
of variances sigma sigma squared  that parametrize these variables  
and define probability distributions for each of  our latent variables and the way we can actually  
generate new data instances is by sampling  from the distribution defined by these muse  
and sigmas to to generate a latent sample and get  probabilistic representations of the latent space  
and what i'd like you to appreciate about this  network architecture is that it's very similar to  
the autoencoder i previously introduced just that  we have this probabilistic twist where we're now  
performing the sampling operation to compute  samples from each of the latent variables  
all right so now because we've introduced this  sampling operation this stochasticity into our  
model what this means for the actual computation  and learning process of the network the encoder  
and decoder is that they're now probabilistic in  their nature and the way you can think of this is  
that our encoder is going to be trying to learn  a probability distribution of the latent space  
z given the input data x while the decoder is  going to take that learned latent representation  
and compute a new probability distribution of the  input x given that latent distribution z and these  
networks the encoder the decoder are going to be  defined by separate sets of weights phi and theta  
and the way that we can train  this variational autoencoder  
is by defining a loss function that's going to be  a function of the data x as well as these sets of  
weights phi and theta and what's key to how vaes  can be optimized is that this loss function is now  
comprised of two terms instead of just one we  have the reconstruction loss just as before which  
again is going to capture the this difference  between the input and the reconstructed output  
and also a new term to our loss which we call  the regularization loss also called the vae loss  
and to take a look in more  detail at what each of these  
loss terms represents let's first emphasize again  that our overall loss function is going to be  
defined and uh taken with respect to the sets of  weights of the encoder and decoder and the input x  
the reconstruction loss is very similar to before  right and you can think of it as being driven by  
log likelihood a log likelihood  function for example for image  
data the mean squared error between the input  and the output and we can self-supervise the  
reconstruction loss just as before to force  the latent space to learn and represent  
faithful representations of the input data  ultimately resulting in faithful reconstructions  
the new term here the regularization term  is a bit more interesting and completely  
new at this stage so we're going to dive in  and discuss it further in a bit more detail  
so our probability distribution that's going  to be computed by our encoder q phi of z of x  
is a distribution on the latent space z given  the data x and what regularization enforces  
is that as a part of this learning process we're  going to place a prior on the latent space z  
which is effectively some initial hypothesis about  what we expect the distributions of z to actually  
look like and by imposing this regularization  term what we can achieve is that the model will  
try to enforce the zs that it learns  to follow this prior distribution and  
we're going to denote this prior as p of z  this term here d is the regularization term  
and what it's going to do is it's going to try  to enforce a minimization of the divergence or  
the difference between what the encoder is trying  to infer the probability distribution of z given x  
and that prior that we're going to place on  the latent variables p of z and the idea here  
is that by imposing this regularization factor we  can try to keep the network from overfitting on  
certain parts of the latent space by enforcing  the fact that we want to encourage the latent  
variables to adopt a distribution that's similar  to our prior so we're going to go through now  
you know both the mathematical basis for this  regularization term as well as a really intuitive  
walk through of what regularization  achieves to help give you a concrete  
understanding and an intuitive understanding  about why regularization is important and why  
placing a prior is important so let's first  consider um yeah so to re-emphasize once  
again this this regularization term  is going to consider the divergence  
between our inferred latent distribution  and the fixed prior we're going to place  
so before we to to get into this let's consider  what could be a good choice of prior for each  
of these latent uh variables how do we select  p of z i'll first tell you what's commonly done  
the common choice that's used very extensively in  the community is to enforce the latent variables  
to roughly follow normal gaussian distributions  which means that they're going to be a normal  
distribution centered around mean 0 and have a  standard deviation and variance of 1. by placing  
these normal gaussian priors on each of the latent  variables and therefore on our latent distribution  
overall what this encourages is that the learned  encodings learned by the encoder portion of our  
vae are going to be sort of distributed evenly  around the center of each of the latent variables  
and if you can imagine in picture when you have  sort of a roughly even distribution around the  
center of a particular region of the latent space  what this means is that outside of this region  
far away there's going to be a greater penalty  and this can result in instances from instances  
where the network is trying to cheat and  try to cluster particular points outside  
the center these centers in the latent space  like if it was trying to memorize particular  
outliers or edge cases in the data after we place  a normal gaussian prior on our latent variables  
we can now begin to concretely define the  regularization term component of our loss function  
this loss this term to the loss is very similar  in principle to a cross-entropy loss that we saw  
before where the key is that we're going to be  defining the distance function that describes the  
difference or the or the divergence between the  inferred latent distribution q phi of z given x  
and the prior that we're going to be placing p of  z and this term is called the kublac libor or kl  
divergence and when we choose a normal gaussian  prior we res this results in the kl divergence  
taking this particular form of this equation  here where we're using the means and sigmas  
as input and computing this distance metric  that captures the divergence of that learned  
latent variable distribution from the normal  gaussian all right so now i really want to  
spend a bit of time to get some build up  some intuition about how this regularization  
and works and why we actually want to regularize  our vae and then also why we select a normal prior  
all right so to do this let's let's  consider the following question  
what properties do we want this  to achieve from regularization  
why are we actually regularizing our our network  in the first place the first key property  
that we want for a generative model like a vae is  what i can what i like to think of as continuity  
which means that if there are points that  are represented closely in the latent space  
they should also result in similar reconstructions  similar outputs similar content after they are  
decoded you would expect intuitively that regions  in the latent space have some notion of distance  
or similarity to each other and this indeed is  a really key property that we want to achieve  
with our generative model the second property is  completeness and it's very related to continuity  
and what this means is that when we sample from  the latent space to decode the latent space  
into an output that should result in a meaningful  reconstruction and meaningful sampled content  
that is you know resembling  the original data distribution  
you can imagine that if we're sampling from the  latent space and just getting garbage out that  
has no relationship to our input this could be  a huge huge problem for our model all right so  
with these two properties in mind continuity and  completeness let's consider the consequences of  
what can occur if we do not regularize our model  well without regularization what could end up  
happening with respect to these two properties  is that there could be instances of points that  
are close in latent space but not similarly  decoded so i'm using this really intuitive in  
illustration where these dots represent abstracted  away sort of regions in the latent space  
and the shapes that they relate to you can think  of as what is going to be decoded after those  
uh instances in the latent space are passed  through the decoder so in this example we have  
these two dots the greenish dot and the reddish  dot that are physically close in latent space but  
result in completely different shapes when they're  decoded we also have an instance of this purple  
point which when it's decoded it doesn't result  in a meaningful content it's just a scribble so  
by not regularizing and i'm abstracting a  lot away here and that's on purpose we could  
have these instances where we don't have  continuity and we don't have completeness  
therefore our goal with regularization is to be  able to realize a model where points that are  
close in the latent space are not only similarly  decoded but also meaningfully decoded so for  
example here we have the red dot and the orange  dot which result in both triangle like shapes but  
with slight variations on the on the triangle  itself so this is the intuition about what  
regularization can enable us to achieve and what  are desired properties for these generative models  
okay how can we actually achieve this  regularization and how does the normal prior  
fit in as i mentioned right vaes they don't just  learn the latent variable z directly they're  
trying to encode the inputs as distributions  that are defined by mean and variance so my first  
question to you is is it going to be sufficient  to just learn mean and variance learn these  
distributions can that guarantee continuity  and completeness no and let's understand why  
all right without any sort of regularization  what could the model try to resort to  
remember that the vae or or that the vae the loss  function is defined by both a reconstruction term  
and a regularization term if there is no  regularization you can bet that the model is going  
to just try to optimize that reconstruction term  so it's effectively going to learn to minimize the  
reconstruction loss even though we're encoding  the latent variables via mean and variance  
and two instances two consequences of that  is that you can have instances where these  
learned variances for the latent variable  end up being very very very small  
effectively resulting in pointed distributions and  you can also have means that are totally divergent  
from each other which result in discontinuities  in the latent space and this can occur while  
still trying to optimize that reconstruction  loss direct consequence of not regularizing
by in order to overcome these pro these problems  we need to regularize the variance and the mean  
of these distributions that are being returned  by the encoder and the normal prior placing that  
normal gaussian distribution as our prior helps  us achieve this and to understand why exactly this  
occurs is that effectively the normal prior is  going to encourage these learned latent variable  
distributions to overlap in latent space recall  right mean zero variance of one that means all the  
all the latent variables are going to be enforced  to try to have the same mean a centered mean and  
all their variances are going to be regularized  for each and every of the latent variable  
distributions and so this will ensure a smoothness  and a regularity and an overlap in the lane space  
which will be very effective in helping us achieve  these properties of continuity and completeness  
centering the means regularizing the variances  
so the regularization via this normal prior  by centering each of these latent variables  
regularizing their their variances is that  it helps enforce this continuous and complete  
gradient of information represented  in the latent space where again points  
and distances in the latent space have  some relationship to the reconstructions  
and the content of the reconstructions that result  note though that there's going to be a trade-off  
between regularizing and reconstructing the more  we regularize there's also a risk of suffering the  
quality of the reconstruction and the generation  process itself so in optimizing gaze there's going  
to be this trade-off that's going to try  to be tuned to fit the problem of interest  
all right so hopefully by walking through this  this example and considering these points you've  
built up more intuition about why regularization  is important and how specifically the normal prior  
can help us regularize great so now we've defined  our loss function we know that we can reconstruct  
the inputs we've understood how we can regularize  learning and achieve continuity and completeness  
by this normal prior these are all the components  that define a forward pass through the network  
going from input to encoding  to decoded reconstruction  
but we're still missing a critical step  in putting the whole picture together  
and that's of back propagation and the key  here is that because of this fact that we've  
introduced this stochastic sampling layer we  now have a problem where we can't back propagate  
gradients through a sampling layer  that has this element of stochasticity  
backpropagation requires deterministic nodes  deterministic layers for which we can iteratively  
apply the chain rule to optimize gradients  optimize the loss via gradient descent all right  
vaes introduced sort of a breakthrough idea  that solved this issue of not being able to  
back propagate through a sampling layer  and the key idea was to actually subtly  
re-parametrize the sampling operation such that  the network could be trained completely end-to-end  
so as we as we already learned right  we're trying to build up this latent  
distribution defined by these variables  z uh defining placing a normal prior  
defined by a mean and a variance and we can't  simply back propagate gradients through the  
sampling layer because we can't compute  gradients through this stochastic sample  
the key idea instead is to try to consider the  sampled latent vector z as a sum defined by a  
fixed mu a fixed sigma vector and scale that sigma  vector by random constants that are going to be  
drawn from a prior distribution such as a normal  gaussian and by reparameterizing the sampling  
operation as as so we still have this element of  stochasticity but that stochasticity is introduced  
by this random constant epsilon which  is not occurring within the bottleneck  
latent layer itself we've reparametrized and  distributed it elsewhere to visualize how this  
looks let's consider the following where  originally in the original form of the
vae we had this deterministic nodes which are  the weights of the network as well as an input  
vector and we are trying to back propagate  through the stochastic sampling node z  
but we can't do that so now by  re-parametrization what we've  
we've achieved is the following form where our  latent variable z are defined with respect to  
uh mu sigma squared as well as these noise  factor epsilon such that when we want to do  
back propagation through the network to update we  can directly back propagate through z defined by  
mu and sigma squared because this epsilon value  is just taken as a constant it's re-parametrized  
elsewhere and this is a very very powerful  trick the re-parametrization trick because  
it enables us to train variational auto encoders  and to end by back propagating with respect to z  
and with respect to the actual gradient the  actual weights of the encoder network all right  
one side effect and one consequence of imposing  these distributional priors on the latent variable  
is that we can actually sample from these latent  variables and individually tune them while keeping  
all of the other variables fixed and what  you can do is you can tune the value of  
a particular latent variable and run the  decoder each time that variable is changed  
each time that variable is perturbed to  generate a new reconstructed output so an  
example of that result is is in the following  where this perturbation of the latent variables  
results in a representation that has some semantic  meaning about what the network is maybe learning  
so in this example these images  show variation in head pose  
and the different dimensions of z the latent  space the different latent variables are in  
this way encoding different latent features  that can be interpreted by keeping all other  
variables fixed and perturbing the  value of one individual lane variable  
ideally in order to optimize vas and try to  maximize the information that they encode we want  
these latent variables to be uncorrelated with  each other effectively disentangled and what that  
could enable us to achieve is to learn the richest  and most compact latent representation possible  
so in this case we have head pose on the x-axis  and smile on the y-axis and we want these to be  
as uncorrelated with each other as possible one  way we can achieve this that's been shown to  
achieve this disentanglement is rather a quite  straightforward approach called beta vaes so if  
we consider the loss of a standard vae again we  have this reconstruction term defined by a log  
likelihood and a regularization term defined  by the kl divergence beta vaes introduce a new  
hyperparameter beta which controls the strength  of this regularization term and it's been shown  
mathematically that by increasing beta the effect  is to place constraints on the latent encoding  
such as to encourage disentanglement and there  have been extensive proofs and discussions  
as to how exactly this is achieved but to  consider the results let's again consider  
the problem of face reconstruction where using a  standard vae if we consider the latent variable of  
head pose or rotation in this case where beta  equals one what you can hopefully appreciate  
is that as the face pose is changing the  smile of some of these faces is also changing  
in contrast by en enforcing a beta much larger  than one what is able to be achieved is that  
the smile remains relatively constant while  we can perturb the single latent variable  
of the head of rotation and achieve  perturbations with respect to head rotation alone  
all right so as i motivated and introduced in the  beginning in the introduction of this lecture one  
powerful application of generative models and  latent variable models is in model d biasing  
and in today's lab you're actually going to get  real hands-on experience in building a variational  
auto encoder that can be used to achieve automatic  de-biasing of facial classification systems facial  
detection systems and the power and the idea of  this approach is to build up a representation  
a learned latent distribution of face data and  use this to identify regions of that latent space  
that are going to be over-represented or  under-represented and that's going to all be  
taken with respect to particular learned features  such as skin tone pose objects clothing and then  
from these learned distributions we can actually  adjust the training process such that we can  
place greater weight and greater sampling on those  images and on those faces that fall in the regions  
of the latent space that are under represented  automatically and what's really really cool  
about deploying a vae or a latent variable model  for an application like model d biasing is that  
there's no need for us to annotate and prescribe  the features that are important to actually devise  
against the model learns them automatically and  this is going to be the topic of today's lab  
and it's also raises the opens the door to a much  broader space that's going to be explored further  
in a later spotlight lecture that's going to focus  on algorithmic bias and machine learning fairness  
all right so to summarize the key points on vaes  they compress representation of data into an  
encoded representation reconstruction of the data  input allows for unsupervised learning without  
labels we can use the reparameterization trick  to train vas end to end we can take hidden latent  
variables perturb them to interpret their content  and their meaning and finally we can sample  
from the latent space to generate new examples  but what if we wanted to focus on generating  
samples and synthetic samples that were as  faithful to a data distribution generally as  
possible to understand how we can achieve  this we're going to transition to discuss  
a new type of generative model called a  generative adversarial network or gam for short  
the idea here is that we don't want  to explicitly model the density or the  
or the distribution underlying some data but  instead just learn a representation that can  
be successful in generating new instances that  are similar to the data which means that we  
want to optimize to sample from a very very  complex distribution which cannot be learned and  
modeled directly instead we're going to have to  build up some approximation of this distribution  
and the really cool and and breakthrough idea  of gans is to start from something extremely  
extremely simple just random noise and try  to build a neural network a generative neural  
network that can learn a functional transformation  that goes from noise to the data distribution  
and by learning this functional generative mapping  we can then sample in order to generate fake  
instances synthetic instances that are going  to be as close to the real data distribution  
as possible the breakthrough to achieving this was  this structure called gans where the key component  
is to have two neural networks a generator network  and a discriminator network that are effectively  
competing against each other they're adverse areas  specifically we have a generator network which i'm  
going to denote here on out by g that's going to  be trained to go from random noise to produce an  
imitation of the data and then the discriminator  is going to take that synthetic fake data as  
well as real data and be trained to actually  distinguish between fake and real and in training  
these two networks are going to be competing each  other competing against each other and so in doing  
so overall the effect is that the discriminator is  going to get better and better at learning how to  
classify real and fake and the better it becomes  at doing that it's going to force the generator to  
try to produce better and better synthetic data to  try to fool the discriminator back and forth back  
and forth so let's now break this down and go from  a very simple toy example to get more intuition  
about how these gans work the generator is  going to start again from some completely  
random noise and produce fake data and i'm  going to show that here by representing these  
data as points on a one-dimensional line the  discriminator is then going to see these points  
as well as real data and then it's going  to be trained to output a probability that  
the data it sees are real or if they are fake and  in the beginning it's not going to be trained very  
well right so its predictions are not going to  be very good but then you're going to train it  
and you're going to train it and it's going  to start increasing the profit probabilities  
of real versus not real appropriately such  that you get this perfect separation where  
the discriminator is able to perfectly distinguish  what is real and what is fake now it's back to the  
generator and the generator is going to come back  it's going to take instances of where the real  
data lie as inputs to train and then it's going to  try to improve its imitation of the data trying to  
move the fake data the synthetic data that is  generated closer and closer to the real data  
and once again the discriminator is  now going to receive these new points  
and it's going to estimate a probability  that each of these points is real  
and again learn to decrease the  probability of the fake points being real  
further and further and now we're going to repeat  again and one last time the generator is going to  
start moving these fake points closer and  closer to the real data such that the fake  
data are almost following the distribution of the  real data at this point it's going to be really  
really hard for the discriminator to effectively  distinguish between what is real and what is fake  
while the generator is going to continue  to try to create fake data instances to  
fool the discriminator and this is really the key  intuition behind how these two components of gans  
are essentially competing with each other  all right so to summarize how we train gowns  
the generator is going to try to synthesize fake  instances to full discriminator which is going  
to be trained to identify the synthesized  instances and discriminate these as fake  
to actually train we're going to see that we  are going to define a loss function that defines  
competing and adversarial objectives for each of  the discriminator and the generator and a global  
optimum the best we could possibly do would mean  that the generator could perfectly reproduce the  
true data distribution such that the discriminator  absolutely cannot tell what's synthetic  
versus what's real so let's go through how  the loss function for again breaks down  
the the loss term for again is based  on that familiar cross-entropy loss and  
it's going to now be defined between  the true and generated distributions  
so we're first going to consider the loss  from the perspective of the discriminator  
we want to try to maximize the probability  that the fake data is identified as fake  
and so to break this down here g of z  defines the generator's output and so d  
of g of z is the discriminator's estimate of the  probability that a fake instance is actually fake  
d of x is the discriminator's estimate of  the probability that a real instance is fake  
so one minus d of x is its probability  estimate that a real instance is real  
so together from the point of view of the  discriminator we want to maximize this probability  
maximize probability fake is fake maximize  the estimate of probability really is real  
now let's turn our attention to the  generator remember that the generator  
is taking random noise and generating an instance  it cannot directly affect the term d of x  
which shows up in the loss right because d of x is  solely based on the discriminator's operation on  
the real data so for the generator the generator  is going to have the adversarial objective  
to the discriminator which means is going to  try to minimize this term effectively minimizing  
the probability that the discriminator can  distinguish its generated data as as uh fake  
d of g of z and the goal for the generator  is to minimize this term of the objective
so the objective of the generator is to try  to synthesize fake instances that fool the  
discriminator and eventually over the course  of training the discriminator the discriminator  
is going to be as best as it possibly can be  discriminating real versus fake therefore the  
ultimate goal of the generator is to synthesize  fake instances that fool the best discriminator  
and this is all put together in this min max  objective function which has these two components  
optimized adversarially and then after training  we can actually use the generator network which  
is now fully trained to produce new data instances  that have never been seen before so we're going  
to focus on that now and what is really cool is  that when the train generator of a gam synthesizes  
new instances it's effectively learning a  transformation from a distribution of noise to a  
target data distribution and that transformation  that mapping is going to be what's learned over  
the course of training so if we consider one point  from a latent noise distribution it's going to  
result in a particular output in the target  data space and if we consider another point  
of random noise feed it through the generator it's  going to result in a new instance that and that  
new instance is going to fall somewhere else on  the data manifold and indeed what we can actually  
do is interpolate and trans and traverse in the  space of gaussian noise to result in interpolation  
in the target space and you can see an example of  this result here where a transformation in series  
reflects a traversal across the alert the  target data manifold and that's produced in  
the synthetic examples that are outputted by the  generator all right so in the final few minutes of  
this lecture i'm going to highlight some of the  recent advances in gans and hopefully motivate  
even further why this approach is so powerful so  one idea that's been extremely extremely powerful  
is this idea of progressive gans progressive  growing which means that we can iteratively build  
more detail into the generated instances that are  produced and this is done by progressively adding  
layers of increasing spatial resolution in  the case of image data and by incrementally  
building up both the generator and discriminator  networks in this way as training progresses it  
results in very well resolved synthetic images  that are output ultimately by the generator  
so some results of this idea of progressive a  progressive gan are displayed here another idea  
that has also led to tremendous improvement in the  quality of synthetic examples generated by gans  
is a architecture improvement called stylegan  which combines this idea of progressive growing  
that i introduced earlier with a principles of  style transfer which means trying to compose an  
image in the style of another image so for  example what we can now achieve is to map  
input images source a using application of coarse  grained styles from secondary sources onto those  
targets to generate new instances that mimic the  style of of source b and that's that result is  
shown here and hopefully you can appreciate that  these coarse-grained features these coarse-grained  
styles like age facial structure things like that  can be reflected in these synthetic examples this  
same style gan system has led to tremendously  realistic synthetic images in the areas of both  
face synthesis as well as for animals other  objects as well another extension to the gan  
architecture that's has enabled particularly  powerful applications for select problems and  
tasks is this idea of conditioning which imposes  a bit of additional further structure on the types  
of outputs that can be synthesized by again so the  idea here is to condition on a particular label  
by supplying what is called a conditioning  factor denoted here as c and what this allows  
us to achieve is instances like that of  paired translation in the case of image  
synthesis where now instead of a single input  as training data for our generator we have pairs  
of inputs so for example here we consider both a  driving scene and a corresponding segmentation map  
to that driving scene and the discriminator can  in turn be trained to classify fake and real pairs  
of data and again the generator is going to be  learned to going to be trained to try to fool the  
discriminator example applications of this idea  are seen as follows where we can now go from a  
input of a semantic segmentation map to generate  a synthetic street scene mapping that mapping um  
according to that segmentation or we can go  from an aerial view from a satellite image  
to a street map view or from particular labels  of an architectural building to a synthetic  
architectural facade or day to night black  and white to color edges to photos different  
instances of paired translation that are  achieved by conditioning on particular labels  
so another example which i think is really  cool and interesting is translating from  
google street view to a satellite view and vice  versa and we can also achieve this dynamically  
so for example in coloring given an edge input the  network can be trained to actually synthetically  
color in the artwork that is resulting from  this particular edge sketch another idea  
instead of pair translation is that of unpaired  image to image translation and this is can be  
achieved by a network architecture called cyclegan  where the model is taking as input images from  
one domain and is able to learn a mapping that  translates to another domain without having a  
paired corresponding image in that other domain  so the idea here is to transfer the style and  
the the distribution from one domain to another  and this is achieved by introducing the cyclic  
relationship in a cyclic loss function where  we can go back and forth between a domain x  
and a domain y and in this system there are  actually two generators and two discriminators  
that are going to be trained on their  respective generation and discrimination tasks  
in this example the cyclogan has been trained to  try to translate from the domain of horses to the  
domain of zebras and hopefully you can appreciate  that in this example there's a transformation of  
the skin of the horse from brown to a zebra-like  skin in stripes and beyond this there's also a  
transformation of the surrounding area from  green grass to something that's more brown  
in the case of the zebra i think to get  an intuition about how this cyclogan  
transformation is going is working let's  go back to the idea that conventional gans  
are moving from a distribution of gaussian noise  to some target data manifold with cycle gans the  
goal is to go from a particular data manifold x  to another data manifold why and in both cases and  
i think the underlying concept that makes gans  so powerful is that they function as very very  
effective distribution transform transformers and  it can achieve these distribution transformations
finally i'd like to consider one additional  application that you may be familiar with  
of using cycle gans and that's to transform  speech and to actually use this psychogam  
technique to synthesize speech in someone  else's voice and the way this is done is  
by taking a bunch of audio recordings in one  voice and audio recordings in another voice and  
converting those audio waveforms into an image  spec representation which is called a spectrogram  
we can then uh train the cycle gan to operate  on these spectrogram images to transform  
representations from voice a to make like to  make them appear like they appear that they  
are from another voice voice be and  this is exactly how we did the speech  
transformation for the synthesis of obama's voice  in the demonstration that alexander gave in the  
first lecture so to inspect this further let's  compare side by side the original audio from  
alexander as well as the synthesized version in  obama's voice that was generated using a cyclegan  
hi everybody and welcome to mit 6s191 the official  introductory course on deep learning taught here  
at mit so notice that the spectrogram that results  for obama's voice is actually generated by an  
operation on alexander's voice and effectively  learning a domain transformation from obama domain  
onto the domain of alexander domain and the end  result is that we create and synthesize something  
that's more obama-like all right so to summarize  hopefully over the course of this lecture you  
built up understanding of generative modeling and  classes of generative models that are particularly  
powerful in enabling probabilistic density  estimation as well as sample generation  
and with that i'd like to close the lecture and  introduce you to the remainder of of today's  
course which is going to focus on our second  lab on computer vision specifically exploring  
this question of de-biasing in facial  detection systems and using variational  
auto encoders to actually achieve an approach for  automatic de-biasing of classification systems  
so i encourage you to come to the class  gather town to have your questions  
on the labs answered and to discuss  further with any of us thank you
Hi everyone and welcome back to 6.S191! Today is  a really exciting day because we'll learn about  
how we can marry the very long-standing field  of reinforcement learning with a lot of the very  
recent advancements that we've been seeing so  far in this class in deep learning and how we  
can combine these two fields to build some really  extraordinary applications and really agents that  
can outperform or achieve super human performance  now i think this field is particularly amazing  
because it moves away from this paradigm that  we've been really constrained to so far in this  
class so so far deep learning the way we've seen  it has been really confined to fixed data sets the  
way we kind of either collect or have can obtain  online for example in reinforcement learning  
though deep learning is placed in some environment  and is actually able to explore and interact with  
that environment and it's able to learn how to  best accomplish its goal usually does this without  
any human supervision or guidance which makes  it extremely powerful and very flexible as well
this has huge obvious impact in fields  like robotics self-driving cars and robot  
manipulation but it also has really  revolutionized the world of gameplay  
and strategic planning and it's this really  connection between the real world and deep  
learning the virtual world that makes this  particularly exciting to me and and i hope  
this video that i'm going to show  you next really conveys that as well  
starcraft has imperfect information and is played  in real time it also requires long-term planning  
and the ability to choose what action to take from  millions and millions of possibilities i'm hoping  
for a 5-0 not to lose any games but i think the  realistic goal would be four and one in my favor
i think he looks more confident  than __ was quite nervous before  
the room was much more tense this time  
really didn't know what to expect he's been  playing starcraft pretty much since his fight
i wasn't expecting the ai to be that good  everything that he did was proper it was  
calculated and it was done well  i thought i'm learning something
it's much better than i expected it i  would consider myself a good player right  
but i lost every single one of five games
all right so in fact this is an example of  how deep learning was used to compete against  
humans professionally trained game players  and was actually trained to not only compete  
against them but it was able to achieve remarkably  superhuman performance beating this professional  
uh starcraft player five games to zero so let's  start by taking a step back and really seeing how  
reinforcement learning fits within respect to all  the other types of learning problems that we have  
seen so far in this class so the first piece and  the most comprehensive piece of learning problems  
that we have been exploring so far in this class  has been that of supervised learning problems  
so this was kind of what we talked about  in the first second and third lectures  
and in this domain we're basically given a  bunch of data x and we try to learn a neural  
network to predict its label y so this goal is  to learn this functional mapping from x to y  
and i like to describe this very intuitively  if if i give you a picture of this apple for  
example i want to train a neural network to  determine and tell me that this thing is an apple  
okay the next class of algorithms that we  actually learned about in the last lecture was  
unsupervised learning so in this case we  were only given data with no labels so a  
bunch of images for example of apples and we were  forced to learn a neural network or learn a model  
that represented this underlying structure in  the data set so again in the apple scenario  
we tried to learn a model that says back to us  if we show it these two pictures of apples that  
these things are basically like each other  we don't know that they're apples because we  
were never given any labels that explicitly  tell the model that this thing is an apple  
but we can tell that oh this thing is pretty  close to this other thing that it's also  
seen and it can pick out those underlying  structure between the two to identify that  
now in the last part in rl and reinforcement  learning which is what today's lecture is  
going to be focused on we're given only data in  the form of what we call state action pairs now  
states are what are the observations of the system  and the actions are the behaviors that that system  
takes or that agent takes when it sees those  states now the goal of rl is very different than  
the goal of supervised learning and the goal  of unsupervised learning the goal of rl is to  
maximize the reward or the future reward of that  agent in that environment over many time steps  
so again going back to the apple example what the  analog would be would be that the agent should  
learn that it should eat this thing because it  knows that it will keep you alive it will make you  
healthier and needs and you need food to survive  again like the unsupervised case it doesn't know  
that this thing is an apple it doesn't even  recognize exactly what it is all it knows is  
that in the past they must have eaten it and it  was able to survive longer and because it was a  
piece of food it was able to to become healthier  for example and through these state action pairs  
and somewhat trial and error it was able to  learn these representations and learn these plans
so our focus today will be explicitly on  this third class of learning problems and  
reinforcement learning so to do that i think it's  really important before we start diving into the  
details and the nitty-gritty technical details  i think it's really important for us to build up  
some key vocabulary that is very important  in reinforcement learning and it's going  
to be really essential for us to build up on  each of these points later on in the lecture  
this is very important part of the lecture so  i really want to go slowly through this section  
so that the rest of the lecture is going to make  as much sense as possible so let's start with the  
central part the core of your reinforcement  learning algorithm and that is your agent  
now the agent is something that can take actions  in the environment that could be things like a  
drone making a delivery in the world it could be  super mario navigating a video game the algorithm  
in reinforcement learning is your agent and you  could say in real life the agent is each of you  
okay the next piece is the environment the  environment is simply the world in which the  
agent lives it's the place where the agent exists  and operates and and conducts all of these actions  
and that's exactly the connection between  the two of them the agent can send commands  
to the environment in forms of actions  now capital a or lowercase a of t is  
the action at time t that the agent takes  in this environment we can denote capital a  
as the action space this is the set of all  possible actions that an agent can make now  
i do want to say this even though i think it  is somewhat self-explanatory an action is uh or  
the list by which an action can be chosen the set  of all possible actions that an agent can make in  
the environment can be either discrete or it can  be from a set of actions in this case we can see  
the actions are forwards right backwards or left  or it could also be continuous actions for example  
the exact location in the environment a real as  a real number coordinate for example like the  
gps coordinates where this agent wants to move  right so it could be discrete as a categorical  
or a discrete probability distribution or it  could be continuous in either of these cases
observations are how the environment interacts  back with the agent it's how the agent the agent  
can observe where in the environment it is and  how its actions affected its own state in the  
environment and that leads me very nicely into  this next point the state is actually a concrete  
and immediate situation in which the agent finds  itself so for example a state could be something  
like a image feed that you see through your eyes  this is the state of the world as you observe it
a reward now is also a way of feedback from the  environment to the agent and it's a way that the  
environment can provide a way of feedback to  measure the success or failure of an agent's  
actions so for example in a video game when  mario touches a coin he wins points and from  
a given state an agent will send out outputs  in the form of actions to the environment and  
the environment will respond with the agent's new  state the next state that it can can achieve which  
resulted on acting on that previous  state as well as any rewards that may  
be collected or penalized by reaching  that state now it's important to note here  
that rewards can be either immediate or delayed  uh they basically you should think of rewards as  
effectively evaluating that agent's actions but  you may not actually get a reward until very late  
in life so for example you might take many  different actions and then be rewarded a  
long time into the future that's called a very  delayed reward but it is a reward nonetheless  
we can also look at what's called the total reward  which is just the sum of all rewards that an agent  
gets or collects at after a certain time t so r  of i is the reward at time i and r of capital r of  
t is just the return the total reward from time t  all the way into the future so until time infinity  
and that can actually be written now expanded we  can we can expand out that summation uh from our  
from r of t plus r of t plus one all the way on  into the future so it's adding up all of those  
uh rewards that the agent collects from this  point on into the future however often it's  
it's very common to consider not just the summed  return the total return as a straight-up summation  
but actually instead what we call the discounted  sum of rewards now this discounting factor which  
is represented here of by gamma is multiplied  by the future awards that are discovered by the  
agent in order to dampen those rewards effect  on the agent's choice of action now why would  
we want to do this so this is actually this this  formulation was created by design to make future  
rewards less important than immediate rewards  in other words it enforces a kind of short-term  
learning in the agent a concrete example of  this would be if i offered to give you five  
dollars today or five dollars in five years from  today which would you take even though it's both  
five dollars your reward would be the same but  you would prefer to have that five dollars today  
just because you prefer short-term rewards over  long-term rewards and again like before we can  
expand the summation out now with this discount  factor which has to be typically between zero  
and one and the discount factor is multiplied by  these future rewards as discovered by the agent  
and like i said it is reinforcing this  this concept that we want to prioritize  
these short-term rewards more than uh  very long-term rewards in the future  
now finally there's a very very important function  in rl that's going to kind of start to put a lot  
of these pieces together and that's called the q  function and now let's look at actually how this  
q function is defined remembering the definition  of this total discounted reward capital r of t  
so remember the total reward r of t measures the  discounted sum of rewards obtained since time t  
so now the q function is very related to that  the q function is a function that takes as input  
the current state that the agent is in and the  action that the agent takes in that state and  
then it returns the expected total future reward  that the agent can receive after that point so  
think of this as if an agent finds itself in some  state in the world and it takes some action what  
is the expected return that it can receive after  that point that is what the q function tells us  
let's suppose i give you this magical q function  this is actually it really is a magical function  
because it tells us a lot about the problem  if i give you this function an oracle that  
you can plug in any state and action pair and it  will tell you this expected return from point time  
point t from your current time point i give you  that function the question is can you determine  
given the state that you're currently in can  you determine what is the best action to take  
you can perform any queries on this function  and the way you can simply do this is  
that you ultimately in your mind you want to  select the best action to take in your current  
state but what is the best action to take well  it's simply the action that results in the highest  
expected total return so what you can do is  simply choose a policy that maximizes this future  
reward or this future return well  that can be simply written by finding  
the arg max of your q function over all  possible actions that you can take at the state  
so simply if i give you this q function and a  state that you're in you can feed in your state  
with every single action and evaluate what the q  function would tell you the expected total reward  
would be given that state action pair and you  pick the action that gives you the highest q value  
that's the best action to take in this current  state so you can build up this policy which here  
we're calling pi of s to infer this best  action to take so think of your policy now  
as another function that takes this  input your state and it tells you  
the action that you should execute in this in  the state so the strategy given a q function to  
compute your policy is simply from this arcmax  formulation find the action that maximizes  
your q function now in this lecture you're going  to we're going to focus on basically these two  
classes of reinforcement learning algorithms  into two categories one of which will actually  
try to learn this q function q of s your state  and your action and the other one will be called  
what are called policy learning algorithms because  they try to directly learn the policy instead of  
using a q function to infer your policy now we're  going to in policy learning directly infer your  
policy pi of s that governs what actions you  should take this is a much more direct way of  
thinking about the problem but first thing we're  going to do is focus on the value learning problem  
and how we can do what is called q learning  and then we'll build up to policy learning  
after that let's start by digging a bit deeper  into this q function so first i'll introduce this  
game of atari breakout on the left for those who  haven't seen it i'll give a brief introduction  
into how the game works now the q value tells  us exactly what the expected total expected  
return that we can expect to see on any state in  this game and this is an example of one state so  
in this game you are the agent is this paddle  on the bottom of the board it's this red paddle  
it can move left or right and those are its two  actions it can also stay constant in the same  
place so it has three actions in the environment  there's also this ball which is traveling in this  
case down towards the bottom of the the board  and is about to hit and ricochet off of this  
paddle now the objective the goal of this game is  actually to move the pedal back and forth and hit  
the ball at the best time such that you can bounce  it off and hit and break out all of these colored  
blocks at the top of the board each time the ball  touches one of these colored blocks it's able to  
break them out hence the name of the the name of  the game breakout and the goal is to knock off  
as many of these as possible each time the ball  touches one it's gone and you got to keep moving  
around hitting the ball until you knock off all  of the of the blocks now the q function tells us  
actually what is the expected total return that  we can expect in a given state action pair and  
the point i'd like to make here is that it's  actually it can sometimes be very challenging to  
understand or intuitively guess what is the uh  q value for a given state action pair so even if  
let's say i give you these two state action pairs  option a and option b and i ask you which one out  
of these two pairs do you think has a higher q  value on option a we can see the ball is already  
traveling towards the paddle and the paddle is  choosing to stay in the same place it's probably  
going to hit the ball and it'll bounce back up  and and break some blocks on the second option  
we can see the ball coming in at an angle and the  paddle moving towards the right to hit that ball  
and i asked you which of these two options  state action pairs do you believe will  
return the higher expected total reward before  i give you the answer i want to tell you  
a bit about what these two policies actually  look like when they play the game instead of  
just seeing this single state action pair so let's  take a look first at option a so option a is this  
this relatively conservative option that doesn't  move when the ball is traveling right towards it  
and what you can see is that as it plays the game  it starts to actually does pretty well it starts  
to hit off a lot of the breakout pieces towards  the center of the game and it actually does pretty  
well it breaks out a lot of the ball a lot of the  colored blocks in this game but let's take a look  
also at option b option b actually does something  really interesting it really likes to hit  
the ball at the corner of the paddle uh it does  this just so the ball can ricochet off at an  
extreme angle and break off colors in the corner  of the screen now this is actually it does this  
to the extreme actually because it even even in  the case where the ball is coming right towards  
it it will move out of the way just so it can come  in and hit it at these extreme ricocheting angles  
so let's take a look at how option  b performs when it plays the game
and you can see it's really targeting the side of  the paddle and hitting off a lot of those colored  
blocks and why because now you can see once  it breaks out the corner the two edges of the  
screen it was able to knock off a ton of blocks  in the game because it was able to basically  
get stuck in that top region let's take another  look at this so once it gets stuck in that top  
region it doesn't have to worry about making any  actions anymore because it's just accumulating a  
ton of rewards right now and this is a very great  policy to learn because it's able to beat the game  
much much faster than option a and with much less  effort as well so the answer to the question which  
stay action pair has a higher q value in this case  is option b but that's a relatively unintuitive  
option at least for me when i first saw this  problem because i would have expected that  
playing things i mean not moving out of the way of  the ball when it's coming right towards you would  
be a better action but this agent actually has  learned to move away from the ball just so it can  
come back and hit it and really attack at extreme  angles that's a very interesting observation  
that this agent has made through learning now the  question is how can we use because q values are so  
difficult to actually define it's hard for humans  to define them as we saw in the previous example  
instead of having humans define that q value  function how can we use deep neural networks to  
model this function and learn it instead so in  this case uh well the q value is this function  
that takes this input to state in action so one  thing we could do is have a deep neural network  
that gets inputs of both its state and the desired  action that it's considering to make in that state  
then the network would be trained to predict the  q value for that given state action pair that's  
just a single number the problem with this is  that it can be rather inefficient to actually  
run forward in time because if remember how we  compute the policy for this model if we want to  
predict what is the optimal action that it should  take in this given state we need to evaluate this  
deep q network n times where n is the number  of possible actions that it can make in this  
time step this means that we basically have to run  this network many times for each time step just to  
compute what is the optimal action and this can be  rather inefficient instead what we can do which is  
very equivalent to this idea but just formulate  it slightly differently is that it's often much  
more convenient to output all of the q values at  once so you input the state here and you output  
a basically a vector of q values instead of one q  value now it's a vector of q values that you would  
expect to see for each of these states so the q  value for state sorry for each of these actions  
so the q value for action one the q value for  action two all the way up to your final action  
so for each of these actions and given  the state that you're currently in  
the this output of the network tells you what  is the optimal set of actions or what is the  
the breakdown of the q values given these  different actions that could be taken  
now how can we actually train this version of  the deep queue network we know that we wanted  
to output these what are called q values but it's  not actually clear how we can train it and to do  
this and well this is actually challenging  you can think of it conceptually as because  
we don't have a data set of q values right all  we have are observations state action and reward  
triplets so to do this and to train this type  of deep queue network we have to think about  
what is the best case scenario how would the  agent perform optimally or perform ideally  
what would happen if it takes all the best  actions like we can see here well this would mean  
that that the target return would be maximized  and what what we can do in this case is we can  
actually use this exact target return to serve  as our ground truth our data set in some sense  
in order to actually train this agent to train  this deep q network now what that looks like  
is first we'll formulate our expected return  if we were to take all of the best actions the  
initial reward r plus the action that we select  that maximizes the expected return for the next  
future state and then we apply that discounting  factor gamma so this is our target this is our our  
q value that we're going to try and uh optimize  towards it's like the what we're trying to  
match right that's what we want our prediction  to mac match but now we should ask ourselves  
what does our network predict well our  network is predicting like we can see  
in this in this network the network is predicting  the q value for a given state action pair  
well we can use these two pieces of information  both our predicted q value and our target q value  
to train and create this what we call q loss this  is a essentially a mean squared error formulation  
between our target and our predicted q values and  we can use that to train this deep queue network
so in summary let's uh walk through this  whole process end to end of deep q learning  
our deep neural network sees as input a state  and the state gets fed through the network and  
we try to output the q value for each pause  of the three possible actions here there are  
three different ways that the network  can play you can either move to the left  
move to the right or it can  stay constant in the same place
now in order to infer the optimal policy  it has to look at each of these q values  
so in this case moving to the left because it sees  that the ball is moving to the left it can it sees  
that okay if i step a little bit to the left i  have a higher chance of probably hitting that  
ball and and continuing the game so my q value  for my expected total reward return my q value  
for moving left is 20. on the other hand if i stay  in the same place let's say i have a q value of 3  
and if i move to the right out of the way of the  ball in this case because the ball is already  
moving towards me i have a q value of 0.  so these are all my q values for all of the  
different possible actions how do i compute  the optimal policy well as we saw before  
the optimal policy is obtained by looking at  the maximum q value and picking the action  
that maximizes our q value so in this case we can  see that the maximum q value is attained when we  
move to the left with action one so we we  select action one we feed this back into  
the game engine we send this back to the  environment and we receive our next state  
and this process repeats all over again the next  state is fed through the deep neural network  
and we obtain a list of q values for each of the  possible actions and it repeats again now deepmind  
showed that these networks deep queue networks  could be applied to solve a variety of different  
types of atari games not just breakout but many  other games as well and basically all they needed  
to do was provide the state pictorially as an  input passing them through these convolutional  
layers followed by non-linearities and pooling  operations like we learned in lecture three  
and at the right hand side it's predicting these q  values for each possible action that it could take  
and it's exactly as we saw in the previous  couple slides so it picks an optimal action  
to execute on the next time step depending on  the maximum q value that could be attained and  
it sends this back to the environment to execute  and and receive its next state this is actually  
remarkably simple because despite its remarkable  simplicity in my opinion of essentially trial and  
error they tested this on many many games in atari  and showed that for over 50 percent of the games  
they were able to surpass human level performance  with this technique and other games which you can  
see on the right hand side of this plot were more  challenging but still again given how simple this  
technique is and how clean it is it really  is amazing that this works at all to me  
so despite all of the advantages  of this approach the simplicity  
the the cleanness and how elegant the solution  is i think it's and uh i mean above all that  
the ability for this solution to learn superhuman  uh policies policies that can beat humans even on  
some relatively simple tasks there are some  very important downsides to queue learning  
so the first of which is the simplistic model  that we learned about today this model can only  
handle action spaces which are discrete and it  can only really handle them when the action space  
is small so when we're only given a few possible  actions as each step it cannot handle continuous  
action spaces so for example if an autonomous  vehicle wants to predict where to go in the world  
instead of predicting to go left right or  go straight these are discrete categories  
how can we use reinforcement learning to learn a  continuous steering wheel angle one that's not not  
discretized into bins but can take  any real number within some bound  
of where the steering wheel angle can execute this  is a continuous variable it has an infinite space  
and it would not be possible in the version of q  learning that we presented here in this lecture  
it's also its flexibility of q learning is also  somewhat limited because it's not able to learn  
policies that can be stochastic that can change  according to some unseen probability distribution  
so they're deterministically computed from the  q function through this maximum formulation it  
always is going to pick the maximum the action  that maximally elevates your expected return  
so it can't really learn from these stochastic  policies on the other hand to address these  
we're really going to dive into this next phase of  today's lecture focused on policy gradient methods  
which will hopefully we'll see  tackle these remaining issues  
so let's dive in the key difference now  between what we've seen in the first part  
of the lecture and the second part that we're  going to see is that in value learning we try  
to have a neural network to learn our q value  q of our state given or action and then we use  
this q value to infer the best action to take  given a state that we're in that's our policy  
now policy learning is a bit different it tries  to now directly learn the policy using our neural  
network so it inputs the state and it tries to  directly learn the policy that will tell us which  
action we should take this is a lot simpler since  this means we now get directly the action for free  
by simply sampling straight away from this policy  function that we can learn so now let's dive into  
the the details of how policy learning works  and and first i want to really really narrow  
or sorry drive in this difference from q learning  because it is a subtle difference but it's a very  
very important difference so deep q networks  aim to approximate this q function again  
by first predicting given a state the q value for  each possible action and then it simply picks the  
best action where best here is described by which  action gives you the maximum q value the maximum  
expected return and execute that that action now  policy learning the key idea of policy learning is  
to instead of predicting the q values we're going  to directly optimize the policy pi of s so this  
is the policy distribution directly governing  how we should act given a current state that  
we find ourselves in so the output here  is for us to give us the desired action  
in a much more direct way the outputs represent  the probability that the action that we're going  
to sample or select should be the correct action  that we should take at this step right in other  
words it will be the one that gives us the maximum  reward so take for example this if we see that we  
predict these probabilities of these given actions  being the optimal action so we get the state and  
our policy network now is predicting a probability  distribution of uh we can basically aggregate them  
into our policy so we can say our policy is  now defined by this probability distribution  
and now to compute the action that we should  take we simply sample from this distribution  
to predict the action that we should execute in  this case it's the car going left which is a1  
but since this is a probability distribution  the next time we sample we might not we might  
get to stay in the same place we might sample  action 2 for example because this does have a  
nonzero probability a probability of 0.1 now note  that because this is a probability distribution  
this this p of actions given our state must  sum to one now what are some of the advantages  
of this type of formulation over first of all  over q learning like we saw before besides the  
fact that it's just a much more direct way to get  what we want instead of optimizing a q function  
and then using the q function to create our policy  now we're going to directly optimize the policy  
beyond that though there is one very important  advantage of this formulation and that is that it  
can handle continuous action spaces so this was an  example of a discrete action space what we've been  
working with so far in this atari breakout game  moving left moving right or staying in the center  
there are three actions and they're discrete  there's a finite number of actions here that can  
be taken for example this is showing the prob our  action space here is representing the direction  
that i should move but instead a continuous action  space would tell us not just the direction but  
how fast for example as a real number that  i should move questions like that that are  
infinite in the number of possible answers  this could be one meter per second to the left  
half a meter per second to left or any  numeric velocity it also tells us direction  
by nature through plus or minus sign so if i say  minus one meter per second it tells me that i want  
to move to left at one meter per second if i say  positive one it tells me i want to move to the  
right at one meter per second but now when  we plot this as a probability distribution  
we can also visualize this as a continuous action  space and simply we can visualize this using  
something like a gaussian distribution in this  case but it could take many different you can  
choose the type of distribution that fits best  with your problem set gaussian is a popular choice  
here because of its simplicity so here again we  can see that the probability of moving to the left  
faster to the left is much greater than moving  faster to the right and we can actually see that  
the mean of this distribution the average the  point where this normal distribution is highest  
tells us an exact numerical value of how fast it  should be moving not just that it should be moving  
to the left but how fast it should be moving  to the left now let's take a look at how we can  
actually model these continuous action spaces with  a policy gradient method instead of predicting  
the probability of taking an action given  a possible state which in this case since  
we're in the continuous domain there would be an  infinite number of actions let's assume that our  
output distribution is actually a normal gaussian  an output a mean and a variance for that gaussian  
then we only have two outputs but it allows us  to describe this probability distribution over  
the entire continuous space which otherwise would  have been infinite an infinite number of outputs  
so in this case that's if we predict that the mean  that uh the mean action that we should take mu  
is negative one and the variance is of 0.5 we can  see that this probability distribution looks like  
this on the bottom left-hand side it should move  to the left with an average speed of negative  
one meters per second and with some variance so  it's not totally confident that that's the best  
speed at which it should move to the left but  it's pretty set on that being the the place  
to move to the left so for this picture we can  see that the paddle needs to move to the left  
if we actually plot this distribution like this  and we can actually see that the mass of the  
distribution does lie on the left-hand side of  the number line and if we sample for example from  
this distribution we can actually see that in this  case we're getting that the action that we should  
take the concrete velocity that should be executed  would indicate that we need to move left negative  
at a speed of 0.8 meters per second so again  that means that we're moving left with a  
speed of 0.8 meters per second note here that  even though the the mean of this distribution  
is negative one we're not constrained to that  exact number this is a continuous probability  
distribution so here we sampled an action that  was not exactly the mean but that's totally fine  
and that really highlights that the difference  here between the discrete action space and the  
continuous action space this opens up a ton  of possibilities for applications where we  
do model infinite numbers of actions and again  like before like the discrete action case this  
probability distribution still has all of the  nice properties of probability distributions  
namely that the integral of this computed  probability distribution does still sum to one  
so we can indeed sample from it which  is a very nice confirmation property  
okay great so let's take a look now of how  the policy gradients algorithm works in a  
concrete example let's start by revisiting this  whole learning loop of reinforcement learning  
again that we saw in the very beginning of  this lecture and let's think of how we can  
use the policy gradient algorithm that we have  introduced to actually train an autonomous vehicle  
using using this trial and error policy gradient  method so with this case study study of autonomous  
vehicles or self-driving cars what are all of  these components so the agent would be our vehicle  
it's traveling in the environment which is the  the world the lane that it's it's traveling  
in it has some state that is obtained through  camera data lidar data radar data et cetera  
it obtains sorry it makes actions what are the  actions that it can take the actions in this  
case are the steering wheel angle again this is a  concrete example of a continuous action space you  
don't discretize your steering wheel angle into  unique bins your steering wheel angle is infinite  
in the number of possibilities it can take and  it can take any real number between some bounds  
so that is a continuous problem that is a  continuous variable that we're trying to  
model through this action and finally uh it  receives rewards in the in the form of the  
distance it can travel before it needs to  be uh needs some form of human intervention  
so let's now dive in now that we've  identified all of those so how can we train  
this car using policy gradient network in this  context here we're taking self-driving cars as  
an example but you can hopefully see that we're  only using this because it's nice and intuitive  
but this will also apply to really any domain  where you can identify and set up the problem  
like we've set up the problem so far so let's  start by initializing our agent again our agent  
is the vehicle we can place it onto the road in  the center of the road the next step would be to  
let the agent run in the beginning it doesn't  run very well because it crashes and well it's  
never been trained before so we don't expect it  to run very well but that's okay because this is  
reinforcement learning so we run that policy until  it terminates in this case we mark terminations by  
the time that it it crashes and needs to be  taken over along that uh what we call rollout  
we start to record all of the state action pairs  or sorry state action reward pairs so at each step  
we're going to record where was the robot what  was it state what was the action that it executed  
and what was the reward that it obtained  by executing that action in that state now
next step would be to take all of  those state action reward pairs  
and actually decrease the probability of taking  any action that it took close to the time  
where the terminated determination happened so  close to the time where the crash occurred we  
want to decrease the probability of making  any of those actions again in the future  
likewise we want to increase the probability  of making any actions in the beginning of this  
episode note here that we don't necessarily  know that there was something good in this  
first part of the episode we're just assuming  that because the crash occurred in the second  
part of the episode that was likely due to an  action that occurred in that second part this  
is a very unintelligent if you could say algorithm  because it that's all it assumes it just tries to  
decrease the probability of anything that resulted  in a low reward and increase the probability of  
anything that resulted in a high reward it doesn't  know that any of these actions were better than  
the other especially in the beginning because  it doesn't have that kind of feedback this is  
just saying that we want to decrease anything that  may have been bad and increase anything that would  
have been good and if we do this again we can see  that the next time the car runs it runs for a bit  
longer and if we do it again we do the same thing  now on this rollout we decrease the probability of  
actions that resulted in low reward and increase  the probability that resulted in positive or  
high reward we reinitialize this and we run it  until completion and update the policy again  
and it seems to run a bit longer and we  can do this again and we keep doing this  
until eventually it learns to start to follow the  lanes without crashing and this is really awesome  
i think because we never taught this vehicle how  anything well we never taught it anything about  
lanes we never taught it what a lane marker is  it learns to avoid lanes though and not crash  
and not crash just by observing very sparse  rewards of crashing so it observed a lot of  
crashes and it learned to say like okay i'm not  going to do any of these actions that occurred  
very close to my crashes and just by observing  those things it was able to successfully avoid  
lanes and survive in this environment longer and  longer times now the remaining question is how  
we can actually update our policy on every  training iteration to decrease the probability of  
bad events and increase the probability of these  good events or these good actions let's call them  
so that really focuses and narrows us into points  four and five in this this training algorithm  
how can we do this learning process of decreasing  these probabilities when it's bad and increasing  
the probabilities when they're good let's  take a look at that in a bit more detail  
so let's look at specifically the loss function  for training policy gradients and then we'll  
dissect it to understand exactly why this works  so this loss consists of really two parts that  
i'd like to dive into the first term is this log  likelihood term the log likelihood of our pro of  
our our policy our probability of an action  given our state the second term is where we  
multiply this negative log likelihood by the total  discounted reward or the total discounted return  
excuse me r of t so let's assume that we get a  lot of reward for an action that had very high  
log likelihood this loss will be great and it  will reinforce these actions because they resulted  
in very good returns on the other hand if the  reward is very low for an action that it had high  
probability for it will adjust those probabilities  such that that action should not be sampled again  
in the future because it did not result in  a desirable return so when we plug in these  
uh this loss to the gradient descent algorithm to  train our neural network we can actually see that
the policy gradient term here  which is highlighted in blue  
which is where this this algorithm gets  its name it's the policy because it has to  
compute this gradient over the policy part of this  function and again uh just to reiterate once more  
this policy gradient term consists of these  two parts one is the likelihood of an action  
the second is the reward if the action is very  positive very good resulting in good reward it's  
going to amplify that through this gradient  term if the action is very is very probable  
or sorry not very probable but it did result in  a good reward it will actually amplify it even  
further so something that was not probable before  will become probable because it resulted in a good  
return and vice versa on the other side as well  now i want to talk a little bit about how we can  
extend some of these reinforcement  learning algorithms into real life and  
this is a particularly challenging question  because this is something that has a particular  
interest to the reinforcement learning field  right now and especially right now because  
applying these algorithms in the real world  is something that's very difficult for one  
reason or one main reason and that is this step  right here running a policy until termination  
that's one thing i touched on but i didn't spend  too much time really dissecting it why is this  
difficult well in the real world terminating means  well crashing dying usually pretty bad things  
and we can get around these types of things  usually by training and simulation but then  
the problem is that modern simulators do not  accurately depict the real world and furthermore  
they don't transfer to the real world when  you deploy them so if you train something  
in simulation it will work in simulation it will  work very well in simulation but when you want  
to then take that policy deployed into the real  world it does not work very well now one really  
cool result that we created in my lab was actually  developing a brand new photo realistic simulation  
engine specifically for self-driving cars that i  want to share with you that's entirely data driven  
and enables these types of reinforcement learning  advances in the real world so one really cool  
result that we created was developing this type of  simulation engine here called vista and allows us  
to use real data of the world to simulate brand  new virtual agents inside of the simulation now  
the results here are incredibly photorealistic as  you can see and it allows us to train agents using  
reinforcement learning in simulation using exactly  the methods that we saw today so that they can be  
directly deployed without any transfer learning  or domain adaptation directly into the real world  
now in fact we did exactly this we placed agents  inside of our simulator train them using exactly  
the same policy grading algorithm that we learned  about in this lecture and all of the training was  
done in our simulator then we took these policies  and put them directly in our full-scale autonomous  
vehicle as you can see in this video and on the  left hand side you can actually see me sitting  
in this vehicle in the bottom of the interior  shot you can see me sitting inside this vehicle  
as it travels through the real world completely  autonomous this represented the first time at  
the time of when we published these results the  first time an autonomous vehicle was trained  
using rl entirely in simulation and was able to be  deployed in the real life a really awesome result
so now we have covered the fundamentals behind  value learning as well as policy gradient  
reinforcement learning approaches i think now  it's really important to touch on some of the  
really remarkable deep reinforcement learning  applications that we've seen in recent years  
and for that look we're going to turn first  to the game of go where reinforcement learning  
agents were put to the test against human  champions and achieved what at the time was  
and still is extremely exciting results so first  i want to provide a bit of an introduction to the  
game of go this is a game that consists of 19  by 19 uh grids it's played between two players  
who hold either white pieces or black pieces  and the objective of this game is to occupy  
more board territory with your pieces than your  opponent now even though the grid and the rules  
of the game are very simple the problem of go  solving the game of go and doing it to beat  
the grand masters is an extremely complex problem  it's it's actually because the number of possible  
board positions the number of states that you  can encounter in the game of go is massive  
the full size with the full-size board there  are greater number of legal board positions  
than there are atoms in the universe now the  objective here is to train an ai to train a  
machine learning or deep learning algorithm  that can master the game of go not only to  
beat the existing gold standard software but  also to beat the current world human champions  
now in 2016 google deepmind rose to this challenge  and a couple and several years ago they actually  
developed a reinforcement learning based pipeline  that defeated champion go players and the idea at  
its core is very simple and follows along with  everything that we've learned in this lecture  
today so first a neural network was trained and  it got to watch a lot of human expert go players  
and basically learn to imitate their behaviors  this part was not using reinforcement learning  
this was using supervised learning you basically  got to study a lot of human experts then  
they use these pre-trained networks to play  against reinforcement learning policy networks  
which allows the policy to go beyond what  the human experts did and actually play  
against themselves and achieve actually  superhuman performance in addition to this  
one of the really tricks that brought this to be  possible was the usage of this auxiliary network  
which took the input of the state of the board  as as input and predicted how good of a state  
this was now given that this network the ai  could then hallucinate essentially different  
board position actions that it could take  and evaluate how good these actions would  
be given these predicted values this essentially  allowed it to traverse and plan its way through  
different possible actions that it could take  based on where it could end up in the future
finally a recently published extension of these  approaches just a few years ago in 2018 called  
alpha zero only used self-play and generalized to  three famous board green board games not just go  
but also chess shogi and go and in these examples  the authors demonstrate that it was actually not  
necessary to pre-train these networks from human  experts but instead they optimized them entirely  
from scratch so now this is a purely reinforcement  learning based solution but it was still able to  
not only beat the humans but it also beat the  previous networks that were pre-trained with  
human data now as recently as only last month  very recently the next breakthrough in this  
line of works was released with what is called  mu0 where the algorithm now learned to master  
these environments without even knowing the rules  i think the best way to describe mu0 is to compare  
and contrast its abilities with those previous  advancements that we've already discussed earlier  
already today so we started this discussion  with alphago now this demonstrates superhuman  
performance with go on go using self pro self  play and pre-training these models using human  
grand master data then came alphago 0 which showed  us that even better performance could be achieved  
entirely on its own without pre-training  from the human grandmasters but instead  
directly learning from scratch then came alpha  zero which extended this idea even further  
beyond the game of go and also into chess  and shogi but still required the model to  
know the rule and be given the rules of the  games in order in order to learn from them  
now last month the authors demonstrated  superhuman performance on over 50 games  
all without the algorithm knowing the rules  beforehand it had to learn them as well as  
actually learning how to play the game  optimally during its training process  
now this is critical because in many scenarios  we do not know the rules beforehand to tell the  
model when we are placed in the environment  sometimes the rules are unknown the rules or  
the dynamics are unknown objects may interact  stochastically or unpredictably we may also  
be in an environment where the rules are simply  just too complicated to be described by humans so  
this idea of learning the rules of the game  or of the task is a very very powerful concept  
and let's actually walk through very very briefly  how this works because it's such an awesome  
algorithm but again at its core it really builds  on everything that we've learned today so you  
should be able to understand each part of this of  this algorithm we start by observing the board's  
state and from this point we predict or we perform  a tree search through the different possible  
scenarios that can arise so we take some actions  and we look at the next possible scenarios or the  
next possible states that can arise but now since  we don't know the rules the network is forced to  
learn the dynamics the dynamics model of how to  do this search so to learn what could be the next  
states given the state that it currently sees  itself in and the action that it takes now at  
the base time this gives us this probability of  executing each of these possible actions based on  
the value that it can attain through this branch  of the tree and it uses this to plan the next  
action that it should take this is essentially  the policy network that we've been learning  
about but amplified to also encounter this tree  search algorithm for planning into the future  
now given this policy network it takes this  action and receives a new observation from  
the game and repeats this process over and over  again until of course that it the game finishes or  
the game is over a very similar this is very  very similar to how we saw alpha zero work  
but now the key difference is that the dynamics  model as part of the tree search that we can see  
at each of these steps is entirely learned  and greatly opens up the possibilities for  
these techniques to be applied outside of  rigid game scenarios so in these scenarios  
we do know the rules of the games very well so  we could use them to train our algorithms better  
but in many scenarios this type of advancement  allows us to apply these algorithms to areas  
where we simply don't know the rules and where we  need to learn the rules in order to play the game  
or simply where the rules are much harder  to define which in reality in the real world  
many of the interesting scenarios  this would be exactly the case  
so let's briefly recap with what we've learned in  the lecture today we started with the foundations  
of deep reinforcement learning we defined what  agents are what actions are what environments  
are and how they all interact with each other in  this reinforcement learning loop then we started  
by looking at a broad class of q learning  problems and specifically the deep q network  
where we try to learn a q function given a state  and action pair and then determine a policy by  
selecting the action that maximizes that q  function and finally we learned how we could  
optimize instead of optimizing the q value or the  q function learn to directly optimize the policy  
straight from straight from the state and we saw  that this has really impactful applications in  
continuous action spaces where q functions or  this q learning technique is somewhat limited  
so thank you for attending this lecture on deep  reinforcement learning at this point we'll now be  
taking the next part of the class which we focused  on reinforcement learning and you'll get some  
experience on how you can apply these algorithms  onto all by yourself specifically focusing on  
the policy gradient algorithms in the context  of a very simple example of pong as well as  
more complex examples as well so you'll actually  build up this body and the brain of the agent and  
the environment from scratch and you'll really  get to put together a lot of the ideas that  
we've seen today in this lecture so please  come to the gather town for if you have any  
questions and we'd be happy to discuss questions  on the software lab specifically as well as any  
questions on today's lecture so we look  forward to seeing you there thank you
Hi everyone and welcome to lecture six of MIT  6.S191! This is one of my absolute favorite  
lectures in the course and we're going to  focus and discuss some of the limitations of  
deep learning algorithms as well as some of the  emerging new research frontiers in this field  
before we dive into the technical content  there are some course related and logistical  
announcements that i'd like to make the first  is that our course has a tradition of designing  
and delivering t-shirts to students participating  this year we are going to continue to honor that  
so to that end we have a sign up sheet on canvas  for all students where you can indicate your  
interest in receiving a t-shirt and once you  fill out that sign up sheet with the necessary  
information will ensure that a t-shirt is  delivered to you by the appropriate means  
as soon as possible and if after the class if the  canvas is closed and you can't access that signup  
form please just feel free to send us an email  and we'll find a way to get the t-shirt to you  
so to provide a take a step back and give  an overview of our schedule of this course  
so far where we've been and where we're going  following this lecture on limitations and new  
frontiers we'll have the due date for our final  software lab on reinforcement learning tomorrow  
we're going to have two really exciting hot  topic spotlight lectures with brand new content  
and that will be followed by a series of four  guest lectures you'll have time over the rest  
of this week to continue to work on your final  projects and the class will conclude on friday  
with the student final project presentations  and proposal competition as well as our award  
ceremony so speaking of those final projects  let's get into some details about those for  
those of you taking the course for credit you  have two options to fulfill your grade the first  
is a project proposal where you will work  in up to a group of four to develop a new  
and novel deep learning idea or application and we  realize that two weeks is a very short amount of  
time to come up with and implement a project  so we are certainly going to be taking this  
into consideration in the judging then on friday  january 29th you will give a brief three-minute  
presentation on your project proposal to a group  of judges who will then award the final prizes  
as far as logistics and timelines you  will need to indicate your interest  
in presenting by this wednesday at midnight  eastern time and will need to submit the slide  
for your presentation by midnight eastern time  on thursday instructions for the project proposal  
and submission of these requirements are on  the course syllabus and on the canvas site  
our top winners are going to be awarded  prizes including nvidia gpus and google homes  
the key point that i'd like to make  about the final proposal presentations  
is that in order to participate and be eligible  for the prize synchronous attendance is required  
on friday's course so friday january 29th from 1  to 3 p.m eastern time you will need to be present  
your or your group will need to be present  in order to participate in the final proposal  
competition the second option for  fulfilling the credit requirement  
is to write a one-page review of a deep learning  paper with the evaluation being based on  
the completeness and clarity of your review this  is going to be due by thursday midnight eastern  
time and further information and instruction  on this is also available on canvas so  
after this lecture next we're going to have a  series of two really exciting hot topic spotlight  
talks and these are going to focus on two rapidly  emerging in developing areas within deep learning  
deep learning research the first is going  to highlight a series of approaches called  
evidential deep learning that seeks to develop  algorithms that can actually learn and estimate  
the uncertainties of neural networks and the  second spotlight talk is going to focus on machine  
learning bias and fairness and here we're going to  discuss some of the dangers of implementing biased  
algorithms in society and also emerging strategies  to actually mitigate these unwanted biases  
that will then be followed by a series of  really exciting and awesome guest lectures  
from leading researchers in industry and  academia and specifically we're going to have  
talks that are going to cover a diversity  of topics everything from ai and healthcare  
to document analysis for business applications  and computer vision and we highly highly highly  
encourage you to join synchronously for these  lectures if you can on january 27th and january  
28th from 1 to 3 p.m eastern these are going to  be highlighting very exciting topics and they  
may extend a bit into the designated software lab  time so that we can ensure we can have a live q a  
with our fantastic guest speakers all right  so that concludes the logistical and course  
related announcements let's dive into the fun  stuff and the technical content for this lecture  
so so far in taking success 191 i hope that  you've gotten a sense of how deep learning  
has revolutionized and is revolutionizing so  many different research areas and fields from  
advances in autonomous vehicles to medicine  and healthcare to reinforcement learning  
generative modeling robotics and a variety  of other applications from natural language  
processing to finance and security and alongside  with understanding the tremendous application  
utility and power of deep learning i hope that  you have also established concrete understanding  
of how these algorithms actually work and how  specifically they have enabled these advances
to take a step back at the types of algorithms and  models that we've been considering we've primarily  
dealt with systems that take as input data as the  in the form of signals images other sensory data  
and move forward to produce a decision as the  output this can be a prediction this can be a  
outputted detection it can also be an action as  in the case of reinforcement learning we've also  
considered the inverse problem as in the case  of generative modeling where we can actually  
train neural networks to produce new data  instances and in both these paradigms we can  
really think of neural networks as very powerful  function approximators and this relates back to a  
long-standing theorem in the theory of neural  networks and that's called the universal  
approximation theorem and it was presented in  1989 and generated quite the stir in the community  
and what this theorem the universal approximation  theorem states is that a neural network with a  
single hidden layer is sufficient to approximate  any arbitrary function to any arbitrary position
all it requires is a single layer and  in this class we've primarily dealt with  
deep neural models where we are stacking multiple  hidden layers on top of each other but this  
theorem completely ignores that fact and says  okay we only need one layer so long as we can  
reduce our problem to a set of outputs inputs and  a set of outputs this means there has to exist a  
neural network that can solve this problem it's  a really really powerful and really big statement  
but if you consider this closely there are a  couple of caveats that we have to be aware of  
the first is that this theorem makes no  guarantees on the number of hidden units  
or size of the layer that's  going to be required to solve  
such a problem right and it also leaves open  the question of how we could actually go about  
training such a model finding the weights  to support that architecture it doesn't make  
any claims about that it just says it proves  that one such network exists but as we know  
with gradient descent finding these weights  is highly non-trivial and due to the very  
non-convex nature of the optimization problem  the other critical caveat is that this theorem  
places no guarantees on how well the resulting  model would actually generalize to other tasks  
and indeed i think that this this theorem  the universal approximation theorem points  
to a broader issue that relates to the possible  effects of overhype in artificial intelligence and  
us as a community as students invested  in advancing the state of this field  
i think we need to be really careful in how  we consider and market and advertise these  
algorithms while the universal approximation  theorem was able to generate a lot of excitement  
it also provided a false sense of  hope to the community at the time  
which was that neural networks could be used to  solve any problem and as you can imagine this  
overhype is very very very dangerous and this  over hype has also been tied in to what were two  
historic a.i winters where research in artificial  intelligence and neural networks more specifically  
slowed down very significantly and i think we're  still in this phase of explosive growth which is  
why today for the rest of the lecture i want  to focus in on some of the limitations of the  
algorithms that we've learned about and extend  beyond to discuss how we can go beyond this to  
consider new research frontiers  all right so first the limitations  
one of my favorite and i think one of  the most powerful examples of a potential  
danger and limitation of deep neural networks  come from this paper called understanding deep  
neural networks requires rethinking generalization  and what they did in this paper was a very simple  
experiment they took images from the dataset  imagenet and each of these images are associated  
with a particular class label as seen here and  what they did was they did this experiment where  
for every image in the data set not class but  individual images they flipped a die a k sided  
die where k was the number of possible classes  they were considering and they used this this  
flip of the die to randomly assign a brand new  label to a particular image which meant that these  
new labels associated were completely random with  respect to what was actually present in the image  
so for example a remapping could be visualized  here and note that these two instances of dogs  
have been mapped to different classes altogether  so we're completely randomizing our labels  
what they next did was took this data this  scrambled data and tried to fit a deep neural  
network to the to the imagenet data by applying  varying degrees of randomization from the original  
data with the untouched class labels to the  completely randomized data and as you ex may  
expect the model's accuracy on the test set an  independent test set progressively tended to zero  
as the randomness in the data increased but what  was really interesting was what they observed when  
they looked at the performance on the training  set and this is what they found they found that  
no matter how much they randomized the labels the  model was able to get close to 100 accuracy on the  
training set and what this highlights is that in a  very similar way to the statement of the universal  
approximation theorem it gets at this idea that  deep neural networks can perfectly fit to any  
function even if that function is associated with  entirely random data driven by random labeling
so to draw really drive this point home i  think the best way to consider and understand  
neural networks is as very very good function  approximators and all the universal approximation  
theorem states is that neural networks are very  good at this right so let's suppose here we have  
some data points and we can learn using a neural  network a function that approximates this this  
data and that's going to be based on sort of a  maximum likelihood estimation of the distribution  
of that data what this means is that if we give  the model a new data point shown here in purple  
we can expect that our neural network is going  to predict a maximum likelihood estimate for that  
data point and that estimate is probably going  to lie along this function but what happens now  
if i extend beyond this in distribution region to  now out of domain regions well there are really  
no guarantees on what the data looks like in  this region in these regions and therefore we  
can't make any statements about how our model  is going to behave or perform in these regions  
and this is one of the greatest limitations  that exist with modern deep neural networks  
so there's a revision here to this  statement about neural networks being really  
excellent function approximators they're  really excellent function approximators  
when they have training data and this  also raises the question of what happens  
in these out-of-distribution regions where the  network has not seen training examples before  
how do we know when our network doesn't know  is not confident in the predictions it's making
building off this idea i think there can be this  conception that can be amplified and inflated  
by the media that deep learning is basically  alchemy right it's this magic cure it's this be  
all and all solution that can be applied to any  problem i mean its power really seems awesome and  
i'm almost certain that was probably a draw  for you to attend and take this course but  
you know if we can say that deep learning  algorithms are sort of this be all  
all convincing uh solution that can be applied  to any arbitrary problem or application there's  
this also resulting idea and belief that you can  take some set of training data apply some network  
architecture sort of turn the crank on your  learning algorithm and spit out excellent results  
but that's simply not how deep learning works your  model is only going to be as good as your data  
and as the adage in the community goes if you  put garbage in you're going to get garbage out  
i think an example that really highlights  this limitation is the one that i'm going  
to show you now which emphasizes just how much  these neural network systems depend on the data  
they're trained with so let's say we have this  image of a dog and we're going to pass it into  
a cnn based architecture where our goal is to try  to train a network to take a black and white image  
and colorize it what happened to this image of  a dog when it was passed into this model was  
as follows take a cl close look at this result if  you'll notice under the nose of the dog there's  
this pinkish region in its fur which probably  doesn't make much sense right if if this was just  
a natural dog but why could this be the case why  could our model be spitting out this result well  
if we consider the data that may  have been used to train the network  
it's probably very very likely that amongst  the thousands upon thousands of images of  
dogs that were used to train such a model the  majority or many of those images would have  
dogs sticking their tongues out  right because that's what dogs do  
so the cnn may have mapped that region under the  mouth of the dog to be most likely to be pink  
so when it saw a dog that had its mouth closed  it didn't have its tongue out it assumes in a way  
right or it's it's built up representation is such  that it's going to map that region to a pink color  
and what this highlights is that deep learning  models build up representations based on the  
data they've seen and i think this is a really  critical point as you go out you know you've taken  
this course and you're interested in applying  deep learning perhaps to some applications  
and problems of interest to you your model is  always going to be only as good as your data
and this also raises a question of how do  neural networks handle data instances where  
that they have not encountered before and  this i think is highlighted uh very potently  
by this infamous and tragic example from a  couple years ago where a car from tesla that  
was operating autonomously crashed while operating  autonomously killing the driver and it turned out  
that the driver who was the individual killed  in that crash had actually reported multiple  
instances in the weeks leading up to the crash  where the car was actually swiveling towards  
that exact same barrier into which it crashed why  could it have been doing that well it turned out  
that the images which were representative  of the data on which the car's autonomous  
system was trained the images from that region  of the freeway actually lacked new construction  
that altered the appearance of that barrier  recent construction such that the car  
before it crashed had encountered a data instance  that was effectively out of distribution and it  
did not know how to handle this situation  because i had only seen particular bear a  
particular style and architecture of the barrier  in that instance causing it tragically to crash  
and in this instance it was a a occurrence where  a neural network failure mode resulted in the loss  
of human life and this points these sorts of  failure modes points to and motivate the need  
for really having systematic ways to understand  when the predictions from deep learning models  
cannot be trusted in other words when it's  uncertain in its predictions and this is a very  
exciting and important topic of research in deep  learning and it's going to be the focus of our  
first spotlight talk this notion of uncertainty  is definitely very important for the deployment  
of deep learning systems and what i like  to think of as safety critical applications  
things like autonomous driving things like  medicine facial recognition right as these  
algorithms are interfacing more and more with  human life we really need to have principled ways  
to ensure their robustness uncertainty metrics  are also very useful in cases where we have to  
rely on data sets that may be imbalanced  or have a lot of noise in present in them  
and we'll consider these different use  cases further in the spotlight lecture  
all right so before as a preparation for  tomorrow's spotlight lecture i'd like to  
give a bit of an overview about what uncertainties  we need and what uncertainties we can talk about  
when considering deep learning algorithms so  let's consider this classification problem where  
we're going to try to build a neural network that  models probabilities over a fixed set of classes  
so in this case we're trying to train a neural  network on images of cats images of dogs and  
then output whether a new image has a cat or has  a dog right keep in mind that um the probabilities  
of cat and dog have to sum to one so what happens  when we now train our model we're ready to test it  
and we have an image that contains both a cat and  a dog still the network is going to have to output  
class probabilities that are going to sum to one  but in truth this image has both a cat and a dog  
this is an instance of what we can think about  as noise or stochasticity that's present in the  
data if we train this model on images of cats  alone or dogs alone a new instance that has  
both a dog and a cat is noisy with respect to  what the the model has seen before uncertainty  
metrics can help us assess the noise that's the  statistical noise that's inherent in the data  
and present in the data and this is called  data uncertainty or alliatoric uncertainty  
now let's consider another case let's take our  same cat dog classifier and input now an image  
of a horse to this classifier again the output  probabilities are going to have to sum to one  
but even if the network is predicting that this  image is most likely containing a dog we would  
expect that it should really not be very confident  in this prediction and this is an instance where  
our model is now being tested on an image that's  totally out of distribution an image of a horse  
and therefore we're going to expect that  it's not very confident in its prediction  
this type of uncertainty is a different type  of uncertainty than that data uncertainty  
it's called model or epistemic uncertainty and it  reflects how confident a given prediction is very  
very important for understanding how well neural  networks gener generalized to out of distribution  
regions and how they can report on their  performance in out-of-distribution regions and  
in the spotlight lecture you'll really take a deep  dive into these ideas of uncertainty estimation  
and explore some emerging approaches to actually  learn neural network uncertainties directly
the third failure mode i'd like to  consider is one that i think is super fun  
and also in a way kind of scary and  that's this idea of adversarial examples  
the idea here is we take some input example for  example this image of a temple and a standard cnn  
trained on you know a set of images is  going to classify this particular image  
as a temple with 97 probability we then take that  image and we apply some particular perturbations  
to that image to generate what we call an  adversarial example such that if we now feed  
this perturbed example to that same cnn it  no longer recognizes that image as a temple  
instead it incorrectly classifies this image  as an ostrich which is kind of mind-boggling  
right so what was it about this perturbation that  actually achieved this complete adversarial attack  
what is this perturbation doing remember that when  we train neural networks using gradient descent  
our our task is to take some objective j and try  to optimize that objective given a set of weights  
w an input x and a prediction y and our goal  and what we're asking in doing this gradient  
descent update is how does a small change in  the weights decrease the loss specifically how  
can we perturb these weights in order to minimize  the loss the objective we're seeking to minimize
in order to do so we train the network with a  fixed image x and a true label y and perturb  
only the weights to minimize the loss with  adversarial attacks we're now asking how can  
we modify the input image in order to increase the  error in the network's prediction therefore we're  
trying to predict to perturb the input x in some  way such that when we fix the set of weights w  
and the true label y we can then increase the  loss function to basically trip the network up  
make it make a mistake this idea of adversarial  perturbation was recently extended by a group  
here at mit that devised an algorithm that could  actually synthesize adversarial examples that were  
adversarial over any set of transformations  like rotations or color changes and they  
were able to synthesize a set of 2d adversarial  attacks that were quite robust to these types  
of transformations what was really cool was  they took this a step further to go beyond 2d  
images to actually synthesize physical  objects 3d objects that could then be  
used to fool neural networks and this was the  first demonstration of adversarial examples that  
actually existed in the real physical world so the  example here these turtles that were 3d printed  
adversarial to be adversarial were incorrectly  classified as rifles when images of those turtles  
were taken again these are real physical  objects and those images were then fed into  
a classifier so a lot of interesting questions  raised in terms of what how can we guarantee  
the robustness and safety of deep learning  algorithms to such adversarial attacks which can  
be used perhaps maliciously to try to perturb the  systems that depend on deep learning algorithms
the final limitation but certain that i'd like  to introduce in this lecture but certainly not  
the final limitation of deep learning overall is  that of algorithmic bias and this is a topic and  
an issue that deservingly so has gotten a lot of  attention recently and it's going to also be the  
focus of our second hot topic lecture and this  idea of algorithmic bias is centered around the  
fact that neural network models and ai systems  more broadly are very susceptible to significant  
biases resulting from the way they're built the  way they're trained the data they're trained on  
and critically that these biases can lead to very  real detrimental societal consequences so we'll  
discuss this issue in tomorrow's spotlight talk  which should be very exciting so these are just  
some of many of the limitations of neural networks  and this is certainly not an exhaustive list and  
i'm very excited to again re-emphasize that we're  going to focus in on two of these limitations  
uncertainty and algorithmic bias in  our next two upcoming spotlight talks  
all right for the remainder  of this talk this lecture  
i want to focus on some of the really exciting  new frontiers of deep learning that are being  
targeted towards tackling some of these  limitations specifically this problem of  
neural networks being treated as like black box  systems that uh lack sort of domain knowledge and  
structure and prior knowledge and finally  the broader question of how do we actually  
design neural networks from scratch  does it require expert knowledge  
and what can be done to create more generalizable  pipelines for machine learning more broadly  
all right the first new frontier that we'll delve  into is how we can encode structure and domain  
knowledge into deep learning architectures to take  a step back we've actually already seen sort of an  
example of this in our study of convolutional  neural networks convolutional neural networks  
cnns were inspired by the way that visual  processing is thought to work in in the brain  
and cnns were introduced to try to capture spatial  dependencies in data and the idea that was key to  
enabling this was the convolution operation  and we saw and we discussed how we could use  
convolution to extract local features present  in the data and how we can apply different sets  
of filters to determine different features and  maintain spatial invariance across spatial data
this is a key example of how the structure of  the problem image data being defined spatially  
inspired and led to a advance in encoding  structure into neural network architecture  
to really tune that architecture specifically for  that problem and or class of problems of interest
moving beyond image data or sequence data the  truth is that all around us there are there  
are data sets and data problems that have  irregular structures in fact there can be a  
the paradigm of graphs and of networks is one  where there's a very very high degree of rich  
structural information that can be encoded in a  graph or a network that's likely very important  
to the problem that's being considered but it's  not necessarily clear how we can build a neural  
network architecture that could be well suited to  operate on data that is represented as a graph so  
what types of data or what types of examples could  lead naturally to a representation as a graph well  
one that we're all too immersed in and  familiar with is that of social networks  
beyond this you can think of state machines which  define transitions between different states in  
a system as being able to be represented  by a graph or patterns of human mobility  
transportation chemical molecules where you can  think of the individual atoms in the molecule  
as nodes in the graph connected by the bonds  that connect those atoms biological networks  
and the commonality to all these instances and  graphs as a structure more broadly is driven by  
this appreciation for the fact that there are so  many real world data examples and applications  
where there is a structure that can't be readily  captured by a simple a simpler data encoding like  
an image or a temporal sequence and so we're going  to talk a little bit about graphs as a structure  
that can provide a new and non-standard  encoding for a series of of of problems
all right to see how we can do this and to build  up that understanding let's go back to a network  
architecture that we've seen before we're  familiar with the cnn and as you probably know  
and i hope you know by now in cnn's we  have this convolutional kernel and the  
way the convolutional operation in cnn layers  works is that we slide this rectangular kernel  
over our input image such that the kernel can  pick up on what is inside and this operation  
is driven by that element-wise multiplication  and addition that we reviewed previously  
so stepping through this if you have an  image right the the convolutional kernel is  
effectively sliding across the image applying  its filter its set of weights to the image  
going on doing this repeatedly and repeatedly  and repeatedly across the entirety of the image  
and the idea behind cnns is by designing these  filters according to particular sets of weights  
we can pick up on different types of  features that are present in the data  
graph convolutional networks operate on  a very using a very similar idea but now  
instead of operating on a 2d image the network is  operating on data that's represented as a graph  
where the graph is defined by nodes shown here in  circles and edges shown here in lines and those  
edges define relationships between the nodes  in the graph the idea of of how we can extract  
information from this graph is very similar in  principle to what we saw with cnns we're going  
to take a kernel again it's just a weight matrix  and rather than sliding that kernel across the 2d  
the 2d matrix representation of our image that  kernel is going to pop around and travel around  
to different nodes in the graph and as it does so  it's going to look at the local neighborhood of  
that node and pick up on features relevant to the  local connectivity of that node within the graph  
and so this is the graph convolution operation  where we now learn to the the network learns to  
define the weights associated with  that filter that capture the edge  
dependencies present in the graph so let's  step through this that weight kernel is going  
to go around to different nodes and it's  going to look at its emergent neighbors  
the graph convolutional operator is going to  associate then weights with each of the edges  
present and is going to apply those weights across  the graph so the kernel is then going to be moved  
to the next node in the graph extracting  information about its local connectivity  
so on applying to all the different nodes in the  graph and the key as we continue this operation  
is that that local information is going to be  aggregated and the neural network is going to  
then learn a function that encodes that local  information into a higher level representation  
so that's a very brief and intuitive introduction  hopefully about graph confirmation on neural  
networks how they operate in principle and it's  a really really exciting network architecture  
which has now been enabling  enormously powerful advances  
in a variety of scientific domains for example  in chemical sciences and in molecular discovery  
there are a class of graph neural networks  called message passing networks which have been  
very successfully deployed on 2d two-dimensional  graph-based representations of chemical structures  
and these message passing networks build up a  learned representation of the atomic and chemical  
bonds and relationships that are present in a  chemical structure these same networks based on  
graph neural networks were very recently applied  to discover a novel antibiotic a novel drug  
that was effective at killing resistant bacteria  in animal models of bacterial infection i think  
this is an extremely exciting avenue for research  as we start to see these deep learning systems  
and neural network architectures being applied  within the biomedical domain another recent and  
very exciting application area is in mobility and  in traffic prediction so here we can take streets  
represent them as break them up to represent  them as nodes and model the intersections and the  
regions of the street network by a graph where the  nodes and edges define the network of connectivity  
and what teams have done is to build up  this graph neural network representation  
to learn how to predict traffic patterns across  road systems and in fact this modeling can  
result in improvements in how well estimated  time of arrivals can be predicted in things  
and interfaces like google maps another very  recent and highly relevant example of graph  
neural networks is in forecasting the spread of  covin-19 disease and there have been groups that  
have looked into incorporating both geographic  data so information about where a person lives and  
is located who they may be connected to as well as  temporal data information about that individual's  
movement and trajectory over time and using  this as the input to graph neural networks and  
because of the spatial and temporal component to  this data what has been done is that the graph  
neural networks have been integrated with temporal  embedding components such that they can learn  
to forecast the spread of the covid19 disease  based not only on spatial geographic connections  
and proximities but also on temporal patterns  another class of data that we may encounter is  
that of three-dimensional data three-dimensional  sets of points which are often referred to  
as point clouds and this is another domain in  which the same idea of graph neural networks is  
enabling a lot of powerful advances  so to appreciate this you will first  
have to understand what exactly these  three-dimensional data sets look like  
these point clouds are effectively unordered sets  of data points in space a cloud of points where  
there's some underlying spatial dependence  between the points so you can imagine having  
these sort of point-based representations of  a three-dimensional structure of an object  
and then training a neural network on these  data to do many of the same types of of tasks  
and problems that we saw in our computer vision  lecture so classification taking a point cloud  
identifying that as an object as a particular  object segmentation taking a point cloud  
segmenting out instances of that point cloud  that belong to particular objects or particular  
content types we what we can do is we can extend  graph convolutional networks to be able to operate  
to point clouds the way that's done which i  think is super awesome is by taking a point cloud  
expanding it out and dynamically computing a graph  using the meshes inherent in the point cloud and  
this is example is shown with this this structure  of a rabbit where we're starting here from the  
point cloud expanding out and then defining the  local connectivity uh across this 3d mesh and  
therefore we can then apply graph convolutional  networks to sort of maintain invariances about  
the order of points in 3d space and also still  capture the local geometries of such a data system
all right so hopefully that gives you a sense  of different types of ways we can start to  
think about encoding structure internal  neural network architectures moving beyond  
the architectures that we saw in the first five  lectures for the second new frontier that i'd like  
to focus on and discuss in the remainder of this  talk it's this idea of how we can learn to learn  
and i think this is a very  powerful and thought-provoking  
domain within deep learning research and  it spawns some interesting questions about  
how far and how deep we can push the  capabilities of machine learning and ai systems
the motivation behind this field of what is now  called automated machine learning or auto ml  
is the fact that standard deep neural network  architectures are optimized for performance on  
a single task and in order to build a new model we  require sort of domain expertise expert knowledge  
to try to define a new architecture that's going  to be very well suited for a particular task the  
idea behind automated machine learning is that  can we go beyond this this tuning of of you know  
optimizing a particular architecture robustly  for a single task can we go beyond this to build  
broader algorithms that can actually learn what  are the best models to use to solve a given  
problem and what we mean in terms of best model  or which model to use is that its architecture  
is optimal for that problem the hyper parameters  associated with that architecture like the number  
of layers it has the number of neurons per layer  those are also optimized and this whole system  
is built up and learned via an a a  algorithm this is the idea of automl  
and in the original automl work which stands  for automated machine learning the original work  
used a framework based on reinforcement learning  where there was a neural network that is  
referred to as a controller and in this case this  controller network is a recurrent neural network  
the controller what it does is it proposes a  sample model architecture what's called the child  
architecture and that architecture is going  to be defined by a set of hyper parameters  
that resulting architecture is can then be trained  and evaluated for its performance on a particular  
task of interest the feedback of the performance  of that child network is then used as sort of the  
reward in this reinforcement learning framework  to try to promote and inform the controller  
as to how to actually improve its network  proposals for the next round of optimization  
so this cyclic process is repeated thousands upon  thousands of times generating new architectures  
testing them giving that feedback to  the controller to build and learn from  
and eventually the controller is going to tend  towards assigning high probabilities to hyper  
parameters and regions of the architecture  search space that achieve higher accuracies  
on the problem of interest and will assign low  probability to those areas of the search space  
that perform poorly so how does this agent  how does this controller agent actually work  
well at the broad view at the macro scale it's  going to be a rnn based architecture where  
at each step each iteration of this pipeline the  model is this controller model is going to sample  
a brand new network architecture and that this  controller network is specifically going to be  
optimized to predict the hyper parameters  associated with that spawned child network  
so for example we can consider the  optimization of a particular layer  
that optimization is going to involve prediction  of hyper parameters associated with that layer  
like as for a convolutional layer the size  of the filter the length of the stride  
and so on and so forth then that resulting  network that child network that's spawned and  
defined by these predicted hyper parameters  is going to be tested trained and tested  
such that after evaluation we can take the  resulting accuracy and update the recurrent  
neural network controller system based on how  well the child network performed on our task  
that rnn controller can then learn to create an  even better model and this fits very nicely into  
the reinforcement learning framework where the  agent of our controller network is going to be  
rewarded and updated based on the performance  of the child network that it spawns this idea  
has now been extended to a number of different  domains for example recently in the context of  
image recognition with the same principle of a  controller network that spawns a child network  
that's then tested evaluated to improve the  controller was used to design a optimized  
neural network for the task of image  recognition in this paradigm of designing this  
designing an architecture can be thought of as  neural architecture search and in this work the  
controller system was used to construct and design  convolutional layers that were used in an overall  
architecture tested on image recognition tasks  this diagram here on the left depicts what that  
learned architecture of a convolutional cell in  a convolutional layer actually looked like and  
what was really really remarkable about this work  was when they evaluated was the results that they  
found when they evaluated the performance of  these neural network designed neural networks  
i know that's kind of a mouthful but let's  consider those results so first here in black  
i'm showing the accuracy of the state-of-the-art  human-designed convolutional models on an image  
recognition task and as you can appreciate  the accuracy shown on the y-axis scales with  
the number of parameters in the millions shown  on the x-axis what was striking was when they  
compared the performance of these human-designed  models to the models spawned and returned by the  
automl algorithm shown here in red these neural  designed neural architectures achieved superior  
accuracy compared to the human-designed  systems with relatively fewer parameters  
this idea of using machine learning using deep  learning to then learn more general systems or  
more general paradigms for predictive modeling and  decision making is a very very powerful one and  
most recently there's now been a lot of emerging  interest in moving beyond automl and neural  
architecture search to what we can think of more  broadly as auto ai an automated complete pipeline  
for designing and deploying machine learning  and ai models which starts from data curation  
data pre-processing to model selection and design  and finally to deployment the idea here is that  
perhaps we can build a generalizable pipeline  that can facilitate and automatically  
accelerate and design all steps of this process  
i think this idea spawns a very very  thought-provoking point which is can we  
build ai systems that are capable of generating  new neural networks designed for specific tasks  
but the higher order ai system that's built is  then sort of learning beyond a specific task  
not only does this reduce the need for us as  experienced engineers to try to hand design  
and optimize these networks it also makes these  deep learning algorithms more accessible and more  
broadly we start to get at this consideration of  what it means to be creative what it means to be  
intelligent and when alexander introduced this  course he spoke a little bit about his thoughts  
on what intelligence means the ability to take  information using it to inform a future decision  
and as humans our learning pipeline is definitely  not restricted to optimization for a very specific  
task our ability to learn and achieve and  solve problems impacts our ability to learn  
completely separate problems are and  improves our analytical abilities the  
models and the neural network  algorithms that exist today  
are certainly not able to extend to this point  and to capture this phenomena of generalizability  
i think in order to reach the point of true  artificial intelligence we need to be considerate  
of what that true generalizability  and problem-solving capability means  
and i encourage you to think about this this  point to think about how automl how auto ai  
how deep learning more broadly falls into  this broader picture of the intersection  
and the interface between artificial and human  intelligence so i'm going to leave you with that  
as a point of reflection for you at this  point in the course and beyond with that  
i'm going to close this lecture and remind  you that we're going to have a software  
lab and office hour session we're going to be  focusing on providing support for you to finish  
the final lab on reinforcement learning  but you're always welcome to come  
discuss with us ask your questions discuss with  your classmates and teammates and for that we  
encourage you to come to the class gather town  and i hope to see you there thank you so much
hi everyone and welcome back to today's
lecture
in the first six lectures of 6s191 we
got a taste of some of the foundational
deep learning algorithms and models
in the next two lectures we're going to
actually be diving into
a lot more detail focusing specifically
on two critically important
hot topic areas in modern deep learning
research that have really impacted
everything that we have been learning so
far in this class
now these two hot topic lectures are
going to focus first
on probabilistic modeling with deep
neural networks for uncertainty
estimation
in this lecture as well as algorithmic
bias and fairness
in the next lecture now today we'll
learn about a very powerful new
technique called
evidential deep learning for learning
when we can trust the output
of the neural networks that we're
training and interpret
when they're not confident or when they
are uncertain
in their predictions
now this whole field of uncertainty
estimation in deep learning
is super important today more than ever
as we're seeing these deep learning
models like we've been learning about so
far in this class
start to move outside of the lab and
into reality
interacting with or at the very least
impacting the lives of humans around
them
traditional deep learning models tend to
propagate biases from training
and are often susceptible to failures on
brand new out-of-distribution data
instead we need models that can reliably
and quickly estimate
uncertainty in the data that they are
seeing as well as
the outputs that they're predicting in
this lecture
we'll actually start to learn about
evidential deep learning for uncertainty
estimation of neural networks
so training our model not only just to
make a prediction
and predict an answer but also to
understand
how much evidence it has in that
prediction
how much we should trust its answer
a big reason and actually cause for
uncertainty estimation
for uncertainty in deep learning is due
to
the very large gap in how neural
networks are trained in practice
and how they're evaluated in deployment
in general when we're training machine
learning models
we make the following assumption that
our training set
is drawn from the same distribution as
our test set
but in reality this is rarely true
for example when we look at many of the
state-of-the-art
state-of-the-art algorithms that we've
learned about so far
they are almost all trained on extremely
clean
pre-processed data sets often with
minimal
occlusions minimal noise or ambiguity
in the real world though we are faced
with so many edge cases of our model
that our model is going to be completely
unequipped to handle
for example if we're training this
classifier to identify
dogs and we train it on these clean
images of dogs on the left hand side
it will yield very poor performance when
we take it into the real world
and we start showing it dogs in brand
new positions dogs in
upside down configurations or even this
parachuting dog coming through the sky
or if we take this driving model trained
on clean urban
streets and then we take it out into the
real world and starts to see all kinds
of strange
unexpected edge cases in reality
now the point of today's lecture is to
build models that are not only
super accurate and have high performance
but
also to build quantitative estimation
techniques
into our learning pipeline such that our
model will be able to tell us
when it doesn't know the right answer
now there's this famous quote here by
george box
that i've adapted a bit to convey this
now in the end
all models are wrong but some that know
when they can be
trusted are actually going to be useful
in reality
now very rarely do we actually need
a model to achieve a perfect 100
accuracy
but even if our accuracy is slightly
lower if we can understand
when we can trust the output of that
model then we have something extremely
powerful now the problem of knowing when
we don't know
something turns out to be extremely
difficult though and this is true even
for humans
there are so many tasks that we believe
we can achieve
even though we don't really know the
right answer uh
and are probably more likely to sorry
are more likely to fail
than to succeed now this picture
probably doesn't even need an
explanation
for anyone who has driven in a new city
before we had things like google maps
in our phones there's often this
tendency
to refuse to accept that you might be
lost and ask for help
even though we may truly be lost in a
location that we've never been in before
so today we're going to learn about how
we can teach neural networks to
predict probability distributions
instead of purely deterministic
point outputs and how formulating our
neural networks like this can allow us
to model
one type of uncertainty but not all
types and specifically we'll then
discuss
how it does not capture one very
important form of uncertainty
which is the uncertainty in the
prediction
itself finally we'll see how we can
learn
neural representations of uncertainty
using evidential deep learning which
will allow us
to actually capture this uh this other
type of uncertainty
and estimate it quickly while also
scaling to some very high dimensional
learning problems and high dimensional
output problems
let's start and discuss what i mean when
i say that we want to learn
uncertainties using
neural networks and what this term of
probabilistic learning
and how it what is this term of
probabilistic learning and how does it
tie into everything
that we've been seeing so far in this
class
now in the clay in the case of
supervised learning problems there have
been two main sources
of data input to our model the first is
the data itself
we've been calling this x in the past
and it's what we actually feed
into the model given our data we want to
predict some target
here denoted as y this could be a
discrete class
that x belongs to it could also be any
real number that we want to forecast
given our data
either way we're going to be given a
data set of both
x and y pairs and have been focused
so far in this class on really in the
case of at least supervised learning
learning a mapping to go from x to y and
predict
this expected value of y given our
inputs
x this is exactly how we've been
training deterministic
supervised neural networks in the past
classes now the problem here
is that if we only model the expectation
of our target
the expectation of why then we only have
a point estimate
of our prediction but we lack any
understanding of how spread or uncertain
this prediction is and this is exactly
what we
mean when we talk about estimating the
uncertainty of our model
instead of predicting an answer on
average
the expectation of y we want to also
estimate the variance of our predicted
target
y this gives us a deeper and more
probabilistic sense
of the output and
you might be thinking when i'm saying
this that this sounds very very similar
to what we have already seen in this
lecture in in this class
and well you would be completely right
because in the very first lecture
of this course we already had gotten a
big sense of training neural networks to
output
full distributions for the case of
classification specifically
so we saw an example where we could feed
in an image
to a neural network and that image
needed to be classified
into either being a cat or being a dog
for example
now each output here is a probability
that it belongs
to that category and both of these
probabilities sum to one
or must sum to one since it's a
probability distribution our output is a
probability distribution
now this is definitely one example of
how neural networks can be trained to
output a probability distribution
in this case a distribution over
discrete class
categories but let's actually dive
into this a bit further and really
dissect it and see how we're able to
accomplish this
well first we had to use this special
activation function if you recall this
was called the
softmax activation function we had to
use this activation function
to satisfy two constraints on our output
first
was that each of the probability outputs
had to be greater than zero
and second that we needed to make sure
that the sum of all of our class
probabilities
was normalized to one now given
an output of class probabilities
emerging from this softmax activation
function we could then define this
special loss that allowed us to optimize
our distribution learning
we could do this by minimizing what we
called the negative log likelihood of
our predicted distribution to match the
ground truth
category distribution now this is also
called the cross entropy loss
which is something that all of you
should be very familiar with now
as you've implemented it and used it in
all three of your software labs
let's make some of this even more formal
on why we made
some of these choices with our
activation function and our loss
function
well it really all boils down to this
assumption
that we made before we even started
learning
and that was that we assumed that our
target class labels y
are drawn from some likelihood function
in this case
a categorical distribution defined by
distributional parameters
p the distributional parameters here
actually
define our distribution and our
likelihood over that predicted label
specifically the probability of our
answer being in the i
class is exactly equal to the ith
probability
parameter now similarly
we also saw how we could do this for the
case of continuous class targets
as well in this case where we aren't
learning a class
probability but instead a probability
distribution
over the entire real number line like
the classification domain
this can be applied to any supervised
learning problem we saw
when we saw it in the previous lectures
we were focusing in the case of
reinforcement learning
where we want to predict the steering
wheel angle that a vehicle should take
given a raw image pixel image of the
scene
now since the support of this output
is continuous and infinite we cannot
just output
raw probabilities like we did in the
case of classification
because this would require an infinite
number of outputs coming from our
network
instead though we can output the
parameters of our distribution
namely the mean and the standard
deviation or the variance
of that distribution and this defines
our probability
density function our mean is unbounded
so we don't need to
constrain it at all on the other hand
though our standard deviation
sigma must be strictly positive so for
that we can use an exponential
activation function to enforce that
constraint
and again in the classification domain
we
are similar to the classification domain
we can optimize these networks
by using a negative log likelihood loss
and again how did we get to this point
well we assumed
we made this assumption about our labels
we assumed that our labels
were drawn from a normal distribution or
a gaussian distribution with known
parameters
mu and sigma squared our mean and our
variance
which we wanted to train our model to
predict
to output now i think this is actually
really amazing
because we don't have any ground truth
uh variables in our data set for
ground truth means and ground truth
variances all we have
are ground truth wives our labels but we
use this formulation and this loss
function
to learn not a point estimate of our
label
but a distribution a full distribution a
gaussian
around describing that data likelihood
now we can summarize the details of
these likelihood estimation problems
using neural networks for both the
discrete
classification domain as well as the
regression the continuous regression
domain
now fundamentally the two of these
domains differ
in terms of the type of target that
they're fitting to in the classification
domain the targets can be one of a fixed
set of classes one to k in the
regression domain our target can be any
real number
now before getting started we assume
that our labels
came or were drawn from some underlying
likelihood
function in the case of classification
again they were being drawn from a
categorical distribution whereas in the
case of regression we assume that they
were being drawn
from a normal distribution now each of
these likelihood functions
are defined by a set of distributional
parameters in the case of categorical we
have these probabilities that define
our categorical distribution and in the
case of a normal
or a gaussian distribution regression we
had a mean and a
variance to ensure that these were
valid probability distributions we had
to apply
some of the relevant constraints to our
parameters through way of activation
functions cleverly constructed
activation functions
and then finally for both of these like
we previously saw we can optimize these
entire systems
using the negative log likelihood loss
this allowed us to learn the parameters
of a distribution over our labels
while we do have access to the
probability and the variance
here it is critically important
for us to remember something and that is
that probability or likelihood let's
call it
that we get by modeling the problem like
we have seen
so far in this lecture should never be
mistaken
for the confidence of our model these
probabilities that we obtain are
absolutely not the same
as confidence what we think of as
confidence at least and here's why
let's go back to this example of
classification where we feed in an image
it can be either cats or dogs to this
neural network and we have our neural
network
predict what is the probability that
this image is of a cat or what
probability that
it is of a dog now because if we
feed in an image of let's say a cat our
model should be able to identify
assuming it's been trained to identify
some of the key features specific to
cats and say okay
this is very likely a cat because i see
some cat-like features in this image
and likewise if i fed in a dog input
then
we should be more confident in
predicting a high probability that this
image is of a dog
but what happens if we say that let's
feed in this image of
both a cat and a dog together in one
image our model will identify
some of the features corresponding to
the successful detection of a cat
as well as a successful detection of a
dog simultaneously
within the same image and it will be
relatively split
on its decision now this is not to say
that we are not confident about this
answer
we could be very confident in this
answer because we did detect
both cat features and dog features in
this image
it simply means that there's some
ambiguity in our input data
leading to this uncertainty that we see
in our output
we can be confident in our prediction
even though our answer
we are answering with a probability or a
percentage of 50 percent
cat and 50 dog because we're actually
because we we're actually training on
images
that share these types of features but
what about in cases where we
are looking at images where we did not
train on these types of features so for
example if we take that same neural
network but now we feed in this image
of a boat something completely new
unlike anything that we have seen during
training
the model still has to output two things
the probability that this image is of a
cat and the probability that this image
is of
a dog and we know that because this is a
probability distribution
trained with the soft max activation
function we know that this
is a categorical distribution both of
these probabilities
probability of cat plus probability of
dog must sum to one
no matter what thus the output
likelihoods in this scenario will be
severely unreliable
if our input is unlike anything that we
have ever seen
during training and we call this being
out of distribution or out of the
training distribution
and we can see that in this case our
outputs
or of these two probabilities are
unreasonable here they should not be
trusted
now this is really to highlight to you
that when we say
uncertainty estimation there are
different types of uncertainty that we
should be concerned about
while training neural networks like this
captures probabilities they do not
capture the uncertainty in the
predictions themselves
but rather the uncertainty in the data
this brings into question
what are the different types of
uncertainty that exist
and how can we learn them using
different algorithmic
techniques in deep learning
and what this all boils down to is the
different ways that we can try to
estimate
when the model doesn't know or when it
is
uncertain of its answer or prediction we
saw that there are different types of
these uncertainties
and this is true even in our daily lives
i think a good way to think about this
is through this two by two matrix of
knowns
and unknowns and i'll give a quick
example just illustrating this
very briefly so imagine you are in an
airport for a flight
you have some known knowns for example
that there will be some flights taking
off from that airport
that's a known known you're very
confident and you're very certain
that that will happen there are also
things like known unknowns
things that we know uh we know there are
things
that we simply cannot predict for
example
we may not know when our flight the
exact time that our flight is going to
take off that's something that we cannot
predict maybe because
it could get delayed or it's just a it's
just something that we don't have total
control over and
can possibly change then there are
unknown knowns
things that others know but you don't
know a good example of that would be
someone else's scheduled departure time
their scheduled flight time
you know that someone else knows their
scheduled departure time but
to you that's an unknown known and
finally there are unknown unknowns these
are completely unexpected or
unforeseeable events a good example of
this would be a meteor
crashing into the runway
now this is an emerging and exciting
field of research
in fundamental machine learning
understanding how we can build
algorithms
to robustly and efficiently model and
quantify uncertainties
of these deep learning models and this
is really tough actually because these
models have millions and millions
billions and now even trillions of
parameters uh
and understanding and introspecting them
inspecting their insides
uh to estimate understanding when they
are going to
not know the correct answer is
definitely not a straightforward problem
now people do not typically train neural
networks to account for these types of
uncertainties
so when you train on some data for
example here you can see
the observations in black we can train a
neural network to make some predictions
in blue
and the predictions align with our
observations inside this region where we
had
training data but outside of this region
we can see that
our predictions start to fail a lot and
estimating the uncertainty in situations
like this
comes in two forms that we're going to
be talking about today the first form is
epistemic uncertainty
which models the uncertainty in the
underlying predictive process
this is when the model just does not
know
the correct answer and it's not
confident in its answer
the other form of uncertainty is
aliatoric uncertainty this is
uncertainty in the data itself
think of this as statistical or sensory
noise this is
known as this is known as irreducible
uncertainty so
since no matter how much data that you
collect
you will have some underlying noise in
the collection process
it is inherent in the data itself the
only way to reduce
alliatoric uncertainty is to change your
sensor and get more accurate data
now we care a lot about both of these
forms of uncertainty both alliatoric
uncertainty
and epistemic uncertainty and again just
to
recap the differences between these two
forms of uncertainty
we have aliatoric on the left-hand side
this focuses on the statistical
uncertainty of our data
it describes how confident we are in our
data itself
it's highest when our data is noisy
and it cannot be reduced by adding more
data
on the other hand we have epistemic
uncertainty this is
much more challenging to estimate than
aliatoric uncertainty
and there are some emerging approaches
that seek to determine epistemic
uncertainty
now the key is that epistemic
uncertainty reflects the model's
confidence
in the prediction we can use these
estimates to begin to understand
when the model cannot provide a reliable
answer
and when it's missing some training data
to provide that answer
unlike aliatoric uncertainty epistemic
uncertainty can be reduced by adding
more data
and improving that confidence of the
model
so while alliatoric uncertainty can be
learned directly
using neural networks using likelihood
estimation techniques like we learned
about
earlier in this lecture today epistemic
uncertainty is very challenging to
estimate
and this is because epistemic
uncertainty reflects the uncertainty
inherent to the model's predictive
process itself
with a standard neural network a
deterministic neural network
we can't obtain a sense of this
uncertainty because the network is
deterministic
with a given set of weights passing in
one
input to the model multiple times will
yield the same output
over and over again it's going to always
have the same fixed output
no matter how many times we feed in that
same input
but what we can do instead of having a
deterministic
neural network where each weight is a
deterministic number
a single number for each weight we can
represent
every single weight by a probability
distribution
so in this case we're going to actually
model
each of these weights by a distribution
such that when we pass in an input
to our neural network we sample from
the point in this distribution for every
single weight in the neural network
this means that every time we feed in an
input to the model we're going to get
out
a slightly different output depending on
the samples of our weights that we
we realized that time that we fed it
through the model
these models are called bayesian neural
networks these model
distributions likelihood functions over
the network weights themselves so
instead of modeling a single number for
every weight bayesian neural networks
try to
learn neural networks that capture a
full distribution
over every single weight and then use
this distribution
to actually learn the
the uncertainty of our model the
epistemic uncertainty of our model
now we can formulate this epistemic
uncertainty and formulate this learning
of bayesian
neural networks as follows while
deterministic neural networks
learn this fixed set of weights w
bayesian neural networks learn a
posterior distribution
over the weights this is a probability
of our weights
given our input data and our labels
x and y now these are called bayesian
neural networks
because they formulate this posterior
probability of our weights given our
data
using bayes rule they actually write it
out like this using bayes rule
however in practice this posterior is
intractable to compute analytically it's
not possible for any
real or non-toy examples
which means that we have to resort to
what are called sampling
techniques to approximate and try to
estimate this posterior
so we can approximate this posterior
through sampling
where the idea is to make multiple
stochastic evaluations
through our model each using a different
sample
of our weights now this can be done
many different ways one way to do this
approximation is using a technique that
we've learned about in class already
called dropout typically dropout is used
during training but now we're talking
about using dropout
during testing time to obtain these
multiple samples through our network
so the way this works is nodes or
neurons are dropped
or dropped out or not dropped out based
on the value of a bernoulli random
variable with some probability p
that we define as part of our dropout
procedure
each time we feed our input our same
input through the model
depending on which nodes are dropped in
and out we're going to get a slightly
different output
and that's going to each one of those
outputs is going to represent
a different sample through our network
now alternatively we can sample a
different way using an
ensemble of independently trained models
each of these will learn a unique
set of weights after seeing a unique set
of training data or potentially the same
training data just shown in a different
order or sequence in both of these cases
though
what we're doing here is very similar
we're drawing a set of t
samples of our weights from and using
these to compute
t forward passes using either dropout
or t models in an ensemble this allows
us to formulate
two terms one is the expectation of our
prediction y
as well as the variance of our
prediction over the course of these
forward passes so if our variance over
these predictions
is very large so if we take t stochastic
forward passes through the model
and we have a very large variance if
none of our outputs really agree with
each other
this is a great indicator that our model
has a high
epistemic uncertainty and in fact it's
not only an indicator that it has high
fsm
uncertainty this is the epistemic
uncertainty the variance
of our prediction over these t
stochastic forward passes
while these sampling based approaches
are very commonly used techniques for
epistemic uncertainty estimation
they do have a few very notable
downsides and limitations
first as you may have realized from the
fact that they draw
multiple samples to approximate this
weight distribution
this means that they require running the
model
multiple times except t times just to
obtain
their predictions for ensembles it's
even worse than this because you have to
initialize and train multiple
independent models which is extremely
computationally costly then you have to
store them in memory and repeatedly
run them in memory as well
repeatedly relatedly this imposes
a memory constraint due to having to
actually keep
all of these models in parallel on on
your network
on your computer and together this means
that these sampling approaches
are not very efficient which is a
significant limitation
for applications where uncertainty
estimation is necessary to be made
in real time on edge devices for example
on robotics or other mobile devices
and the last point i'd like to make is
that bayesian approaches abrasion
approximation methods such as dropout
tend to produce
typically overconfident uncertainty
estimates which may be
problematic also in safety critical
domains where we
really need calibrated uncertainty
estimates and we really
prefer consider ourselves uncertain
rather than confident so the backup case
we don't want to assume
in the backup case that we are confident
that we know what we're doing when we
don't really know what we're doing
but what are sampling approaches really
trying to do
when they try to approximate this
uncertainty
let's look at this in a bit more detail
and to answer this question
let's suppose we're working within the
context again of
self-driving cars since this is a very
nice and intuitive example
we have a model that's looking to
predict the steering wheel angle
of the car given this raw image that it
sees on the left
the mean here it's predicting two things
a mean and a variance the mean here is
the angle that the wheel should be
the variance is the alliatoric
uncertainty the data uncertainty
if you remember now consider the
epistemic uncertainty this is the
model uncertainty that we said was
really difficult to capture
as we've seen with sampling based
approaches we can we can compute
the epistemic uncertainty using ensemble
of many independently trained
instances of this model so we could take
one model and obtain
its estimates of mu and sigma squared
and we can plot for this given image
where that model believes mu and sigma
squared
should be on this two-dimensional graph
on the right on the x-axis is mu on the
axis is sigma squared and we can place
exactly where that network believes the
output should be
in this two-dimensional space and we can
repeat this for several different models
each model
we take its output and we can plot it in
this space
and over time if we do this for a bunch
of models we can start to estimate the
uncertainty
by taking the variance across these
predictions to get a metric
of uncertainty intuitively
if we get a huge variance here a huge
variance over our muse
if the answers are said differently if
the answers are very spread out
this means that our model is not
confident on the other hand
if our answers are very close to each
other this is a great indicator that we
are very confident
because even across all these different
models that we independently trained
each one is getting a very similar
answer
that's a good indicator that we are
confident in that answer
indeed this is the epistemic or the
model uncertainty
in fact these estimates these these
estimates that are coming from
individually trained models are actually
being drawn
from some underlying distribution now
we're starting to see this distribution
shape up and the more samples
that we take from that distribution the
more it will start to appear
like this background distribution that
we're seeing
capturing this distribution though
instead of sampling if we could just
capture this distribution directly
this could allow us to better and more
fully understand the model uncertainty
so what if instead of drawing samples
from this distribution
approximating it we just tried to learn
the parameters of this distribution
directly now
this approach this is the approach that
has been taken by
an emerging series of uncertainty
quantification methods called evidential
deep learning
which consider learning as an evidence
acquisition process
evidential deep learning tries to enable
direct estimation of both
alliatoric uncertainty as well as
epistemic uncertainties by trying to
learn what what we call these higher
order
distributions over the individual
likelihood parameters
in this case over the parameters of mu
and sigma we try to learn
a distribution on top of them so to
understand this
consider how different types of degrees
of uncertainties
may manifest in these evidential
distributions
when we have very low uncertainty like
we can see on the left hand side
the spread along our mean or mu
and our sigma squares is going to be
very small we're going to have very
concentrated
density right in a specific point this
indicates that we have
low uncertainty or high confidence when
we have
something like high aleatoric
uncertainty on the other hand
we might see high values of sigma
squared
which we can actually represent by an
increase along the y
axis the sigma squared axis of this plot
here represented in the middle finally
if we have
high epistemic uncertainty we'll have
very high variability in the actual
values
of mu that are being returned so you can
see this along spread
along the mu axis now evidential
distributions allow us to capture each
of these modes
and the goal is to train a neural
network now to learn
these types of evidential distributions
so let's take a look at how we can do
evidential learning specifically in the
case of
regression first now we call these
distributions over the likelihood
parameter evidential distributions like
i've been referring to them
now the key thing to keep in mind when
trying to wrap your head around these
evidential distributions
is that they represent a distribution
over distributions if you sample from an
evidential distribution
you will get back a brand new
distribution
over your data how we can formulate this
is is uh using or how can we
actually formulate these evidential
distributions well first let's consider
like i said the case of continuous
learning problems like
regression we assume
like we saw earlier in the class that
our target labels y
are going to be drawn from some normal
distribution
with parameters distribution parameters
mu and sigma squared this is exactly
like we saw earlier in the class
no different the key here is that
instead of before when we assumed that
mu and sigma were known things that our
network could predict
now let's say that we don't know mu and
sigma and we want to probabilistically
estimate those as well we can formally
we can formalize this by actually
placing priors
over each of these parameters each of
these distribution parameters
so we assume that our distribution
parameters here mu
and sigma squared are not known and
let's place parameters over each of
these and try to
probabilistically estimate them so
drawing from mu
will we can draw mu from a normal
parametrized
as follows and we can draw sigma squared
our variance
from an inverse gamma parameterized as
follows here using
these new hyperparameters of this
evidential distribution
now what this means is that mu and sigma
squared are now being
drawn from this normal inverse gamma
which is the joint of these two priors
the normal inverse gamma distribution is
going to be parametrized
by a different set of parameters gamma
epsilon
alpha and beta now this is our
evidential distribution or what we call
our evidential prior
it's a distribution this normal inverse
gamma defined by our model parameters
that gives rise when we sample it
when we sample from our evidential
distribution we're actually getting back
individual realizations of mu and sigma
squared these are
their own individual gaussians defining
this original
distribution on the top line over our
data itself
y so we call these evidential
distributions since they have greater
density
in areas where there is more evidence in
support of a given likelihood
distribution
realization so on the left hand side or
in the middle here you can see
an example of this evidential
distribution one type of this evidential
distribution with this normal inverse
gamma prior
over mu and sigma squared both
parameters
of the gaussian distribution which is
placed over our
data our likelihood function but you can
see on the
top left hand corner now different
realizations
of mu and sigma over this space of this
evidential distribution
then correspond to distinct realizations
of our likelihood function
which describes the distribution of our
target values y
so if we sample from any point in this
evidential distribution
we're going to receive back a mu and a
sigma squared
that defines its own gaussian that we
can see on the right hand side
we can also consider the analog of
evidential learning
in the case of classification once again
keep in mind that if you sample from an
evidential distribution
you get a brand new distribution over
the data
so for classification our target labels
y
are over a discrete set of classes of k
classes to be specific we assumed that
our
class labels were drawn from a
likelihood function of the categorical
form parameterized by some probabilities
now in this case we can
probabilistically estimate
those distribution parameters p
using what is called a dirichlet prior
the dirichlet prior is itself
parametrized by a set of concentration
parameters
here called alpha again per class so
there are
k alpha parameters in this dirichlet
distribution and when we sample
from this dirichlet we're going to
receive realizations
of our distributional parameters
distributional
probabilities defining our categorical
loss function again it's this hierarchy
of distributions
let's take a look at the simple example
of what this evidential distribution in
the case of classification
looks like here we have three possible
classes
in this case the probability mass of our
dirschlag
distribution will live entirely on this
triangular simplex
sampling from any point within this
triangle corresponds to sampling a brand
new
categorical probability distribution
for example let's say we sample from the
center of this simplex
this will correspond to equal
probabilities over the three classes
so for those who have not seen the
simplex plot before imagine the corners
of these triangles represent
perfect prediction in one of the classes
so sampling from the middle
of the simplex corresponds to equal
probabilities of each of the three
classes
being at one of the corners like i said
corresponds to all of the mass being on
one of those classes and zero mass
being on the other two and anywhere else
in this simplex corresponds to a sampled
categorical distribution that is defined
by these class probabilities that have
to sum
to one the color inside of this triangle
this gradient of blues that you can see
provides one
example of how this mass can be
distributed
throughout the simplex where the
categorical distributions can be sampled
more frequently so this is one example
representing that the
majority of the mass is placed in the
center of the simplex
but the whole power of evidential
learning is that we're going to try to
learn
this distribution so our network is
going to try to predict
what this distribution is for any given
input
so this distribution can change and the
way we're going to sample our
categorical
likelihood functions will as a result
also change
so to summarize here is a breakdown of
evidential distributions for both
regression
and classification in the regression
case the targets take
continuous values from real numbers we
assumed here that the targets
are drawn from a normal distribution
parameterized by mu and sigma
and then we had in turn a higher order
evidential distribution over these
likelihood parameters
according to this normal inverse gamma
distribution
in the case of classification the
targets here represented
a set of k or were shown actually
here over a set of k independent classes
here we assumed that our likelihood of
observing a particular class label y
was drawn from a categorical
distribution of class probabilities p
and this p was drawn from a higher order
evidential
dirichlet distribution parameterized by
alphas
now here's a also a quick interesting
side note
you may be asking yourself why we chose
this specific evidential distribution in
each of these cases why did we pick the
dirichlet distribution why did we pick
the normal inverse gamma distribution
there are many distributions out there
that we could have picked for our
likelihoods
picked over our likelihoods rather
but we chose these very special forms
because these are known as what are
called conjugate priors
picking them to be of this form makes
analytically computing our loss
tractable specifically if our prior or
evidential distribution p of theta
is of the same family of our likelihood
p
of y given theta then we can
analytically compute
this integral highlighted in yellow as
part of our loss during training which
makes
this whole process feasible
so with this formulation of these
evidential distributions
now let's consider concretely how we can
build and train
models to learn these evidential
distributions and learn and use them
to estimate uncertainty what is key here
is that the network is trained to
actually output
the parameters of these higher order
evidential distributions
so in the case of regression we're
predicting gamma epsilon
alpha and beta in the case of
classification we're predicting a vector
of k alphas where k is the number of
classes that we have
and once we have these parameters we can
directly formulate estimates for each of
the alliatoric
and epistemic uncertainties and that is
determined by the resulting
distributions
over these likelihood parameters
we optimize these distributions by
incorporating both
maximization of our model fit and
minimization of wrong evidence
into the objective for regression this
is manifested as follows
where our maximization captures the
likelihood of seeing
data given the likelihood parameters
which are controlled by
this evidential prior parameters here
denoted as
m the minimization of incorrect evidence
is captured in this regularization term
on the right hand side by minimizing
we seek to lower the incorrect evidence
in instances where model where the model
is making errors so think of
the right hand side that's really
fitting all of our evidential
distributions to our data
and the left-hand side is inflating that
uncertainty
when we see that we get some incorrect
evidence and start to make some errors
in training we can evaluate this method
actually on some simple toy example
learning problems
where we're given some data points in
the distribution and
some some regions in our in our scene
here
where we seek to predict the target
value
or in some cases the class label
in regression we have this case where we
try to fit
to our data set where we have data in
the middle white region but we don't
have data on the two edge regions and we
can see that our
evidential distributions are able to
inflate the uncertainty
in the regions where we are out of
distribution which is exactly what we
want to see that we're able to
recognize that those predictions where
we don't have data
should not be trusted similarly in the
case of classification
operating on the mnist data set we can
generate out of distribution
examples by synthetically rotating the
handwritten digits so here
along the bottom side you can actually
see an example
one digit being rotated from left to
right
being rotated and you can see the
uncertainty of
our evidential distribution increasing
in this
out of distribution regime where the one
no longer is
even closely resembling a one but the
uncertainty
really drops down on the two end points
where the one comes back into shape
of a true one
evidential learning can also be applied
in much more complex
high dimensional learning applications
as well recently it's been demonstrated
to learn neural networks to output
thousands of evidential distributions
simultaneously
while learning to quantify pixel-wise
uncertainty
of monocular depth estimators given
only raw rgb inputs this is a regression
problem
since the predicted depth of each pixel
is a real number it's not a class
but in the case of classification
evidential deep learning has also been
applied to perform
uncertainty aware semantic segmentation
of raw lidar point clouds
also extremely high dimensional where
every point in the point cloud
must be predicted must predict
which object or what type of class
it belongs to evidential deep learning
allows us to not only classify
each of these points as an object but
also
recognize which of these objects in the
scene
express a form of an object that we
don't know
the answer to so evidential deep
learning really gives us the ability to
express
a form of i don't know when it sees
something in its input that it doesn't
know how to
predict confidently and it's able to let
the user know
when its prediction should not be
trusted
now to start wrapping up i wanted to
provide a brief comparison of all of the
different types of approaches
of uncertainty estimation that we've
learned about today and how evidential
neural networks fit into these
there were three techniques that we
touched on today starting first with
manila likelihood estimation of our over
our data
then moving to bayesian neural networks
and finally exploring
evidential neural networks each of these
methods really has
its own differences strengths and
advantages
at the highest level of difference we
saw that fundamentally each of these
methods placed probabilistic priors
over different aspects of the pipeline
over the data
in the case of the likelihood estimators
that we saw very early on
in the lecture over the weights in the
case of
bayesian neural networks and over the
likelihood for
function itself in the case of
evidential neural networks
unlike bayesian neural networks though
evidential neural networks are very fast
and very memory efficient since they
don't require any sampling
to estimate their uncertainty and even
though both
methods capture a form of epistemic
uncertainty
this is one huge advantage that means
that you don't need to train an ensemble
of models you can train just one model
and you only need to run it once
for every single input there's no
sampling required
so in summary in this lecture we got to
dive deep into
uncertainty estimation using neural
networks this is a
super important problem in modern
machine learning as well as
as we as really start to deploy our
models into the real world
we need to quickly be able to understand
when we should trust them more
and more importantly when we should not
we learned about some of the different
forms of uncertainty and how these
different methods can help us
capture both uncertainty in the data as
well as uncertainty in the model
and finally we got to have some insight
in how we can use evidential deep
learning
to learn fast and scalable calibrated
representations of uncertainty using
neural networks
thank you for attending this lecture in
the next lecture we're going to be going
through
another very impactful topic in today's
world
focusing on ai bias and fairness and
seeing some strategies for mitigating
adverse effects
of these models so we look forward to
seeing you for that lecture as well
thank you
hi everyone
welcome to our second hot topic lecture
in 6s191
where we're going to learn about
algorithmic bias and fairness
recently this topic is emerging as a
truly pervasive issue
in modern deep learning and ai more
generally
and it's something that can occur at all
stages of the ai pipeline
from data collection all the way to
model interpretation
in this lecture we'll not only learn
about what algorithmic bias is
and how it may arise but we will also
explore some new and exciting
methodological advances where we can
start to think about how we can build
machines
capable of identifying and to some
degree actually mitigating these biases
the concept of algorithmic bias points
to this observation
that neural networks and ai systems more
broadly are susceptible to significant
biases
such that these biases can lead to very
real and detrimental societal
consequences
indeed today more than ever we are
already seeing this manifesting in
society
from everything from facial recognition
to medical decision making to voice
recognition
and on top of this algorithmic bias can
actually perpetuate
existing social and cultural biases such
as racial
and gender biases now we're coming to
appreciate and recognize
that algorithmic bias in deep learning
is a truly pervasive and severe issue
and from this we really need strategies
on all levels to actually combat this
problem
to start first we have to understand
what exactly
does algorithmic bias actually mean
so let's consider this image what do you
see in this image
how would you describe it well the first
thing you may say
to describe this image is watermelon
what if i tell you to look closer
and describe it in more detail okay
maybe you'll say watermelon slices
or watermelon with seeds or other
descriptors like juicy watermelon layers
of watermelon watermelon slices next to
each other
but as you were thinking about this to
yourself i wonder how many of you
thought to describe this image as red
watermelon if you're anything like me
most likely you did not now let's
consider this new image
what is in this image here now you're
probably much more likely to place
a yellow descriptor when describing this
watermelon your top answer is probably
going to be yellow watermelon
and then with slices with seeds juicy
etc etc
but why is this the case and why did we
not say red watermelon when we saw this
original image
well when we see an image like this our
tendency
is to just think of it as watermelon
rather than
red watermelon and that's because of our
own biases
for example due to geography that make
us used to seeing watermelon that look
like this and have this red color
as this represents the prototypical
watermelon flesh that i
expect to see but perhaps if you're from
another part of the world
where the yellow watermelon originated
from you could have a different
prototypical sense
of what color watermelons should be
this points to this broader fact about
how we as humans go about perceiving and
making sense of the world
in all aspects of life we tend to label
and categorize things as a way of
imposing order to simplify and make
sense of the world
and as a result what this means is that
for everything there's
generally going to be some typical
representation what we can think of as a
prototype
and based on the frequencies of what
each of us observe
our tendency is going to be to point out
things that don't fit what we
as individuals consider to be the norm
those things that are atypical to us
for example the yellow watermelon for me
and critically biases and stereotypes
can arise
when particular labels which may not
necessarily be the minority label
can confound our decision making whether
that's human-driven
or suggested by an algorithm and in this
lecture we're going to focus on sources
of algorithmic bias
and discuss some emerging approaches to
try to combat it
to do that let's first consider how
exactly bias
can and does manifest in deep learning
and ai
one of the most prevalent examples of
bias in deep learning that we see today
is in facial detection and recently
there have been a couple of review
analyses
that have actually evaluated the
performance of commercial
facial detection and classification
systems across
different social demographics for
example
in an analysis of gender classifiers
this review
showed that commercial pipelines
performed significantly worse
on faces of darker females relative to
other demographic groups
and another analysis which considered
facial recognition algorithms
again found that error rates were
highest on female faces of color
this notion of algorithmic bias can
manifest in a myriad of different ways
so as another example let's consider the
problem of image classification
generally
and let's say we have a trained cnn and
this image on the left here
which shows a prototypical example of a
bride in some north american and
european countries
now in recent analysis when this
particular image of a bride
was passed into a cnn that was trained
on a
open source large scale image data set
the predicted data class labels that
were outputted by the cnn were
perhaps unsurprisingly things like bride
dress
wedding ceremony women as expected
however when this image which is a
prototypical example of a bride
in other parts of the world such as in
south asia
was passed into that very same cnn
now the predicted classroom labels did
not in fact
reflect the ground truth label of bride
clothing event costume art as you can
see
nothing here about a bride or a wedding
or even a human being
so clearly this is a very very
significant problem and this is not at
all the desired
or expected behavior for something that
deep learning
we may think of has solved quote-unquote
image classification
and indeed the similar behavior as what
i showed here
was also observed in another setting for
object recognition
when again this image of spices which
was taken from a home in north america
was passed in to a cnn trained to do
object detection
object detection and recognition the
labels for the detected objects in this
image were as expected
seasoning spice spice rack ingredient
as we'd expect now again
for this image of spices shown now on
the left
which was in fact taken from a home in
the philippines when that image was fed
into that very same cnn
once again the predicted labels did not
reflect the ground truth label
that this image was an image of spices
again
um pointing to something really really
concerning going on
now what was really interesting about
this analysis was
they asked okay not only do we observe
this bias but what could be the actual
drivers
and the and the reasons for this bias
and it turned out from this analysis
that the accuracy
of the object recognition model actually
correlated
with the income of the homes where the
test images
were taken and generated this points to
a clear bias
in these algorithms favoring data from
from homes of higher incomes versus
those from lower incomes
why could this be what could be the
source for this bias
well it turns out that the data that was
used to train such a model
the vast majority of it was taken from
the united states
canada and western europe but in reality
this distribution does not at all match
the distribution of the world's
population
given that the bulk of the world's
population is in east and south asia
so here i think this is a really telling
and powerful example
because it can show it shows how bias
can be perpetuated
and exist on multiple levels in a deep
learning or ai pipeline
and this particular analyses sort of
started to uncover and
unearth some of those biases
and indeed as i mentioned bias can truly
poison all stages of the ai development
and life cycle
beginning with the data where imbalances
with respect to class labels
or even features can result in unwanted
biases
to the model itself to the actual
training and deployment pipeline which
can reinforce
and perpetuate biases to evaluation and
the types of
analyses that are and should be done
to evaluate fairness and performance
across various demographics and
subgroups
and finally in our human interpretation
of the results and the outcomes and the
decisions
from these ai systems where we ourselves
can inject human error and impose our
own biases that distort
the meaning and interpretation of such
results
so in today's lecture we're going to
explore this problem of algorithmic bias
both in terms of first different
manifestations and sources of this bias
and we'll then move to discuss different
strategies to mitigate each of these
biases
and to ultimately work towards improving
uh
fairness of ai algorithms and by no
means is this is
is this a solved problem in fact the
setup and the motivation behind this
lecture
is to introduce these topics so we can
begin to think about how we can continue
to
advance this field forward all right
so let's start by thinking about some
common types of biopsies
that can manifest in deep learning
systems i think we can broadly
categorize these as being data driven or
interpretation driven
on the data-driven side we can often
face problems where data are selected
such that
proper randomization is not achieved or
particular types of data or features in
the data
are represented more or less frequently
relative to others
and also instances in which the data
that's available
to us as users does not reflect the real
world likelihood of
particular instances occurring all of
these as you'll see and appreciate are
very very intertwined and very related
interpretation driven biases refer more
to issues
in how human interpretation of results
can
perpetuate some of these types of
problems for example
in with respect to falsely equating
correlation and causation
trying to draw more general conclusions
about the
performance or the generalization of a
ai system
even in the face of very limited test
data
and finally in actually favoring or
trusting decisions from an algorithm
over that of a human and we're going to
um by by no means is this
this survey of common biases that can
exist an exhaustive list
it's simply meant to get you thinking
about different ways
and different types of biases that can
manifest so
today we're going to touch on several of
these types of biases
and i'd first like to begin by
considering interpretation driven
issues of correlation fallacy and over
generalization
all right so let's suppose we have this
plot that as you can see shows
trends in two variables over time and as
you as you notice
these the data from these two variables
are tracking very well together
and let's say and it turns out that in
fact these black points
show the number of computer science phds
awarded in the united states
and we could easily imagine building a
machine learning pipeline
that can use these data to predict the
number of
computer science doctorates that would
be awarded in a given year
and specifically we could use the red
variable because it's
it seems to correlate very well with the
number of cs doctorates
to try to as our input to our machine
learning
model to try to predict the black
variable
and ultimately what we would want to do
is you know train on a particular data
set from a particular time frame
and test in the current time frame 2021
or further beyond
to try to predict the number of computer
science phds that are going to be
awarded well it turns out that this red
variable
is actually the total revenue generated
by
arcades in a given year and it was a
variable that
correlated with the number of computer
science doctorates over this particular
time frame
but in truth it was obscuring some
underlying causal factor that was what
was ultimately driving
the observed trend in the number of
computer science doctorates
and this is an instance of the
correlation fallacy
and the correlation fallacy can actually
result in bias
because a model trained on data like
this generated
revenue generated by arcades as input
computer science doctorates as the
output
could very very easily break down
because it doesn't actually capture the
fundamental driving force
that's leading to this observed trend in
the variable that we're ultimately
trying to predict
so correlation fallacy is not just about
correlation not equating to causation
it can also generate and perpetuate bias
when wrongfully or incorrectly used
let's also consider the assumption of
over generalization
so let's suppose we want to train a cnn
on some images of mugs
from some curated internal data set and
take our resulting model and deploy in
the real world
to try to predict and identify mugs
well mug instances in the real world are
likely to not be very similar to
instances on which the model was trained
and the over-generalization assumption
and
bias means that or reflects the fact
that our model
could perform very well on select
manifestations of mugs
those that are similar to the training
examples it's seen
but actually fail and show performance
poor performance
on mugs that were represented less
significantly in data
although we expect it to generalize and
this phenomena is
can be often thought of and described as
distribution shift
and it can truly bias networks to have
worse performance
on examples that it has not encountered
before
one recent strategy that was recently
proposed to try to mitigate this source
of
bias is to start with the data set and
try to construct an improved data set
that already account for potential
distribution shifts
and this is done for example by
specifying example sets of say
images for training and then shifting
with respect to a particular variable
to construct the test data set so for
example in this instance
the distribution shift that occurs
between the train and the test
is with respect to the time and the
geographic region of the images here
or in the instance of medical images
this could mean
sourcing data from different hospitals
for each of train
validation and test and as greater
awareness of this issue of distribution
shift
um is brought to light data sets like
this could actually help
try to tame and tune back the
generalization bias that can occur
because they inherently impose this
necessity
of already testing your model on a
distribution shifted
series of examples
all right so that gives you hopefully a
sense of interpretation driven
biases and why they can be problematic
next we're going to turn most of our
attention to to
what are in my opinion some of the most
pervasive sources and forms of bias in
deep learning
which are driven by class or feature
imbalances that are present
in the data first let's consider how
class imbalances can lead to bias let's
consider
some example data set as shown here
or some some example data shown here and
let's say that this plot on the left
shows the real world distribution of
that data
that we're trying to model with respect
to some series of classes
and let's suppose that the data that is
available to us
in that data the frequency of these
classes is actually
completely different from what occurs in
the real world
what is going to be the resulting effect
on the model's accuracy
across these classes will the model's
accuracy reflect the real world
distribution of data
no what instead is going to happen
is that the model's accuracy can end up
biased
based on the data that it has seen
specifically such that it is biased
towards improved or or greater
accuracies rather
on the more frequently occurring classes
and this is definitely not desired what
is ultimately desired is
we want the resulting model to be
unbiased with respect to its performance
its accuracy
across these various classes the
accuracies across the classes should be
about the same
and if our goal is then to train
a a model that exhibits fair performance
across all these classes
in order to understand how we can
achieve that we first need to
understand why fundamentally class
imbalance can be problematic
for actually training the model to
understand
the root of this problem let's consider
a simple
binary classification task and let's
suppose we have this data space
and our task is to build a classifier
that sees points
somewhere in this data space and
classifies them as
orange or blue and we begin
in our learning pipeline by randomly
initializing the classifier such that
it divides this up the space
now let's suppose we start to see data
points they're starting to be fed into
the model
but our data set is class imbalanced
such that for every one orange point the
model sees
it's going to see 20 blue points now
the process of learning as you know from
from gradient descent
is that incremental updates are going to
be made to the classifier
on the basis of the data that it has
observed so for example
in this instance after seeing these blue
points
the shift will be to try to move the
decision boundary according to these
particular observations
and that's going to occur now we've made
one update
more data is going to come in and again
they're all going to be blue points
again due to this 1 to 20
class imbalance and as a result the
decision boundary is going to move
accordingly
and so far the random samples that we
have seen have reflected this underlying
class
imbalance but let's suppose now we see
an orange data point for the first time
what's going to happen to that decision
boundary well
it's going to shift to try to move the
decision boundary
closer to the orange point to account
for this new observation
but ultimately remember this is only one
and
one orange point and for every one
orange point we're going to see 20 blue
points
so in the end our classifier's decision
boundary is going to end up
occupying more of the blue space since
it will have seen more blue samples
and it will be biased towards the
majority class
so this is a very very simplified
example of how learning
can end up skewed due to stark class
imbalances
and i assure you that class imbalance is
a very very common problem
which you almost certainly will
encounter when dealing with real world
data that you will have to process and
curate
and in fact one setting in which class
imbalance is particularly relevant
is in medicine in healthcare and this is
because that the incidence of many
diseases
such as cancer is actually relatively
rare when you look at the general
population
so to understand why this could be
problematic and why this is not an ideal
setting for training and learning
let's imagine that we want to try to
build a
deep learning model to detect the
presence of cancer from medical images
like
mri scans and let's suppose we're
working with
a brain tumor called glioblastoma which
is the most aggressive and deadliest
brain tumor that exists
but it's also very rare occurring at a
incidence of approximately 3 out of
every 100 000 individuals
our task is going to be to try to train
a
cnn to detect glioblastoma from mri
scans of the brain
and let's suppose that the class
incidence in our data set
reflected the real-world incidence of
disease
of this disease meaning that for a data
set of 100 000
brain scans only three of them actually
had brain tumors
what would be the consequences on the
model if it was
trained in this way remember that a
classification model is ultimately being
trained to
optimize its classification accuracy so
what this model could basically
fall back towards is just predicting
healthy
all the time because if it did so it
would actually reach
99.997 percent accuracy
even if it predicted healthy for
instances when it saw a brain tumor
because that was the rate at which
healthy
occurred in its data set
obviously this is extremely problematic
because
the whole point of building up this
pipeline was to detect tumors
when they arise all right
so how can we mitigate this
to understand this we're going to
discuss two very common approaches that
are
often used to try to achieve class
balance during learning
let's again consider our simple
classification problem
where we randomly initialize our
classifier dividing our data space
the first approach to mitigate this
class in balance
is to select and feed in batches that
are class balanced
what that means is that we're going to
use data batches that
exhibit a one-to-one class ratio now
during learning our classifier is going
to see equal representation
with respect to these classes and move
the decision boundary
accordingly and once again the next
batch that comes in
is again going to be class balance and
the decision boundary will once again be
updated
and our end result is going to be a
quite a reasonable decision boundary
that divides the space roughly equally
due to the fact that
the data the model has seen is much more
informative
than what would have been seen with
starkly imbalanced data
and in practice this balanced batch
selection is an extremely important
technique
to try to alleviate this issue another
approach
is to actually weight the likelihood of
individual data points being selected
for training
according to the inverse of the
frequency at which
they occur in the data set so classes
that are more frequent will have lower
weights
classes that are less frequent will have
higher weights and the end result is
that
we're going to produce a class balance
data set where
different classes will ultimately
contribute equally to the model's
learning process
another way we can visualize this
rewaiting idea is by
using the size of these data points
to reflect their probability of
selection during training
and what example waiting means is that
we can increase the probability that
rare classes will be selected during
training
and decrease the probability that common
classes will be selected
so so far we have focused on the issue
of class balance class imbalance
and discuss these two approaches to
mitigate class imbalance
what if our classes are balanced could
there still be biases and imbalances
within each class absolutely
to get at this let's consider the
problem where we're trying to train
a facial detection system and let's say
we have an equal number of
images of faces and non-faces that we
can use to train the model
still there could be hidden biases that
are lurking within each class
which in fact may be even harder to
identify and even more dangerous
and this could reflect a lack of
diversity in the
within class feature space that is to
say
the underlying latent space to this data
so
continuing on with the facial detection
example
one example of such a feature may be the
hair color of the individuals
whose images are in our face class
data and it turns out that in the real
world
the ground truth distribution of hair
color is
about 75 to 80 percent of the world's
population having black hair
eighteen to twenty percent having brown
hair two to five percent having blonde
hair
and approximately two percent having red
hair
however some gold standard data sets
that are commonly used for image
classification
and face detection do not reflect this
distribution at all
in that they're over representing brown
and blonde hair and under representing
black hair
and of course in contrast to this a
perfectly balanced data set
would have equal representation for
these four hair colors
i'll say here that this is a deliberate
and over
deliberate oversimplification of the
problem and in truth
all features including hair color will
exist on a spectrum a smooth manifold in
data space and so ideally what we'd
ultimately like is a way that we can
capture more subtlety
about how these features are distributed
across the data manifold
and use that knowledge to actively
de-bias
our deep learning model but for the
purpose of this example
let's continue with the simplified view
and let's suppose we take this
gold standard data set and use it to
train a cnn
for facial detection what could end up
occurring at test time is that
our model ends up being biased with
respect to its performance
across these different hair color
demographics
and indeed as i introduced in the
beginning of this lecture these exact
same types of biases
manifest quite strongly in large-scale
commercial-grade
facial detection and classification
systems and together
i think these result in considerations
raise the critical question
of how can we actually identify
potential biases
which may be hidden and not overly
obvious like
skin tone or hair color and how can we
actually integrate this information into
the learning pipeline
from there going be a step beyond this
how can learning pipelines and
techniques
actually use this information to
mitigate these biases
once they are identified and these two
questions introduce an emerging area of
research
within deep learning and that's in this
idea of using
machine learning techniques to actually
improve
fairness of these systems and i think
this can be done
in two principle ways the first is this
idea of bias mitigation
and in this case we're given some bias
model data set learning pipeline
and here we want to try to apply a
machine learning technique
that is designed to remove aspects of
the signal that are
contributing to unwanted bias and the
outcome is that this bias is effectively
mitigated
reduced along the particular axis from
which we
remove the signal resulting in a model
with improved fairness
we can also consider techniques that
rather than trying to remove signal
try to add back signal for greater
inclusion of
underrepresented regions of the data
space or
of particular demographics to ultimately
increase the degree to which the model
sees particular slices of the data
and in general this idea of using
learning to improve
fairness and improve equitability is an
area of research which i hope will
continue to grow
and advance in the future years as these
as these problems gain more traction
all right so to discuss and understand
how
learning techniques can actually
mitigate bias and improve
fairness we first need to set up a few
definitions
and metrics about how we can actually
formally evaluate
the bias or fairness of a machine or
deep learning model
so for the sake of these examples we'll
consider the setting of supervised
learning
specifically classification a classifier
should ultimately produce
the same output decision across some
series of
sensitive characteristics or features
given what it should be predicting
therefore moving from this we can define
that a classifier is biased if its
decision changes
after it is exposed to particular
sensitive characteristics
or feature inputs which means it is fair
with respect to particular variable z if
the classifier's output
is the same whether we condition on that
variable
or not so for example if we have a
single binary variable
z the likelihood of the prediction being
correct
should be the same whether or not z
equals 0 or z equals
1. so this gives a a framework for which
we can think about
how to evaluate the bias of a supervised
classifier we can do this
we can take this a step further to
actually define performance metrics
and evaluation analyses to
determine these degrees of bias and
fairness one thing that's commonly done
is to measure performance across
different subgroups
or demographics that we are interested
in this is called disaggregated
evaluation
so let's say if we're working with
colored shapes this could be with
respect to the color
feature keeping shape constant or the
shape feature
keeping color constant we can also look
at the performance at the insert
intersections of different subgroups or
demographics
which in our shape and color example
would mean simultaneously considering
both color and shape
and comparing performance on blue
circles against performance
on orange squares and so on and so forth
so together
now that we've defined what a fair um
super
what a fair classifier would look like
and also some ways we can actually
evaluate bias
of a classification system we now have
the framework in place to
discuss some recent works that actually
used deep learning approaches
to mitigate bias in the context of
supervised classification so the first
approach
uses a multi-task learning setup and
adversarial training
in this framework the way it works is
that we the
human users need to start by specifying
an
attribute z that we seek to devise
against and the learning problem is such
that
we train we're going to train a model to
jointly predict and output
y as well as the value of this attribute
z so given a particular input x the
the network is going to this is going to
be passed in
to the network via embedding and hidden
layers
and at the output the network will have
two heads
each corresponding to one of the
prediction tasks
the first being the prediction of the
target label y and the second being the
prediction of the
value of the sensitive attribute that
we're trying to devise against
and our goal is to be to try to remove
any confounding effect of the sensitive
attribute
on the outcome of the task prediction
decision
this effect removal is done by imposing
an adversarial objective into training
specifically by negating the gradient
from the attribute
prediction head during back propagation
and the effect of this
is to remove any confounding effect that
that attribute prediction has on the
task prediction
when this model was proposed it was
first applied to a language modeling
problem
where the sensitive attribute that was
specified was gender
and the task of interest was this
problem of analogy completion
where the goal is to predict the word
that is likely to fill an analogy
for example he is the she as doctor is
to
blank and when a biased model was tested
on this particular analogy
the top predictions it returned were
things like nurse
nanny fiance which clearly suggested a
potential gender bias
however a de-biased model employing this
multi-task
approach with specification of gender as
the attribute
was more likely to return words like
pediatrician
or physician examples or synonyms for
doctor which suggested some degree of
mitigation of the gender bias
however one of the primary limitations
of this approach
is this requirement for us the human
user to specify the attribute
to devise against and this can be
limiting in two ways
first because there could be hidden and
unknown biases that are not
necessarily apparent from the outset and
ultimately we want to actually actually
also devise against these furthermore
by specifying what the sensitive
attribute is
we humans could be inadvertently
propagating
our own biases by way of telling the
model what we think
it is biased against and so
ultimately what we want and what we
desire is an automated system
that could try to identify and uncover
potential biases in the data
without any annotation or specification
and indeed this is a perfect use case
for generative models
specifically those that can learn and
uncover the underlying latent variables
in a data set
and in the example of facial detection
if we're given a
data set with many many different faces
we may not know what the exact
distribution of particular latent
variables
in this data set is going to be and
there could be imbalances with respect
to these different variables
for example face pose skin tone
that could end up resulting in unwanted
biases in our downstream model
and as you may have seen in working
through lab 2
using generative models we can actually
learn these latent variables
and use this information to
automatically uncover
underrepresented and over-represented
features and regions
of the latent landscape and use this
information
to mitigate some of these biases
we can achieve this by using a
variational auto encoder structure
and in recent work we showed that based
on this va
network architecture we can use this to
learn
the underlying latent structure of a
data set in a completely
unbiased unsupervised manner for example
picking up in the case of of face images
particular latent variables such as
orientation which were once again
never specified to the model it picked
up
and learned this as a particular latent
variable by looking at a lot of
different examples of faces
and recognizing that this was an
important factor
from this learned latent structure we
can then
estimate the distributions of each of
these learned latent variables
which means the distribution of values
that these latent variables can take
and certain instances are going to be
over represented
so for example if our data set has many
images of faces of a certain skin tone
those are going to be overrepresented
and thus the likelihood of selecting a
particular image
that has this particular skin tone
during training will be
unfairly high which could result in
unwanted biases
in favor of these types of faces
conversely faces with rare features
like shadows darker skin glasses
hats may be under represented in the
data
and thus the likelihood of selecting
instances with
these features to actually train the
model will be low
resulting in unwanted bias
from this uncovering of the distribution
of latent structure
we showed that this model could actually
adaptively adjust
sampling probabilities of individual
data instances
to re-weight them during the training
process itself
such that these latent distributions and
this resampling approach
could be used to adaptively generate a
more fair
and more representative data dataset for
training
to dig more into the math behind how
this resampling operation works
the key point to this approach is that
the latent space
distribution is approximated by this
joint histogram
over the individual latent variables
specifically
we estimate individual histograms for
each of the individual latent variables
and for the purpose of this
approximation assume that these latent
variables
are independent such that we can take
their product
to arrive at a joint estimate a an
estimate of the joint distribution
across the whole latent space
and based on this estimated joint
distribution we can then define the
adjusted probability
for sampling a particular data point x
during training
based on the latent space for that input
instance x specifically we define the
probability of selecting that data point
according to the inverse of the
approximated joint distribution
across latent space which is once again
defined by each of these individual
histograms and furthermore weighted by a
devicing parameter alpha
which tunes the degree of debian that is
desired
using this approach and applying it to
facial detection
we showed that we could actually
we could actually increase the
probability of resampling
for faces that had underrepresented
features
and this qualitatively manifested when
we inspected
the top faces with the lowest and
highest resampling
probabilities respectively we then could
deploy this approach
to actually select batches during
training itself
such that batches sampled with this
learned debian algorithm
would be more diverse with respect to
features such as skin tone
pose and illumination and the power of
this approach is that it conducts this
resampling
operation based on learn features that
are automatically learned
there's no need for human annotation of
what
the attributes or biases should be and
thus
it's more generalizable and also allows
for de-biasing against
multiple factors simultaneously to
evaluate how well this algorithm
actually mitigated bias we tested on a
recent benchmark data set
for evaluation of facial detection
systems that is balanced with respect to
the male and female sexes as well as
skin tone
and to determine the degree of bias
present
we evaluated performance across
subgroups
in this data set grouped on the basis of
the male female
annotation and the skin tone annotation
and when we considered the performance
first of the model without any devising
so the supposedly bias model we observed
that it exhibited the lowest accuracy
on dark males and the highest accuracy
on light males
with around a 12 difference between the
two
we then compared this accuracy to that
of the d bias models
and found that with increasing
de-biasing
the accuracy actually increased overall
and in particular on the subsets and
subgroups
such as dark males and dark females and
critically
the difference in accuracy between dark
males and light male faces
decreased substantially with the debias
model suggesting that this approach
could actually significantly decrease
categorical bias
to summarize in today's lecture we've
explored how different biases can
arise in deep learning systems how they
manifest
and we also went beyond this to discuss
some emerging strategies
that actually use deep learning
algorithms to mitigate some of these
biases
finally i'd like to close by offering
some perspectives on what i think are
key considerations for
moving towards improved fairness of ai
the first is what i like to call best
practices things that
should become standard in the science
and practice of ai
things like providing documentation and
reporting on the publication of data
sets
as well as that of models that summarize
things like training information
evaluation metrics and model design and
the goal of this being to improve the
reproducibility
and transparency of these data sets and
models as they're used and deployed
the second class of steps i think that
need to be taken
are these new algorithmic solutions to
actually
detect and mitigate biases during all
aspects of the learning pipeline
and today we consider two such
approaches but there's still
so much work to be done in this field to
really build up to
robust and scalable methods that can be
seemly
seamlessly integrated into existing and
new
ai pipelines to achieve improved
fairness
the third criterion i think will be
improvements in terms of
data set generation in terms of sourcing
and representation
as well as with respect to distribution
shift and
also formalized evaluations that can
become standard practice
to evaluate the fairness and potential
bias of new models that are
output above all i think what is going
to be really critical is a sustained
dialogue and collaboration between ai
researchers and practitioners
as well as end users politicians
corporations ethicists
so that there is increased awareness and
understanding of
potential societal consequences of
algorithmic bias
and furthermore discussion about new
solutions that can
mitigate these biases and promote
inclusivity and fairness with that i'll
conclude
and i'd like to remind you that for
those of you
uh taking the course that the entries
for the lab competitions are going to be
due today
at midnight eastern time please submit
them on canvas
and as always if you have any questions
on this lecture
the labs any other aspects of the course
please come to the course gather town
thank you for your attention
so i i lead uh ai at eui globally
and and today we're going to talk to you
about some work we've done in
information extraction
specifically deep cpcfg so hopefully
we'll introduce some concepts that you
may not have come across before
and and uh before we get started
maybe a couple of disclaimers the views
expressed
in this presentation are mine and
friday's they're not necessarily those
of our employer
and i'll let you read those other
disclaimers
ai is very important for ernst young
many of you will be familiar
with the firm we are a global
organization
of more than 300 000 people in more than
150 countries
and we provide a range of services to
our clients that include assurance
consulting
strategy and transformations and tax
both in the context of the services that
we deliver and in the context of our
clients own transformations ai is
incredibly important
we see a huge disruption happening for
many industries
including our own driven by ai and
because of that we're making significant
investments in this area
we've established a network a global
network of ai
research and development labs around the
world as you see in various parts of the
world and we also have
a significant number of global ai
delivery centers
and we there's huge energy and passion
for ai within ey
and so that we have a community of more
than four and a half thousand members
internally
and we maintain uh meaningful
relationships
with academic institutions including for
example mit
and we of course engage heavily with
policy makers regulators legislators and
ngos around issues related to ai
some areas of particular focus for us uh
the first is document intelligence which
we'll talk about
today and this is really the idea of
re using ai to read and interpret
business documents
the key phrase there is business
documents we're not necessarily talking
about emails
or web posts or social media posts or
product descriptions we're talking more
about things like contracts
lease contracts revenue contracts
employment agreements
we're talking about legislation and
regulation
we're talking about documents like
invoices and purchase orders and proofs
of delivery
so there's a very wide range of these
kinds of documents
and hundreds thousands maybe tens of
thousands of different types of
documents here
and we see these in more than 100
languages in more than 100 countries
in tens of different industries and
industry segments
at ui we have built some i think really
compelling technology in the space we've
built some
products that are deployed now in more
than 85 countries and used by thousands
of engagements
and and this space is sufficiently
important to us that we helped to
co-organize the first ever workshop on
document intelligence at nurips
in 2019 and of course we publish and
patent in this area
two other areas that are important to us
that will not be the subject of this
talk but i thought i would just allude
to
are transaction intelligence so the idea
here is that we see
many transactions uh that our clients
execute
and we review those transactions for tax
purposes
and also for audit purposes and we'll
process hundreds of billions of
transactions a year
so it's this enormous amount of
transaction data and
and there's huge opportunities for
machine learning and ai to help
analyze those transactions to help
determine for example tax or accounting
treatments
but also to do things like identify
anomalies or unusual behavior or
potentially identify fraud
another area that's very important for
us is trusted ai
so given the role that we play in the
financial ecosystem we are a
provider of trust to the financial
markets it's important that we help
our clients and ourselves and the
ecosystem at large
build trust around ai and we engage
heavily
with academics ngos and
regulators and legislators in order to
achieve this
but the purpose of this talk is really
to talk about
the purpose of this talk is really to
talk about document intelligence
and specifically we're going to talk
about information extraction from what
we call
semi-structured documents things like
tax forms
you see the document on the screen in
front of you here this is a tax form
there's going to be information in this
form that
that is contained in these boxes
and so we'll need to pull that
information out these forms tend to be
relatively straightforward because it's
consistently located
in these positions but you see to read
this information or to extract this
information it's not just a matter of
reading text
we also have to take layout information
into account
and there are some complexities even on
this document right and there's a
description of property here this is a
list and so we don't know how many
entries might be in this list might be
one there might be two there might be
more
of course this becomes more complicated
when these documents for example are
handwritten
or when they're scanned or when they're
more variable like for example
uh a check so many of you may have
personalized checks
and those checks are widely varied in
terms of their background in terms of
their layout
typically they're handwritten and
typically when we see them they're
scanned
often scanned at pretty poor quality and
so pulling out the information there can
be a challenge and again this is driven
largely by the variability
we have documents like invoices the
invoice here is very very simple
and but you'll note the key thing to
know here is that there are line items
and there are multiple line items each
of these corresponds to an individual
transaction under that invoice
and there may be zero line items or
there may be ten or hundreds or
thousands
or even in some cases tens of thousands
of line items in an invoice
invoices are challenging because a large
enterprise might have hundreds or
thousands
or even tens of thousands of vendors and
each one of those vendors will have a
different format for their invoice
and so if you try to hand code rules to
extract information from these documents
it tends to fail and so machine learning
approaches are designed really to deal
with that variation in these document
types and this complex information
extraction
challenge let me just talk about a
couple of other examples here
and you'll see a couple of other
problems this
receipt on the left hand side is pretty
typical right it's a scan document
clearly it's been crumpled or creased a
little bit the
the information that's been entered is
offset by maybe half an inch
and so this customer id is not lined up
with the customer id tag here and so
that creates additional challenges and
this document on the right hand side is
a pretty typical invoice
and you'll see the quality of the scan
is relatively poor
it contains a number of line items here
and the layout of this
is quite different than the invoice we
saw on the previous slide or on
the receipt on the left hand side here
so there's lots of variability
for the duration of the talk we're going
to refer to
this document which is a receipt and
we're going to use this to illustrate
our approach so our goal here is to
extract
key information from this receipt
and so let's talk a little bit about the
kinds of information we want to extract
so the first kind is what we call header
fields and this includes for example
the date of that receipt right when when
were these transactions executed
it includes a receipt id um or an
invoice id
and it might include this total amount
and these pieces of information are very
important for accounting purposes
make sure that we have paid the receipts
and paid the invoices that we should
they're important for tax purposes is
this expense
and taxable or not taxable have we paid
the appropriate sales tax
and so we do care about pulling this
information out
we refer to these as header fields
because often they do appear at the top
of the document
and usually it's at the top or the
bottom and but they're
information that appear typically once
right there's one total amount for an
invoice or receipt there aren't multiple
values for that
and so there's a you know a fairly
obvious way that we could apply deep
learning to this problem
we can take this document and run it
through optical character recognition
and optical character recognition and
services or vendors have gotten pretty
good
and so they can produce essentially
bounding boxes around
tokens so they produce a number of
bounding boxes and each boundary box
contains a token and some of these
tokens
relate to the information we want to
extract so this five dollars and ten
cents is the total
and so what we could do is we could
apply deep learning to classify these
bounding boxes
right we can use the input to that deep
learning could be this whole image it
could be some context of that bounding
box
but we can use it to classify these
bounding boxes
and that can work reasonably well for
this header information
but there are some challenges so here
for example there
is this dollar amount five dollars and
10 cents there's also 4.50
60 cents 60 cents over here 1.50
if we independently classify all of
those
we may get multiple of them being tagged
as
the receiptal and how do we disambiguate
those
so this problem of disambiguation is
fundamental
and and what often happens in these
systems is that there is post-processing
that encodes heuristics or rules that
are human engineered
to resolve these ambiguities and that
becomes a huge source of
brittleness and this huge maintenance
headache over time and we'll say more
about that later
the other kind of challenge we see here
are fields like this vendor address
so this vendor address contains multiple
tokens
and so we need to classify multiple
tokens as belonging to this vendor
address
and then we have the challenges to which
of those tokens actually belong to the
vendor address how many of them are
there
and what order do we read them in to
recover this address
so while a straightforward machine
learning approach
can achieve some value it still re
leaves many many problems to be resolved
that are typically resolved with this
hand-engineered post-processing this
becomes
even more challenging for line items
and throughout the talk we'll emphasize
line items because this is
where many of the significant
significant challenges arise
so here we have two line items they both
correspond to transactions for buying
postage stamps maybe they're different
kinds of postage stamp
and each one will have a description
posted stamps this one has a transaction
number associated with the two
it will have a total amount for that
transaction
might have a quantity you might have a
unit price right so there's multiple
pieces of information that we want to
extract
so now we need to identify where that
information is
we need to identify how many line items
there are we need to identify
which line items this information is
associated with so is this 60 cents
associated with this first line item or
this second one
and we as humans can read this and
computers obviously have a much harder
time
especially given the variability there
are thousands of different ways in which
this information might be organized
so this is the fundamental challenge so
these are the documents we want to
to read and on the other side of this
are
is the system of record data right
typically this information will be
pulled out
typically by human beings and entered
into some database
this illustrates some kind of database
schema if this was a
relational database we might have two
tables
the first table contains the header
information
and the second table contains all of the
line items
so this is the kind of data that we
might have in a system of record
this is both the information we might
want to extract but also
uh the information that's available to
us for training this system
for the purposes of this talk it's going
to be more appropriate to think of this
uh as a document type schema think of it
as json for example
where we have the header information now
the first three fields
and this is not exactly json schema but
it's meant to
look like that so it's these first three
fields have some kind of type
information and then we have a number of
line items and the number of line items
isn't specified it may be zero and maybe
more than one
and then each one of those has has its
own information
so our challenge then is to extract this
kind of information from those documents
and the training data we have available
is raw documents and this kind of
information
and so i want to take a little aside for
a second and talk about our philosophy
of deep learning
and you know many people think about
deep learning simply as large
deep networks we have a slightly
different philosophy
and if you think how classical machine
learning systems were built
the first thing that we would do is
decompose the problem into sub pieces
and those sub pieces in this case might
include for example something to
classify bounding boxes it might include
something
to identify tables or extract rows and
columns of tables
each one of them then becomes its own
machine learning problem and in order to
solve that machine learning problem we
have to define some learning objectives
and we have to find some data and then
we have to train that model and so this
creates some challenges right
this data does not necessarily naturally
exist right we don't for these documents
necessarily have
annotated bounding boxes that tell us
where the information
is in the document it doesn't tell us
which bounty boxes correspond to the
information we want to extract
and so in order to train in this
classical approach we would have to
create that data
we also have to define these objectives
and there may be a mismatch between
the objectives we define for one piece
of this problem and another and that
creates friction and error propagation
as we start to integrate these
pieces together and then finally
typically these systems have lots of
post-processing at the end
that is bespoke to the specific document
type and is highly engineered
so what happens is these systems are
very very brittle
if we change anything about the system
we have to change many things about the
system
if you want to take the system and apply
it to a new problem we typically have to
re-engineer that post-processing
and for us where we have thousands of
different types of document documents in
hundreds or a hundred
plus languages and you know we simply
cannot apply engineering effort to every
single one of these problems we have to
be able to apply
exactly the same approach exactly the
same software to
every single one of them and really to
me this is the core value of deep
learning as a philosophy
is it's about end-to-end training we
have this idea that we can train the
whole system
end-to-end based on the problem we're
trying to solve and the data we
fundamentally have the natural data
that we have available so again we begin
by decomposing the problem into
sub-pieces
but we build a deep network a network
component that corresponds to each of
those subproblems
and then we compose those networks into
one large network
that we train end to end and
this is great because the integration
problem appears once when we design this
network
it's easy to maintain the data
acquisition problem goes away because
we're designing this as an end-to-end
approach
to model the natural data that exists
for the problem
and of course there are some challenges
in terms of how we design these networks
and really uh that's the key challenge
that arises in this case is how do we
build these building blocks and how do
we compose them
and so we're going to talk about how we
do this in this case it's about
composing deep network
components to solve the problem end to
end
so here what we do is we treat this
problem as a parsing problem
we take in the documents and we're going
to parse them in two dimensions
and this is where some of the key
innovations are on parsing in two
dimensions
where we disambiguate the different
parses using
deep networks and so the deep network is
going to tell us of all the potential
parse trees for one of these documents
which is the most probable
or the most that matches the data the
best and then we're going to simply read
off from that parse tree
the system of record data right no post
processing we just literally read the
parse tree and we read off
that json data as output so
again just uh
uh we
so we have these documents on the
left-hand side right these input
documents we run them through ocr to get
the boundary boxes that contain tokens
sets the input to the system and then
the output of the system is this json
record
that describes the information we have
extracted from the document
right it describes the information we
extracted it doesn't describe the layout
of the document it just describes the
information we have extracted
okay so that's the fundamental approach
and the machinery
that we're going to use here uh is
context-free grammars
and context-free grammars you know
anytime you want to parse you have to
have a grammar to parse against
context for grammars for those of you
with a computer science background are
really the workhorse of computer science
they're the basis for many programming
languages and and
they're they're nice because they're
relatively easy to parse
we won't get into the technicalities of
a context for grammar i think that the
key
thing to know here is that they consist
of rules rules have a left-hand side
and a right-hand side and the way we
think about this is we can take the
left-hand side
and think about think of it as being
made up of or composed of
the right-hand side so a line item is
composed of a description and a total
amount
the descript description can be simply a
single token
or it can be a sequence of descriptions
right description can be multiple tokens
and the way we
encode that in this kind of grammar is
in this recursive fashion
okay so now we're going to apply this
grammar to parse this kind of json
and we do have to augment this grammar a
little bit to capture everything we care
about
but still this grammar is very very
simple right that's a small simple
grammar
really to capture the schema of the
information we want to extract
so now let's talk about how we parse uh
a simple line item
we have a simple line item it's postage
stamps we have three stamps each at a
dollar 50 for a total of 450.
and so the first thing we do is
uh for each of these tokens
we replace we identify a rule where that
token appears on the right-hand side
and we replace it with the left hand
side so if we look at this
postage token we replace it by d for
description
we could have replaced it by t for total
amount or c for count or p for price
in this case i happen to know that d is
the right token so i'm going to use that
for illustration purposes but the key
observation is that there is some
ambiguity here
right it's not clear and which
of these substitutions is the right one
to do and again this is where the deep
learning is going to come in to resolve
that ambiguity
so the first stage of parsing is that we
uh we substitute
everywhere we see a right-hand side of a
rule we substitute the left-hand side of
the rule resolving
ambiguity as we go and the next step is
by construction and for technical
reasons these grammars always have
they either have a single token on the
left-hand side or
they have um a
sorry this looks like maybe there's some
question here and
so they either have a single token uh on
the right-hand side or they have two
uh symbols on the right-hand side and so
since we've dealt with all the tokens
we're now dealing with these pairs of
symbols
and so we have to identify pairs of
symbols that we substitute
again with the left-hand side so this is
description description that we
substitute
with with a description and
and likewise for count and price we
substitute
with u okay so we just repeat this
process
and and get a full parse tree
where the final symbol here is a line
item so that tells us this whole thing
is linear and made up of count
price and total amount okay so as i said
there's some ambiguity
and one place for this ambiguity here is
uh three and a dollar fifty
how do we know that this is in fact a
count and a price right this could
just as easily have been a description
and a description
so resolving this ambiguity is hard but
this is the opportunity for learning
this is where the learning comes in that
can learn that typically a dollar fifty
is probably not part of the description
it probably
relates to some other information we
want to extract and so that
that's what we want the learning to
learn and so the way we do this is we
associate
every rule with a score
so each rule has a score and now we try
to use rules that have
high scores so that we produce in the
end
a parse tree that has a high total score
so what we're actually going to do is
we're going to model these scores with a
deep network so for every rule
we're going to have a deep network
corresponding to that rule which will
give
the score for that rule okay
now let me illustrate that on one simple
example here we have
a dollar fifty and this could be a
description
a total amount account or a price and we
might intuitively think well this should
be biased towards
total amount or price because it's
it's a monetary value but the way we're
going to resolve this is we're going to
apply the deep networks corresponding to
each of these rules
right there's four deep networks we're
going to apply these deep networks
and each of them will return a score and
we
expect that over time they will learn
that
the deep network for total amount will
have a higher score and the network for
price will have a higher score
so that's fundamentally the idea at this
for these
bottom set of rules these um token-based
rules
for the more uh involved rules where we
have two terms on the right-hand side
we have the similar question about
resolving ambiguity and so there's two
i think important insights here the
first is that
um you know we do have this ambiguity as
to how we tag these first
tokens we could do cp or we could do dp
but we quickly see that there is no rule
that has dp
on the right hand side and so the
grammar itself
helps to correct this error right
because there is no rule that would
allow this parse
the grammar will correct that error and
so the grammar allows us to impose some
constraints about the kind of
information these documents contain
it allows us to encode some prior
knowledge of the problem
and that's really important and valuable
and the second
uh kind of ambiguity is where you know
it is allowed
but maybe it's not the right answer
so in this case we cp
could be replaced by a u
and we're going to evaluate the model
for this rule
based on the left hand side of this tree
and the right hand side of the tree so
this is going to have two inputs the
left hand side and the right hand side
and and likewise for this rule which uh
tries to model this as a description
description
and so this each of these models will
have a score
which will help us to disambiguate
between these two
choices so the question then arises
how do we score a full tree
and i'm going to introduce a little
notation for this
this is hopefully not too much notation
but the idea is
we're going to call the full parse tree
t and we're going to denote the score
for that tree by
c t and i'm going to abuse notation here
a little bit and
re-parameterize c as having three
parameters the first is the symbol
at the root of the tree and the other
two are the span
of the sentence the span of the input
covered
by that tree
right in this case it's the whole
sentence so we use the
indices 0 to n now
the same notation works for a subtree
right where again
we the first term is the symbol at the
root of that subtree
and we have the indices of the span here
it's not the whole sentence it
just goes from i to j okay so this is
the score for
um for a subtree
let's see if there's questions here real
quick
um so this is the score
for um
for a sub tree and we're going to
define that again in terms of the deep
network
corresponding to the rule at the root of
that subtree
but also it's made up of these other sub
trees
so we're going to have terms that
correspond to the left-hand side and the
right-hand side of this tree
so these are the uh we've defined this
score now recursively
in terms of the trees that make it up
and i do see there's a question here
and it says by taking into account the
grammar constraints does this help the
neural network learn and be more sample
efficient
yes exactly the point is that we know
things about the problem
that allow the network to be more
efficient because we've applied that
prior knowledge
and that's really helpful in complex
problems like this
so okay so as i said we've defined now
to score for the entire tree
in terms of these three terms and uh
it's key to note here that actually
what we want is this to be the best
possible score that could result in d
at the root and so uh in fact
we're going to model this score as the
max over all possible rules that end
with a d
and over all possible ways to parse the
left tree and the right tree
and this might seem challenging
but actually we can apply dynamic
programming fairly straightforward
to find this in a fairly efficient
manner
okay so we've defined now a scoring
mechanism
for parse trees for these line items
and what happens is that we're then
going to be able to choose among all
possible parse trees
using that score and the one with the
highest score is the one that we
consider the most likely
the one that's most likely is the one
that contains the information we care
about
and then we just read that information
off the parse tree
and so we're now going to train the deep
network in order to select
those most likely our most probable
parse trees
so just a reminder
this first term as i've mentioned is a
deep network there's a deep network for
every single one of these rules
and then we have these recursive terms
that again are defined in terms of these
deep networks
so if we unroll this recursion
if we unroll this recursion we will
build a large network
composed out of the networks for each of
these individual rules
and we'll build that large network for
each and every
document that we're trying to parse
and so uh the the deep network is this
dynamic object that has been
composed out of solutions to these
sub-problems in order to identify
which is the correct parse tree so how
do we train that
it's fairly straightforward we use an
idea from structure prediction
and the idea in structure prediction is
we have some structure in this case
laparse tree
and we want to maximize the score of
good parse trees
and minimize the score of all other
parse trees right so
what this loss function here is trying
to do or this objective is trying to do
is
maximize the score of the correct parse
trees the ones that we see in our
training data
and minimize the scores of all other
parse trees the ones that we don't see
in our training data
and this can be optimized using
back propagation and gradient descent or
any of the machinery that we have from
deep learning
so this now becomes a classic machine
learning
our sorry deep learning optimization
problem and
and the result of this is an end-to-end
system for parsing these documents
now what i haven't talked about is the
fact that and
these documents are actually in two
dimensions so far i've just focused on
one-dimensional
data so now i'll hand over to freddie
and he will talk about how we deal with
this two-dimensional nature of this data
freddie well
thank you nigel uh okay so this is the
portion of the receipt
that we were showing earlier on and we
are going to focus on
the lines within this blue boundary
region over here
moving along what we do is we apply ocr
to the receipts and we get the bounding
boxes that represents each tokens
on the left side that is the line items
as shown in the receipt
on the right side that is the annotation
that we are trying to pass we're trying
to
get the paths to match these annotations
um what you will see is that um there
are many boxes over here
that we considered as irrelevant because
it wouldn't be
shown in the annotations and that
wouldn't be part of the information that
we want to extract
so let's call these extra boxes and to
simplify the parsing i'm going to
remove them and then after going to the
2d parsing
we're going to come back and and see how
we handle these extra boxes
so before talking about 2d parsing let
me just motivate
why we need to do 2d passing so what you
see
over here is this 2d layer of the first
line item
and if we were to take this line item
and we reduce it
into a 1d sequence what happens is that
the the description which was originally
in a contiguous
layout in the 2d layout is no longer
continuous in the 1d representation
um you can see that this the blue the
yellow region is continuous in the 2d
layout and it's no longer contiguous in
the sequence
and that's because there's this blue
region over here the 60 sense which has
truncated the description so while we
can add additional rules to
handle this situation uh it typically
wouldn't generalize to
all the situations of all cases um
a better solution would be actually to
pass it in 2d
and pausing it in 2d would be more
consistent with
how we humans interpret documents in
general
so we begin with the tokens and
as what nigel had mentioned we put it
through the deep network
the deep network is going to give us uh
what it thinks each token represents
so in this case we get the classes for
the tokens and now we can
begin to merge them beginning with the
the token in the top left corner uh is
the word postage
so we know that post stage is the
description
the most logical choice to pass is to
combine with
the token that is nearest to it and
that's the stance
so we can pass it in horizontal
direction and we can do that because
there's a rule in the grammar that says
two description boxes can merge to form
one description
now the next thing to do is we can
either pass it
to the right as we can see over here
or you know we can pass it in the
vertical direction so
which one do we choose if we do it we
deposit horizontally
like what we showed over here this works
because
there is a rule in the grammar that says
that a line can be
simply the total amount but what happens
is that all the other boxes
are left dangling there and then it
wouldn't belong to any line item
and in fact this wouldn't be a ideal
path because then it wouldn't match
the annotations that we have so
an alternative path is to pass it in the
vertical direction
as shown over here what's important to
note is that
we do not hard code the direction of the
pass
instead the deep network is going to
tell us which is the more probable path
in this case you can combine them
together
because we know that postage stamps are
description
and the d network has told us that this
string of numbers over here
there's a description you can join them
to be a description again
the next thing to do is to look at um
the box the token one over here and 60
cents
we know that one is a count six cents is
a price and we can join them because uh
we have a rule in the grammar disease
you can join a counter price to get a
simple u
now then the next thing is we can join a
description and a simple u
and we get a simple cube finally
let's not forget that we still have a
token over there
we know that the token is a total amount
finally we can
join horizontally and we and
as a result of the whole line come over
here
so moving along
that early on we have simplified the
parsing problem by
removing all these extra boxes they're
not there
but what if we put them back
if we put them back it complicates the
parsing it's not in the annotations
and we i didn't show it earlier so what
do we do
okay right
so early on we already know that postage
stamps they are descriptions
and we can join them to become a
description again
so there's this extra words over here
the transaction number words
uh what do we do about them we introduce
a new rule in the grammar and the new
rule is
saying we allow a token to be a noise so
noise becomes a
class that the deep network will
possibly return to us
if we know that transaction number they
can be classified as noise
then the next thing to do is we have
oops sorry about that we can join
two noise uh tokens together
and we get one noise token because we
have introduced a rule in the grammar
that allows us to do that
next thing we're gonna add another rule
that says
the description can be surrounded by
some noise around them
in this case i've added a rule over here
you can see
the exclamation mark here represents a
permutation on this
rule what this means is that we're not
going to put a constraint
on how these right hand side symbols can
appear
in this case noise can come before
description or description can come
after noise
in this case the example shown over here
the noise comes b for the description
and we can combine a noise and a
description together
to get another description the simple d
over here
and moving along i can combine two
description
and i get a description so you can see
that this is how i
how we handle irrelevant tokens
in in the documents so
continuing with the logic eventually we
would end up with
the right freeze for the line items in
the documents
and this is what we get matching
the current information
okay so um finally i would like to talk
about some experimental results that we
have
well our firm is mostly focused on
invoices and most of these invoices
tends to be confidential documents
so while we believe that there could be
other labs other companies also working
on the same problem
it's really very hard to find a public
data set to compare our results with
fortunately there is this interesting
piece of work from
clover ai is a lab we're doing this
company in south korea called naval corp
they also look at the problem online
items and to their credit
they have released a data set release a
set of
data set of receipts a set of reasons
that they have written about
and their paper is on exit as a preprint
the main differences between our
approach and their approach is that
what they require is for every bounding
box within the receipt to be annotated
which means every bounding box you're
gonna go in you're gonna say this
bounding box belongs to this class
and every boundary box you need to have
the associated
coordinates with it in our case
all we do is to rely on the system on
records
in front of a json format which doesn't
have the bounding box coordinates
so effectively what we are doing is that
we are
training we are relying on less
information than we should have
and based on the results we achieve
pretty comparable results
as far as possible we try to implement
the metrics close as possible to what
they described
so uh i guess with that uh i can anybody
nigel
great thanks freddie let me see if i
have control back
okay let me okay i think i do
okay so um you know this was uh a number
of people helped us helped us with this
work
and so i want to acknowledge that help
and
and and
please do get in touch we do lots of
interesting hand work at ui
and all around the globe and we are
hiring please reach out to us at
aiadyy.com
and we referenced some other work
during the talk lots of really
interesting great papers here
definitely worth having a look at and
and with that there were a couple of
questions uh in the chat that i i
thought were really
great and so maybe let me try and answer
them here because i think they also help
to clarify the question
the the the content so there was one
question which is
and can we let the ai model learn the
best grammar to parse rather than
defining the grammar constraints
and it's it's a really good question but
actually the grammar comes from the
problem
right the grammar is intrinsic to the
problem itself the grammar
can be automatically produced from the
schema of the data we want to extract
so it's it's natural for the problem
it's not something we have to invent
it comes from the problem itself there
was another question about is it
possible to share networks across the
rules
and and again really good question uh i
think there's a few ways to think about
this so number one is that
each of these rules has its own network
and we share the weights across every
application of those rules
whether that will be applied multiple
times in a single parse tree
or across multiple parse trees from
multiple documents
the other is that oftentimes we will
leverage things like a language encoder
and bert for example to embed
to provide an embedding for each of the
tokens and so there's lots of shared
parameters there so there are many ways
in which these parameters are shared
and they end up it ends up being
possible to produce relatively small
networks
to solve even really complicated
problems like this and
there was a question as to whether the
2d parsing is done greedily
and so again really good question
the the algorithm for parsing cfgs
leverages dynamic programming so it
doesn't
uh it's not a greedy algorithm it
actually produces
the highest core parse tree and it does
that in an efficient manner
so naively that algorithm looks like it
would be exponential
but with the application of dynamic
programming i believe it's
enqueued um
and then there's a question uh do the
rules make any attempt to evaluate the
tokenized data
for example total actually equaling
price times count when evaluating the
likelihood of a tree again
really good question we have not done
that yet
but it's something that we do have in
mind to do because that's a really
useful constraint right it's something
we know about the problem
is that a line item total tends to equal
the unit count times the unit price and
so that constraint should be really
valuable
in helping with a problem like this um
and then a final question the grammar
rules generalizable to different
document types
and so again these grammar rules are
fundamental or natural to the problem
that they correspond to the schema of
the information we want to extract
so that notion of generalizability of
the grammar between document types is
less important
so thank you uh i'm happy to answer
other questions
hand it back to you alex
i'm
really happy to be here today to
talk to you guys about something that
i'm very excited and interested in
because it's my research area so it's
always fun to
give a talk about your own research so
the topic of my talk is taming data set
bias
via domain adaptation um and i believe
you've already had some
material in this course talking about
sort of bias issues and perhaps fairness
so this will dovetail with that pretty
well i think
okay so um i don't probably don't need
to
tell you guys or spend a lot of time on
how successful
deep learning has been for various
applications here
i'll be focusing mostly on computer
vision applications
because that's my primary research area
so we know that
in computer vision deep learning has
gotten to the point where we can detect
different objects
pretty accurately in a variety of scenes
and we can even detect objects that are
not
real people or could be even cartoon
characters as long as we have training
data we can train models to do this
and we can do things like face
recognition and emotion recognition
so there are a lot of applications where
deep learning has been super successful
but there's also been some problems with
it
um and the one that i want to talk about
is data set bias
so in data set bias what happens is you
you have some data set let's say you're
training a computer vision
model to detect pedestrians and you want
to put it on a self-driving car
uh and so you went and collected some
data um
you labeled the the pedestrians with
bounding boxes and you trained your deep
neural network and it worked really well
on your held out test set that you held
out from that same data set
but now if you put that same model
on your car and have it try to recognize
pedestrians in a different environment
like in new england
which is where i am right now and in
fact if i look on my window that's
exactly what
it looks like there's snow uh there
could be
sort of people wearing heavy coats and
looks different from my training data
which i would say i collected it in
california where we don't see a lot of
snow
so visually this new data that i'm
supposed to label with my model looks
quite different from my training data so
this is what we refer to as data set
shift and it leads to
problems in terms of missing detections
and generally just lower accuracy of our
trained model right so it's called data
set bias it's also
referred to as domain shift right in the
primary
issue here again is that the training
data
uh looks well i'm gonna just say looks
different but i'll define a little more
more um specific in a specific way later
uh it's it looks different from the
target test data
when does data set bias happen well it
happens in
a few different scenarios
i'll just show a few here and they're
actually
i will argue that it happens with every
data set you collect but
one example is you collect as already
mentioned collect a data set in one city
and you want to test on a different city
or maybe you collect a data set from the
web and then you want to put your model
on a robot that
gets images from its environment where
the angle and the background and the
lighting is all different
another very common issue is with
simulated
training that we then want to transfer
to the real world so that's a sim to
real
data set shift very common in robotics
and another way that this could happen
is if your training data set
um is primarily
of a particular demographic say if we're
dealing with people it could be mostly
light-skinned faces and then at test
time you're given darker skin faces and
you didn't train on that kind of data so
again you have a data set bias
or perhaps you're classifying weddings
but your
training data comes from images of
weddings in western culture and then at
test time you have other cultures so you
again have a data set by so it could
happen
my point is that it can happen in many
different ways and
in fact i i believe that no matter what
data set you collect
it will have data set bias no matter
what
just because especially in the visual
domain our
visual world is so complex it's just
very hard to collect enough
variety to cover all possible situations
so let's talk about more specifically
why this is a problem
and i'll give you an example that i'll
use throughout my talk
just to put some real numbers on this
right so
we probably all know by now the mnist
data set
so it's just a handwritten digits very
common for benchmarking neural networks
so if we train a neural network on
amnest and then we test it on
the same domain on mnist we know that
we'll have very good performance
upwards of 99 accuracy this is more or
less a solved task
but if we train our network on this
street view house numbers data set which
is
uh also 10 digits the same time 10
digits but visually it's a different
domain it's from the street view
data set now when we test that model on
the mnist data set
performance drops considerably this is
really really bad performance for this
task right the 10 digits
and in fact even if we train with this
much smaller shift so this is from usps
to
mnist visually actually they look very
similar to the human eye
but there it there are some some small
differences between these two data sets
still performance drops uh pretty much
the same
uh as before and if we swap we still
have
bad performance when training on mnist
and testing on usps
so that's just to put some numbers like
even for such a
very simple task that we should have
solved a long time ago
in deep learning um it doesn't work
right so we if we have this data set
shift
and we test the model on a new domain it
pretty much
breaks um so
okay but this is a very academic data
set um
what about the real world what are the
implications
of data set bias have we seen any actual
implications of data set bias
in the real world and i would argue that
yes we have
this is one example where there have
been several studies
of face recognition models and
gender recognition models commercial
software that's being deployed for these
problems in the real world
and these studies show that facial
recognition algorithms
are far less accurate at identifying
african-american and asian faces
compared to
caucasian faces and a big part of the
reason why
is data set shift because the training
data sets for
use for these models are biased towards
caucasian faces
so another real world example that i
want to
show is a very sad example actually
where a while back there was a
self-driving
car accident it's actually the first
time that a robot has
killed a person so there was an accident
that was fatal
with an uber self-driving car and
according to some reports
the reason uh they think that the car
failed to stop is that its algorithm was
not designed to detect pedestrians
outside of a crosswalk
right so you could actually think of
this as a data set bias problem again
if your training data contains only
pedestrians
on a crosswalk which is reasonable to
assume right because
majority of the time pedestrians follow
the rules and cross
on the crosswalk and only a few times
they you know you might see people jay
walking you will probably not see a lot
of
examples like that in your data set
so you might be wondering at this point
well wait a minute can't we just fix
this problem by
collecting more data just getting more
data and labeling it
well yes we could theoretically
however it gets very very expensive
very quickly and to illustrate
why let's take this example this is
again in the self-driving domain
this is a images from the berkeley bdd
dataset
which has actually quite a variety of
domains already so it has the nighttime
images also has daytime images and the
labels here are semantic
segmentation labels so each pixel is
labeled
like with road or pedestrian so on so if
we wanted to label
um 1000 pedestrians with these polygons
that would cost around one thousand
dollars this is just
you know kind of standard market price
however if we now want to uh
multiply that times how many different
variations we want
in the pose times variations in gender
times
variations in age race clothing style so
on so on
we very quickly see how much data we
have to collect right and somewhere in
there we also want people who ride
bicycles
so this becomes this blows up very
quickly becomes very expensive so
instead maybe what we want to do is
design models that can
use unlabeled data rather than labeled
data
that's what i'm talking about today
um so now let's think about okay what
causes this
poor performance that we've already seen
and there are basically two main reasons
i think the first reason is that the
training and test data distributions
are different and you can see that in
this picture
so here the blue points are feature
vectors extracted from
the digit domain the mnist digit domain
using a network that was trained on the
digit domain so we train a network and
we
take the second to last layer
activations
and we plot them using t-sne embeddings
so that we can plot them in 2d so you
see these blue points are the training
eminence points
and then we take that same network and
extract features from our target domain
which you can see here it's basically m
this but with
different colors as opposed to black and
white
and so those are the red points and you
can see very clearly that the
distribution
over the inputs is very different in the
training and test
domains so that's one issue is that our
classifier which is trained
on the blue points will not generalize
to the red points because of this
distribution shift another
problem is actually it's a little bit
more subtle
but if you look at how the blue points
are much better clustered together
with spaces between them and so these
are clusters according to category
of the digit but the red points are
a lot more kind of spread out and not as
well clustered into categories and this
is
because the model learned discriminative
features for the source domain
and these features are not very
discriminative for the target
domain so the test target points are not
being
grouped into classes using those
features because
you know they just the kinds of features
the model needs
weren't learned from the source domain
all right so what can we do um
well we can actually do quite a lot and
here
actually is a list of methods that
you could use to try to deal with data
set shift
that are fairly simple standard things
that you could do
for example if you just use a better
backbone for your cnn
like resnet 18 as opposed to alexnet
you will have a smaller performance gap
due to domain shift
batch normalization done per domain is a
very good trick
um you can combine it with instance
normalization of course you could do
data augmentation use semi-supervised
methods
like pseudo-labeling and then what i'll
talk about today
is domain adaptation techniques
okay so let's define domain adaptation
all right so we have a source domain
which has a lot of unlabeled data
sorry which has a lot of labeled data so
we have
inputs x i and labels y i in our source
domain
and then we have a target domain which
has unlabeled data
so no labels just the inputs and
our goal is to learn a classifier f that
achieves a low expected loss
under the target distribution dt
right so we're learning on the source
labels but we want to have good
performance on the target
and a key assumption in domain
adaptation
that is really important to keep in mind
is that
in domain adaptation we assume that we
get to see the unlabeled data
we we get access to it
which is which is important we don't get
the labels
um because again we assume it's very
expensive or we just can't label it for
some reason
um but we do get the unlabeled data
okay so what can we do um i'll so here's
the outline of the rest of my talk um
and i'm
i'm sure i'm gonna go pretty quickly um
and i'll try to have time at the end for
questions so please if you have
questions note them down
um so i'll talk about kind of the
standard very
very uh at this point conventional
technique called adversarial domain
alignment
and then i'll talk about a few more
recent techniques that have been applied
to this problem
um and then we'll wrap up
okay so let's start with adversarial
domain alignment
okay so say we have our source domain
with labels
and we're trying to train a neural
network here i've split it into the
encoder
cnn it's a convolutional neural network
because we're dealing with images and
then the classifier which is just
the last layer of the network and we can
train it
in a normal way using standard
classification laws
and then we can extract features from
the encoder to plot them here
to visualize the two categories just for
illustration purposes i'm showing just
two
and then we can also visualize some
notion of discriminator between classes
that the classifier is learning this
decision boundary
now we also have unlabeled target data
which is coming from our target domain
we don't have any labels but we can take
the encoder and generate
features and as we've already seen we'll
see a distribution shift between
the source blue features and the target
orange features
so the goal in adversarial domain
alignment is to
take these two distributions and align
them
so update the encoder cnn such that
the target features are distributed in
the same way as the source
okay so how can we do this
well it involves adding a domain
discriminator think of it as just
another
neural network which is going to take
our features from the source and the
target domain
and it's going to try and predict the
domain label so its output is a binary
label
source or target domain okay
and so this domain discriminator is
trying to
distinguish the blue points from the
orange points
okay so we train it just with
classification loss on the domain labels
and then that's our first step and then
our second step is going to be to fix
the domain discriminator
and instead update the encoder
such that the encoder
results in a poor domain discriminator
accuracy so it's trying to fool the
domain discriminator by generating
features
that are essentially indistinguishable
between the source and target domain
okay so it's an adversarial approach
because of this adversarial back and
forth first we train the domain
discriminator
to do a good job at telling the domains
apart then we fix them we train the
encoder
to fool the discriminator so that it can
no longer tell the domains apart
if everything goes well we have a line
distributions
so does this actually work let's take a
look at our
digits example from before so here we
have
again two digit domains and you see
before adaptation
the distributions of the red and the
blue points were very different
now after applying this adversarial
domain alignment on the features
we can see that in fact the feature
distributions are very well aligned now
you you more or less cannot tell the
difference in terms of the distribution
between the red and the blue points
okay so it works and
not only does it work to align the
features only also works to improve
the accuracy of the classifier we train
because we're still here training this
classifier uh using the source
domain labels right and this actually is
what prevents our
alignment from diverging into something
uh you know really silly like mapping
everything to one point because
we still have this classification loss
that has to be satisfied
so the classifier also improves so let's
see how much
so here i'm going to show results from
our cdr17 paper
called ada or adversarial discriminative
domain adaptation
so with this technique we can improve
accuracy when training on these
domains and then testing on these target
domains quite a lot
so it's a significant improvement
it's a little harder on it's not as good
on the svh end to end this
shift because that is the the hardest of
these shifts
great so the takeaway uh so far
is that domain adaptation can improve
the accuracy of the classifier on target
data
without requiring any labels at all so
we didn't label
our target domains here at all we just
trading with unlabeled data
and so you can think of this as a form
of unsupervised
fine tuning right so fine tuning is
something we often do
to improve a model on some target task
but it requires labels so this is
something we can do if we have no labels
and we just have a label data we can do
this kind of unsupervised fine-tuning
great so
okay so so far i talked about domain
alignment in the feature space
because we were updating features next i
want to talk about pixel space alignment
so the idea in pixel space alignment is
what if we could take
our source data the images themselves
the pixels and actually
just make them look like they came from
the target domain
and we can actually do that thanks to
adversarial generative models organs
which work in a very similar way to what
i already described
but they the discriminator is
looking at the whole image not the
features but the actual image that's
being generated
so we can take this idea and apply it
here
and train again that will
take our source data and translate it in
the image domain to make it look like it
actually
comes from the target domain right so we
can do this because we have
unlabeled target data and there are a
few approaches for this
uh just for doing this image to image
translation
a famous one is called cyclogan
but essentially these are conditional
gans
that use some kind of loss to align the
two domains
in the pixel space
so what's the point of this well if we
now have
this translated source data we still
have labels for this data but it now
looks like it comes from the target
domain so
we can just train on this new fake
target like data with the labels and
hopefully it improves our classifier
error on the target
by the way we can still apply our
previous
feature alignment so we can still add a
domain discriminator on the
features just like we did before and do
these
two things in tandem and they actually
do improve performance
when you do both on many problems
okay so let me show you an example
here's a
training domain which is uh so we're
trying to
do semantic pixel
labeling so our goal is for a neural
network is to label each pixel as
one of several categories like road or
car
or pedestrian or sky and
we want to train on this gta domain
which is
from the grand theft auto game which is
a nice source of data because it
basically is free
the labels are free we just get them
from the game
and then we want to test on this
cityscapes dataset which is a real world
dataset collected
in germany in multiple cities so you can
see what it looks like
so i'm going to show you the result of
doing pixel
to pixel domain alignment between these
two domains
so you see that here we're actually
taking the real
data set and translating it to the
game so the original video here is from
cityscapes and we're translating it to
the gta game
all right so what happens if we apply
this
idea to domain adaptation
um so here our source domain is gta
here's an example of the adapted source
image
that we take from gta and translate it
now into the real
domain and then when we
then train the classifier on these
translated images
our accuracy goes up from 54 to 83.6
per pixel accuracy on this task so it's
a really good improvement in accuracy
again without using any
additional labels on the target domain
and also going back to our digit problem
remember that really difficult
shift we had from the street view image
digits to the mnist digits
well now with this pixel
space adaptation we can see that we can
take those source images
from from the street view domain and
make them
look like mnist images so this middle
plot middle
image shows the images on the left that
are original from svhn and we
translated them to look like amnest and
so if we train on these now we can
improve our accuracy to 90.4
on mnist and if we compare this with our
previous result using only feature space
alignment
we were getting around 76
so so we're improving on that
quite a bit
so the takeaway here is
that unsupervised image to image
translation can
discover and align the corresponding
structures in the two domains so there
so there is corresponding structure
right we have digits
in both domains and they have similar
structure and what this method is doing
is trying to
um align them by discovering these
structures and making them
correspond to each other
great um so next i want to move on to
talk about fu shot pixel alignment
okay so so far i didn't tell you this
explicitly but we actually assume that
we have
quite a lot of unlabeled target data
right so in the case of
that game adapting from the game to the
real data set
we took a lot of images from the real
world
uh they weren't labeled but we had a lot
of them so we had like
i don't know how many thousands of
images what happens if we only have a
few images from our target domain
well it turns out these methods that i
talked about can't really handle that
case
they need a lot more images
so what we did was with
my graduate student and my collaborator
at
nvidia we came up with a method that can
do translation
with just one or a few maybe two or
three or up to five images
in the target domain so suppose we have
our source domain where we have a lot of
images that are labeled
here we're going to look at an example
of different animal
species so the domain will be the
species
of the animal so here we have a
particular breed of dogs
and now we want to translate this image
into a different domain
which is this other breed of dog but we
only have one example of this breed of
dog
so our target domain is only given to us
in one image
so and then our goal is to output a
translated version of our source
image that preserves the content of that
source image
but adds the style of the target domain
image
so here the content is the pose of the
animal and the style
is the species of the animal so in this
case it's the breed of the dog
and you can see that we're actually able
to do that
fairly successfully because as you can
see the
we've we've preserved the pose of the
dog but we've changed the
the breed of the dog to the one from the
target image
okay so so this is a pretty uh cool
result
um and the way we've achieved this is
by modifying an existing model called
funet which you see on the left here
basically by updating the style encoder
part of this
model so we call it coco or content
conditioned style encoder
and so the way our model works is it
takes the content image and the style
image
it encodes the content using an encoder
this is just a convolutional network
and then it also takes both the style
and the content
encodes it as a style vector
and then this image decoder takes the
content
vector and the style vector and combines
them together to generate the final
output image
and there's there's a gan loss on that
image
make sure that we're generating um
images that look like our target
so the main difference between the
previous work
unit and ours that we call cocof unit
is that this style encoder is structured
differently
it's conditioned on both the content
image and the style image
okay so if we um kind of look under the
hood of this model a little more some
more detail
the main difference again and this is in
the style encoder
right so it takes the style image um
encodes it with features and it also
learns
a separate style bias vector which is
concatenated with the
image encoding and these are parameters
that are learned for the entire
data set so they're they're constant
they don't believe they don't depend on
the image
essentially what that does is it helps
the model kind of learn
how to account for pose variation
because in different images we'll have
sometimes very drastic change in pose in
one image we see
the whole body of the animal and then we
could have very occluded
animal with just the head visible like
in this example
and then the content encoding is
combined with these
style encodings to produce the final
style code
which is used in the adaptive instance
normalization framework if you're
familiar with that if not
don't worry about it just some way to
combine these two
vectors to generate an image
so here is some example outputs from our
models
on top we have a style so it's an image
from our target
species that we want our animal to look
like
and then below that is the content which
is
the the source essentially the pose that
we want to preserve
and then at the bottom in the bottom row
you see the
generated result which our model
produced
and so you can see that we're actually
able to preserve
the pose of the content image pretty
well
but combine it with the style or the the
species of the
source sorry of the target style image
and sometimes we even you know make
something that is
a cat look more like a dog because the
the target domain is a dog breed but
the pose is the same as the original cat
image
or here in the last one it's actually a
bear that's
generated to look like a dog
so if we compare this to the previous
method called
unit that i mentioned before we see that
our model
is getting significantly better
generations than funit
which in this case a lot of the time
fails to produce
realistic images it's just not
generating images that are convincing or
photorealistic
and so here i'm going to play a video
just to show you a few more results here
we're actually taking
the whole um a whole video
and translating it into some target
domain so you see various domains on top
here
so the same input video is going to be
translated into each of these target
domains
where we have two images for each target
domain
right so the first one is actually a fox
and now there's another example here
with different bird species
so you can see that the pose of the bird
is preserved from the original
video but its
species is changed to the target
so there's varying levels of you know
success there but overall it's doing
better than the previous approach
and here's another final example here
we're again taking the content image
combining it with the style and
generating
the output not really sure what species
this would be
some kind of strange new species
okay so the takeaway is that by
conditioning on the content
and style image together we're able to
improve the encoding of style
and improve the the domain translation
in this view shot case
all right so uh i have a little bit of
time left i think
how much time do i have actually about
10 minutes
yes okay so in just the last few minutes
i want to
talk about more recent work that we've
done that
goes beyond these alignment techniques
that i talked about and actually
improves on them
and the first one is self-supervised
learning
so one assumption that all of these
methods i talked about
make is that the categories are the same
in the source and target
and they actually break if that
assumption is violated
so why would we violate this assumption
so suppose we have a source domain
of of objects and we want to transfer to
a target domain
from this real source to say a drawings
domain
but in the drawings domain we have some
images of
which some of those images are the same
categories that we have in the source
but some of the source categories are
missing in our target domain
suppose like we don't have cup or cello
and also we might even have new
categories
in the target domain that are not
present in the source
okay so here what we have is a case of
category shift
not just feature shift not just visual
domain shift but actually the categories
are shifting and so this is a difficult
case for domain alignment because in
domain alignment we always assume that
the whole domain should be aligned
together
and and if we try to do that in this
case we'll have
catastrophic uh adaptation results so we
actually could
do worse than just doing nothing doing
no adaptation
so uh in our recent paper from europe's
in 2020 we propose
a solution to this that uses
doesn't use domain alignment but uses
self-supervised learning
okay so the idea is let's say we have
some
source data which is labeled that's the
blue points here we have some
target which is unlabeled and some of
those points could be
unknown classes right
so the first thing we do is we find
points pairs of points that are close
together
and we train a feature extractor
in such a way that these points are
embedded close together so we're
basically
trying to cluster neighboring points
even closer together
while pushing far away points even
further apart
and we can do that because we're
starting with a pre-trained model
already so let's say it's pre-trained on
imagenet so already gives us a pretty
good initialization
so after this neighborhood clustering
which is an unsupervised
loss or we can call it self-supervised
we get a better clustering of our
features that already
is clustering the unlabeled
target points from the known classes
closer to the source
points from the known classes and then
it's clustering the yellow
unknown classes in the target away from
those
known classes now what we want to do
is add an entropy separation loss which
further encourages points that have
excuse me points that um
have a certain entropy away from the
known classes so this is
essentially an outlier rejection
mechanism right
so if we look at a point and we see that
it has very high entropy
it's probably an outlier so we want to
reject it and
push it even further away and so finally
what we obtain
is an encoder that gives us this feature
distribution where
points of the same class are clustered
close to the source but points of novel
classes are clustered away from the
source
okay so if we apply it on this data set
called the vista challenge which
is training on synthetic images
and adapting to a target domain which is
real images
but some of those categories are missing
in the target
and again we don't know which ones in
real life because the target is
unlabeled
right so if we approve if we apply this
dance approach that i just described we
can improve performance
compared to a lot of the recent domain
adaptation methods
and also compared to just training on
the source so that's
we get pretty low performance if we
trade only on the source data
um and then if we do this uh domain
alignment on the entire domain
this that's this d a and n method we
actually see that we
have worse accuracy than doing nothing
than just training on the source
again because this same category uh
assumption is violated in this problem
okay but with our method we're actually
able to do much better than just
training on source
and improve accuracy okay and then
finally i want to mention another cool
uh
idea that um has become uh
more prevalent recently in
semi-supervised literature and we can
actually apply it here as well
so here we start again with
self-supervised pre-training
on the source and target domains
but in this case we're doing a different
uh cell supervised task instead of
clustering points here we are
predicting rotation of images so
we can rotate an image and we know
exactly what orientation it's in but
then we train our feature extractor to
predict
that orientation for example is it is it
rotated 90 degrees or zero degrees
okay so but again that's just another
self-supervised task it helps us
pre-train a better feature encoder which
is
um more discriminative for our source
and target domain
and then we apply this consistency loss
so what is the consistency loss
so here we're going to do some data
augmentation on our unlabeled images
okay so we're going to take our
pre-trained model and then
use that model to generate probability
distributions
of the target uh sorry of the class um
on the original image and also on the
augmented unlabeled image where the
augmentation is
you know cropping color
transformation adding noise adding small
rotations
and things like that so it's it's
designed to preserve the category
of the object but it changes the image
and then we take these two probability
outputs and we
add a loss which ensures that they're
consistent right so we're telling our
model look
if you see an augmented version of this
image you should still predict the same
category
for that image we don't know what it is
because the image is unlabeled but it
should be the same as the original image
so with this idea so we call the
combination of this
rotation prediction pre-training and
consistency training we call this pack
and this is just one small
taste of the results we got just because
i don't have much time but
essentially here we are again adapting
from the synthetic
data set in the visited challenge to
real images
but now we are assuming a few examples
are labeled in our target domain and
we are actually able to just with this
pack method improve
a lot on the domain alignment method
which is called mme that's our
previous work in this case so
basically the point that i want you to
take away from this is that domain
alignment is not
the only approach and we can use other
approaches like
self-supervised training and
consistency training to improve
performance on
target data all right so i'll stop here
just to summarize what i talked about
i hope i've convinced you that data set
bias is a major problem and
i've talked about how we can solve it
using domain adaptation techniques which
try to
transfer knowledge using unlabeled data
and we can think of this as a form of
unsupervised fine-tuning
and the technique i talked about include
adversarial alignment
and also some other techniques that
are relying on self supervision and
consistency training
and so i hope you enjoyed this
talk and if you have any questions
they'll be very happy to answer them
great yeah thanks for a nice
introduction
um i'm gonna talk to you about 3d
content creation and particularly
deep learning techniques to facilitate
3d content creation
most of the work i'm going to talk about
is the work um
i've been doing with my group at nvidia
and the collaborators but it's going to
be a little bit of my work at ufc as
well
all right so you know you guys i think
this is a deep learning class right so
you heard all about how ai has made you
know so much progress
in the last maybe a decade almost
but computer graphics actually was
revolutionized as well with you know
many new rendering techniques or faster
rendering techniques
but also by working together with ai
so this is a latest video that johnson
introduced a couple of months ago
[Music]
quietly so this is all done
all this rendering that you're seeing is
done uh
real time it's basically rendered in
front of your eyes
and you know compared to the traditional
game you're used to maybe real time in
gaming
but here there's no baked lights there's
no brake light
everything is computed online physics
real time retracing lighting everything
is done online
what you're seeing here is rendered in
something called omniverse
is this visualization a collaboration
software that nvidia
has just recently released so you guys
should check it out
it's really awesome
all right
[Music]
oops these lights always get stuck
yeah so when i joined nvidia this was
two years and a half ago
it was actually the orc that i'm in uh
was creating the software called
omniverse the one that i just showed
and i got so excited about it and you
know i wanted to somehow contribute
in this space so somehow introduce ai
in in into this content creation and
graphics pipeline
and and 3d content is really everywhere
and graphics
is really in a lot of domains right so
in architecture
you know designers would create office
spaces apartments whatever everything
would be
done uh you know you know in some some
modeling software with computer graphics
right such that you can judge whether
you like some space
before you go out and build it all
modern games are all like
heavy 3d um in film
there's a lot of computer graphics in
fact because directors just want too
much out of characters or humans
so you just need to have them all done
with computer graphics and animate
in realistic ways now that we are all
home
you know vr is super popular right
everyone wants a tiger in the room or
have a 3d character version 3d avatar of
yourself
and so on there's also robotics so
healthcare and robotics
there's actually also a lot of computer
graphics and and
in these areas and these are the areas
that i'm particularly excited about
and and why is that um it's actually for
simulation
so before you can deploy any kind of
robotic system in the real world
you need to test it in a simulated
environment
all right you need to test it against
all sorts of challenging scenarios
on healthcare for you know robotic
surgery
robotics are driving cars you know
warehouse robots
and and stuff like that i'm going to
show you this uh
simulator called drive sim that nvidia
has been developing
um and this is uh this video is a couple
of years old
now it's not a lot better than this um
but basically simulation is kind of like
a game
it's really a game engine for robots
where now you expose a lot more out of
the
game engine you want to have the creator
the roboticist some control over the
environment
right you want to decide how many cars
you're going to put in there what's
going to be the weather
night or day and so on so this gives you
some control over the scenarios you're
going to test against
but the nice thing about you know having
this computer graphics pipeline is
um everything is kind of labeled in 3d
you already have created a 3d model of
car you know it's a car
and you know the parts of the car is you
know something is a lane
and so on and instead of just rendering
the picture you can also render
you know grand truth for ai to both
train on and be tested against
right so you can get ground truth lanes
crunch with weather ground truth
segmentation all that stuff that's super
hard to collect in the real world
okay um my kind of goal would be you
know
if we if we want to think about all
these applications and particular
robotics
you know can we simulate the world
in some way can we just load up a model
like this
which looks maybe good from far but we
want to create really good content at
street level
and you know both assets as well as
behaviors and just make these virtual
cities alive
so that we can you know test our robot
inside this
all right so it turns out that actually
is super slow let me play this require
significant human effort
here we see a person creating a scene
aligned with a given real world image
the artist places scene elements edits
their poses
textures as well as scene or global
properties
such as weather lighting camera position
this process ended up taking four
hours for this particular scene
so here the artist already had the
assets you know
bottom online or whatever and the only
goal was to kind of recreate the scene
above
and it already took four hours right so
this is really really slow
and i don't know whether you guys are
familiar with you know games like grand
theft auto
that was an effort by a thousand
engineers a thousand people working for
three years
um basically recreating la los
angeles um going around around the city
and taking tons of photographs you know
250 000 photographs many hours of
footage
anything that would give them you know
an idea of what they need to replicate
in the real world
all right so this is where ai can help
you know we know computer vision we know
deep learning can we actually just take
some footage
and recreate these cities both in terms
of the construction the
the assets as well as behavior so that
we can simulate all this content
or this live content all right
so this is kind of my idea what we need
to create and i really hope that some
guys
you know some of you guys are are going
to be equally excited about these topics
and i'm going to work on this
so i believe that we we need ai in this
particular area so we need to
be able to synthesize worlds which means
both
you know scene layouts uh you know where
am i placing these different objects
maybe map
of the world um assets so we need some
way of creating assets like
you know cars people and so on in some
scalable way so
we don't need artists to create this
content very slowly
as well as you know dynamic dynamic
parts of the world so scenarios
you know which means i need to be able
to
have really good behavior for everyone
right how am i going to drive
as well as you know animation which
means that the human
or any articulated objective animate
needs to look realistic
okay a lot of this stuff you know it's
already done there for any game the
artists and engineers need to do that
what i'm saying is can we have ai to do
this much much better much faster
all right so you know what i'm going to
talk about today is kind of like our
humble beginning so
this was this is the main topic of my um
you know toronto nvidia lab
and and i'm gonna tell you a little bit
about all these different topics that we
have been slowly addressing but there's
just so much more to do
okay so the first thing we want to
tackle is can we synthesize
worlds by just maybe looking at real
footage that we can collect let's say
from a self-driving platform
so can we take those videos and and you
know
train some sort of a generative model is
going to generate scenes that look like
the real city that
you know we want to drive in so if i'm
in toronto i might need brick walls if
i'm in la
i just need many more streets like i
need to somehow personalize this content
based on the part of the world that i'm
gonna be in
okay if you guys have any questions just
write them up i
i like if the uh lecture is interactive
all right so how can we compose scenes
and our thinking was really kind of
looking into how games
are built right in games you know
people need to create very diverse
levels so they need
to create in a very scalable way very
large walls
and one way to do that is using some
procedural models
right or probabilistic grammar which
basically tells you you know rules
about how the scene uh is created
such that it looks like a valid scene so
in this particular case and i would
i would sample a road right with some
number of lanes and then on each lane
you know
sample some number of cars and maybe
there's a sidewalk next to a lane with
maybe people on walking there and
there's trees
or something like that right so this
this this probabilistic models can
be fairly complicated you can quickly
imagine how this can become complicated
but at the same time it's not so hard to
actually write this anyone could
would be able to write a bunch of rules
about how to create this content
okay so it's not it's not too tough but
the tough is to really the
the tough part is you know setting all
these distributions here
and you know such that the render scenes
are really going to look like your
target content
right meaning that if i'm in toronto
maybe i want to have more cars
if i'm in a small village somewhere i'm
going to have less cars so for all that
i need to go and
you know kind of personalize these
models set the distributions correctly
so this is just some one example of you
know sampling from
a probabilistic model here the uh the
probabilities for the orientations of
the cars become
randomly set but there's so much the
scene already looks
kind of kind of okay right because it
already incorporates all the rules that
we know about the world
and the model will be needing to
to training all right so you can think
of this as some sort of a graph
right where each node defines the type
of asset we want to place
and then we also have attributes meaning
we need to have location
height pose anything that is necessary
to actually
place this car in the scene and render
it
okay and and this this these things are
typically said by an artist
right they they look at the real data
and then they decide you know
how many pickup trucks i'm going to have
in the city or so on
all right so basically they said this
distribution by hand
what we're saying is can we actually
learn
this distribution by just looking at
data
okay and we had this paper column
metasim a couple of years ago
where the the idea was let's assume that
the structure
of the scenes that i'm sampling so in
this particular case
apps
you know how many lanes i have how many
cars i have
that comes from some distribution that
artist has already designed
so the the graphs are going to be
correct
but the attributes um should be modified
so if i sample this original scene graph
from
that i can render like you saw that
example before the cars were kind of
randomly rotated and so on
the idea is can a neural network now
modify the attributes of these nodes
modify the rotations the colors maybe
even type of object
such that when i render those those
scene graphs
i get images that look like real images
that i have recorded
in distribution so we don't want to go
after
exact replica of each scene we want to
be able to
train a generative model it's going to
synthesize images that are going to look
like images we have recorded
that's the target okay so basically we
have some sort of a graph neural network
that's operating
on scene graphs and it's trying to
repredict attributes for each node
i don't know whether you guys talked
about graph neural nets
and then the loss that's coming out is
through this renderer here
and we're using something called maximum
indiscreptency
so i'm not going to go into details but
basically the idea is you could
you need to compare two different
distributions you could compare them by
you know comparing the means of the two
distributions or maybe higher order
moments
and mmd was designed to to compare
higher order moments
okay now this last can be back prop
through this non-differentiable renderer
back to
graph neural net okay and we just use
numerical gradients to do this step
and the cool part about this is we
haven't
really needed any sort of annotation on
the image we're comparing images
directly
because we're assuming that the image
the synthesized images already look
pretty good
all right so we actually don't need data
we just need to drive around and record
these things
okay you can do something even cooler
you can actually try to
personalize this data to the task you're
trying to solve later
which means that you can train this
network to generate data
that if you train some other neural net
on top of this data it's an object
detector
it's going to really do well on you know
whatever
task you have in the end collected in
the real world
okay which might not mean that the
object need to look really good in the
scene you just might
it just means that you need to generate
scenes that are going to be useful for
some
network that you want to train on that
data
okay and that you you again back prop
this
and you can do this with reinforcement
learning okay
so this was now training the
distribution for the attributes we were
kind of the easy part
and we were sidestepping the issue of
well what about the structure of these
graphs
meaning if i had always generated you
know
five or eight or ten cars in a scene but
now i'm in a village
i will just not train anything very
useful right so the idea would be
can we learn the structure the number of
lanes the number of cars
and so on as well okay and and it turns
out that actually you can do this as
well
where here we had a probabilistic
context free grammar
which basically means you have a you
have a root note you have some symbols
and
which can be non-terminal terminal
symbols and
rules that they that basically expand
non-terminal symbols
into new symbols so an example would be
here
right so you have a road which you know
generates lanes lanes can go into lane
or more lanes
right and so on so these are the the
rules okay
and basically what we want to do is we
want to train a network
that's going to learn to sample from
this probably the context-free grammar
okay so we're going to have some sort of
a latent vector
here we know where we are
in the tree that or the graph we've
already generated
before so imagine we are in in we have
sampled
some lane or whatever so we now we know
the
the corresponding symbols that we can
actually sample from here
we can use that to mask the
probabilities
for everything else out all right and
our network is basically gonna
learn how to produce the correct
probabilities for
the next symbol we should be sampling
okay so basically
at each step i'm going to sample a new
rule until i hit
all the terminal symbols okay
that basically gives me something like
that these are the sample the rules in
this case
which can be converted to a graph and
then using the previous method
we can you know augment this graph with
attributes and then we can render the
scene
okay so basically now we are also
learning how to generate um the the the
actual scenario the actual structure
of the same graph and the attributes
and and this is super hard to train so
there's a lot of bells and whistles to
make this to work
but essentially because this is all
non-differentiable steps you need
something like reinforcement learning
and and there's a lot of tricks to
actually release to work
but i was super surprised how well you
this can actually
turn out so on the right side you see
samples from the real data set
uh kitty is like a real driving data set
on the left side
is samples from probabilistic grammar
here we've set this first probabilities
manually and we purposely made it really
bad
which means that this probably is the
grammar when you sample you got really
few cars
almost no buildings and you can see this
is like almost not
not populated scenes after training
you the generative model learned how to
sample this kind of scenes
because they were much closer to the
real target data
so these were the final trained things
okay and now how can you actually
evaluate that we have done something
reasonable here
you can look at for example the
distribution of cars in
the real real data set this is kitty
over here
so here you will have a histogram of how
many cars you have
in each scene um you have this
orange guy here which is the prior
meaning this badly initialized prolistic
grammar
where we only were sampling most of the
time very few cars
and then the learned model which is the
green the line here
so you can see that the generated scenes
really really closely follow
this distribution of the real data
without any single annotation
at hand right now you guys could argue
well it's super easy
to write you know these distributions by
hand
and and we're done with it i think
there's just this just shows that this
can work
and the next step would just be make
this really large scale
make this you know really huge
probabilistic models where it's hard to
tune all these parameters by hand
and the cool part is that because
everything can be trained now
automatically from real data no any end
user can just take this and it's going
to train on their end they know they
don't need to go and
set all this stuff by hand
okay now the next question is you know
how can i evaluate
that my model is actually doing
something reasonable
and one one way to do that is by
actually
sampling from this model synthesizing
these images along with gran truth
and then train some some you know end
model like a detector on top of this
data and testing it on the real data
and and just seeing whether the
performance has somehow improved
um compared to you know let's say on
that badly initialized
um probabilistic grammar and it turns
out that that's
that's the case okay
now this was the example shown on
driving but
oh sorry so so this model is is just
here i'm just showing basically what's
happening during training let me just
go quickly so the first snapshot is the
first sample from the model
and then what you're seeing is how this
model is actually training so how is
modifying the scene during training
let me show you one one more time
so you can see the first frame was
really kind of badly placed cars
and then it's slowly trying to figure
out where to place them
and to be correct and of course this
generative model
right so you can sample tons of scenes
and everything comes labeled
cool right um this model here was shown
on
on on driving but you can also apply it
everywhere else like in other domains
and here you know medical
or healthcare now is very um you know
important in particular these days when
everyone is stuck at home
um so you know can you use something
like this to also synthesize
medical data and what do i mean by that
right so
doctors need to take you know city or mr
mri
volumes and go and label every single
slice of that with you know let's say a
segmentation mask
such as they can then train like a you
know cancer segmentation or
card segmentation or lung segmentation
kobe detection whatever right
so first of all data is very hard to
come by
right because in some diseases you just
don't have a lot of this data
the second part is that it's actually
super time consuming and you need
experts to label that data
so in the medical domain it's really
important if we can actually somehow
learn how to synthesize this data
label data so that we can kind of
augment the real data sets with that
okay and the model here is going to be
very simple again you know we have some
generative model let's go from a latent
codes to some parameters of a
of a mesh in this case this is our asset
within a material map
and then uh we synthesize this with a
physically based
um ct simulator uh which
you know looks a little bit blurry and
then we train a enhancement model with
something like again
and then you get simulated data out
obviously again there is a lot of belts
and whistles
but you know you can get really nice
looking
synthesized volumes so here the users
can actually play with the shape
of the heart and then they can click
synthesize data
and you get some some labeled volumes
out where the label is basically the
stuff on the
left and this is the simulated sensor in
this case
okay all right so now we talked about
using
procedural models to generate to
generate worlds
and of course the question is well do we
need to write all those rules can we
just learn how to recover all those
rules
and here was our first take on this um
and here we wanted to generate or learn
how to generate
city road layouts okay which means
you know we want to be able to generate
something like that where you know the
lines over here representing roads
okay this is the base of any city
and we want to again have some control
over these worlds we're going to have
something like interactive generation i
want
this part to look like cambridge it's
parked to look like new york inspired to
look like
uh toronto whatever and we want to be
able to generate or synthesize
everything
else you know according to these styles
okay you can interpret road layout
as a graph okay so what does that mean i
have some control points
and two control points being connected
means i have a
road line segment between them
so really the problem that we're trying
to solve here is can we have a neural
net generate
graphs graphs with attributes where each
attribute might be an x y location
of a control point okay and again
giant graph because this is an entire
city we want to generate
um so we had actually a very simple
model where
you're kind of iteratively generating
this graph and imagine that we have
already
you know generated some part of the
graph what we're going to do is take an
a node from from like an unfinished set
what we call we encode every path
that we have already synthesized and
leads to this node which basically means
we wanna
we wanna kind of encode how this node
already looks like
what are the roads that it's connecting
to and we want to generate the remaining
nodes basically how these roads continue
in this case okay and this was super
simple you just have like
r and n's encoding each of these paths
and one
rnand that's decoding these neighbors
okay and you stop where basically you
hit some
predefined size of the city
okay let me show you some some results
so here you can condition on the style
of the city so you can generate
barcelona or berkeley
you can have this control or you can
condition on part of the city being
certain style
and you can use the same model the
generative model to also parse
real maps or real aerial images and
create and
create variations of those maps for
something like simulation because for
simulation we need to be robust
uh to the actual layouts
so now you can turn that graph into an
actual small city
where you can maybe procedurally
generate the rest of the content like we
were discussing before
where the houses are where the traffic
signs are and so on
cool
right so now we can generate you know
the map of the city
um we can place some objects somewhere
in the city so we're kind of close to
our goal of
synthesizing worlds
but we're still missing objects objects
are still a pain that the artists need
to create
right so all this content needs to be
manually designed and that just takes a
lot of time to do
right and maybe it's already available
you guys are going to argue that you
know for cars you can just go online and
pay for this stuff
i first of all it's expensive and second
of all it's not really so widely
available for certain classes like if i
want a raccoon because i'm in toronto
there's just
tons of them there's just a couple of
them and they don't really look like
real raccoons
right so the question is can we actually
do this all these tasks by taking just
pictures
and synthesizing this content from
pictures right so
ideally we would have um something like
an image
and we want to produce out you know a 3d
model
3d texture model
right there can i then insert in my real
scenes
and ideally we want to do this on
just images that are widely available on
the web
right i think the new iphones all have
lidar so maybe this world
is going to change because everyone is
going to be taking 3d pictures
right with some 3d sensor but right now
the majority of pictures that are
available of objects on flickr let's say
it's all single images people just
snapshotting
a scene or snapshotting on a particular
object
so the question is you know how can we
learn from all the data
and go from an image on the left to
a 3d model and in our case we're going
to want to produce
as an output from the image and mesh
which basically has
you know location of vertices xyz and
you know some color material properties
on each vertex right and 3d vertices
along with faces which means which
vertices are connected that's basically
defining this 3d object
okay and now we're going to turn to
graphics to help us
with our goal to do this from you know
the
kind of without supervision learning
from the web
okay and in graphics we know that images
are formed by geometry
interacting with light right that's just
principle of rendering
okay so we know that you can you you if
you have a mesh if you have some
light source or sources and you have a
texture
and also materials and so on which i'm
not writing out here
and some graphics renderer you know
there's many issues choose from
you get out a rendered image okay
now if we make this part differentiable
if you make the graphics
renderer differentiable then maybe there
is hope of going the other way
right you can think of computer vision
being inverse graphics
graphics is going for 3d to images
computer vision wants to go from images
into 3d
and if this model is differentiable
maybe there's hope of doing that
okay so there's been quite a lot of work
lately on
basically this kind of a pipeline with
different modifications
um but basically this summarizes the the
ongoing work
where you have an image you have some
sort of a neural net that you want to
train
and you're making this kind of button
like predictions here which
smash light texture maybe material okay
now instead of having the loss over here
because you don't have it
you don't have the ground truth mesh for
this car
because you otherwise you need to
annotate it what we're going to do
instead
is we're going to send these predictions
over to this renderer
which is going to render an image and
we're going to have the loss defined
on the rendered image and the input
image
we're basically going to try to make
these images to match
okay and of course there's a lot of
other losses that people use here like
multi-view
alloys you're assuming that in training
you have multiple pictures
multiple views of the same objects you
have masks and so on
so there's a lot of bells and whistles
how to really make this pipeline work
but in principle it's a very clean idea
right
where we want to predict these
properties i have this graphics renderer
and now i'm just comparing input and
output and because this is
this render is differentiable i can
propagate these slots back
to all my desired you know neural light
weights
so i can predict this these properties
okay now we in particularly had a very
simple
like opengl type render which we made
differentiable
there's also versions where you can make
rich racing differentiable and so on
but basically the idea that we employed
was super simple
right a mesh is basically projected onto
an image and you get out
triangles and each pixel is basically
just a butter centric
interpolation of the vertices of this
projected triangle
and now if you have any properties
defined of those vertices like color
or you know texture and so on
then you can compute this
value here through your you know
renderer
that assumes some lighting or so on um
in a differentiable manner using this
percentage coordinates this is a
differential function
and you can just go back through
whatever lighting or whatever
um shader model you're using
okay um so very simple and there's you
know much
much richer models that are available
richer differentiable renders available
these days but here we try to be a
little bit
clever as well with respect to data
because most of the
related work was taking synthetic data
to train their model why because most of
the work needed multi-view
data during training which means i have
to have multiple pictures from multiple
different views of the same object
and that is hard to get from just web
data right it's hard to get
so people will just basically taking
synthetic cars from synthetic data sets
and rendering in different views and
then training the model
which really just maybe makes a problem
not so interesting because now we are
actually relying on synthetic data to
solve this
and the question is how can we get data
and and and we try to be a little bit
clever here
and we turn to generative models of
images i don't know whether you guys
cover in class
uh you know image gans but if you take
something like stylogen
which is uh you know generative
adversarial network
designed to really produce high quality
images by by sampling from some some
prior
you get really amazing
pictures out like all these images have
been synthesized none of this is real
this is all synthetic okay
you know these guys basically what they
do is you have some latent
code and then there's a you know some
nice
progressive architecture that slowly
transforms that latent code
into an actual image
okay what happens is that
if you start analyzing this this latent
code or i guess i'm going to talk about
this one
if you take certain dimensions of that
code
and you try and you freeze them okay
and you just manipulate the rest of the
code it turns out that
you can find really interesting
controllers inside this latent code
basically the gun has has learned
about the 3d world and it's just hidden
in that latent code okay what do i mean
by that
so you can find some latent dimensions
that basically control the viewpoint
and the rest of the code is kind of
controlling the content
meaning the type of car and the
viewpoint means the
viewpoint of that car okay so if i look
at it here
we basically varied the viewpoint code
and kept the this content called the
rest of the code frozen
and and this is just basically
synthesized
and the cool part is that it actually
looks like you know multiple views of
the same object
it's not perfect like this guy the third
the third
object in the top row doesn't look
exactly matched but most of them look
like
the same car in different views
and the other side also also holds so if
i keep
a content like a viewpoint code fixed in
each of these columns
but they vary the the content code
meaning different rows here
i can actually get different cars in
each viewpoint okay so this is basically
again synthesized
and that's precisely the data we need so
we didn't do anything super special to
our technique the only thing we were
smart about was how we got the data
and and no now now you can use these
data to train our you know
differentiable
rendering pipeline and you got you know
predictions like this you have an input
image and
a bunch of 3d predictions but also now
we can do cars
so the input image on the left and then
the 3d prediction
rendered in that same viewpoint here in
this column
and that's that prediction rendered in
multiple different viewpoints just to
showcase the 3d nature of the
predictions
and now we basically have this tool that
can take any image and produce a 3d
asset so we can have tons and tons of
cars by just basically taking pictures
okay here is a little demo in that
omniverse tool
where the user can now take a picture
of the car and get out the 3d model
notice that we also estimate materials
because you can see the windshields are
a little bit transparent and the car
body looks like it's
shiny so it's metal because we're also
predicting 3d parts
and you know it's not perfect but
they're pretty good
and now just uh you know a month ago we
have a new version that can also animate
this prediction so you can take an image
predict this guard this guy
and we can just put you know tires
instead of the predicted tires
you can estimate physics and you can
drive these cars around
so they actually become useful assets
this is only on cars now but of course
the system is general so we're gonna
we're in the process of applying it to
sorts of different content
cool i think i don't know how much more
time i have
so maybe i'm just gonna skip today and i
have always too much slides
um so i have all these behaviors and
whatever
and i wanted to show you just the last
project that we did
because i think you guys gave me only 40
minutes um
so you know i we also have done some
work on animation
using reinforcement learning and
behavior that you know maybe i skipped
here
but we basically are building modular
deep learning
blocks for all the different aspects and
the question
is can we can we even sidestep all that
can we just learn how to simulate data
everything with one neural net and we
and we're going to call it neural
simulation
so can we have one ai model that can
just look at
our interaction with the world and then
be able to simulate that
okay so you know in computer games
we know that you know they accept some
user
action left right keyboard control or
whatever
and then the computer engine is
basically synthesizing the next frame
which is going to tell us you know how
how the world
has changed according to your action
okay
so what we're trying to attempt here is
to replace the game engine
with a neural net which means that we
still want to have the interactive part
of the of the game where the user is
going to be
inputting actions gonna be playing but
the screens are going to be synthesized
by a neural net
which basically means that you know this
neural net needs to learn
how the world works right if i hit into
a car it needs to
you know produce a frame that's gonna
look like that
okay now in the beginning our first
project was can we just learn how to
emulate a game engine right can we take
a pac-man and try to
mimic it try to see if the neural net
can can learn how to mimic pac-man
but of course the interesting part is
going to start where we don't have
access to the game engine
like the world right you can think of
the world as being the matrix
where we don't have access to the matrix
but we still want to learn how to
simulate
and emulate the matrix um and that's
really exciting future work
but basically we have you know now we're
just kind of trying to
mimic how what what a game engine does
where you're inputting some you know
action and maybe the previous frame
and then you'll have something called
dynamics engine which is basically just
an ls stand that's trying to learn
how the dynamics in the world looks like
how
how frames change uh we have a rendering
engine that takes
that latent code is going to actually
produce a nice looking image
and we also have some memory which
allows us to push any information that
we want
to be able to consistently produce you
know the consistent
gameplay uh in some some additional
block here okay
and and here's here was like our first
result on pac-man
and release this on the 40th birthday of
batman
[Music]
what you see over here is all
synthesized and the
to me is even if it's such a simple
simple game it's actually not so easy
because
you know the neural net needs to learn
that uh pac-man if it eats the food the
food needs to disappear if the ghost
can become blue and then if you eat a
blue ghost you
survive otherwise you die so there's
already a lot of different
rules that you need to recover along
with just like synthesizing images
right and of course our next step is can
we
can we scale this up can we go to 3d
games and can we eventually go
to the real world okay so again
here the control is going to be the
steering control so like speed and
the steering wheel this is done by the
user by a human
and what you see on the right side is
you know the frames painted
by by game gun by this model
so here we're driving this car around
and you can see what
what the model is painting
is is a pretty consistent world in fact
and there's no 3d there's no nothing
we're basically just synthesizing frames
and here is a little bit more
complicated version where
um we try to synthesize other cars as
well
and this is on a carla simulator that
was the game engine we're trying to
emulate
it's not perfect like you can see that
the cars actually change color
and resume it's quite amazing that it's
able to
do that entirely um
and right now we have a version actually
training on the real driving videos like
a thousand
hours of real driving and it's actually
doing an amazing job
already and you know so i think this
could be a really good alternative
on to the rest of the pipeline
all right you know one thing to realize
when you're doing something that's so
broad and such a big problem
is that you're never gonna solve it
alone you're never gonna solve it alone
so one one mission that i have is also
to provide tools to community
such that you know you guys can take it
and build your own
ideas and build your own 3d content
generation
methods okay so we just recently
released
3d deep learning is an exciting new
frontier
but it hasn't been easy adapting neural
networks to this domain
cowlin is a suite of tools for 3d deep
learning including a pi
torch library and an omniverse
application
cowlin's gpu optimized operations and
interactive capabilities
bring much needed tools to help
accelerate research in this field
for example you can visualize your
model's predictions as its training
in addition to textured meshes you can
view predicted point clouds
and voxel grids with only two lines of
code
you can also sample and inspect your
favorite data set
easily convert between meshes point
clouds and voxel grids
render 3d data sets with ground truth
labels to train your models
and build powerful new applications that
bridge the gap between images and 3d
using a flexible and modular
differentiable renderer
and there's more to come including the
ability to visualize remote training
checkpoints in a web browser
don't miss these exciting advancements
in 3d deep learning research
and how cowlin will soon expand to even
more applications
yeah so a lot of the stuff i talked
about all the basic tooling is available
so you know please take it and do
something amazing with it
i'm really excited about that just to
conclude
you know my goal is to really become
democratized 3d content creation you
know i want my mom to be able to create
really good 3d models and she has no
idea even how to use microsoft word or
whatever so it needs to be super simple
um have ai tools that are going to be
able to also assist maybe more advanced
users like artists game developers
but just you know reduce the load of the
boring
stuff just enable their creativity to
just come to play much faster than
it can right now um and all that is also
connecting to learning to simulate for
robotics simulation is just a
fancy game engine that needs to be real
as opposed to being
from fan fantasy but it can be really
really useful for robotics applications
right and what we have here is really
just like
two years and a half of our lab but you
know there's so much more to do and i'm
really hoping that you guys
are gonna do this i just wanted to
finish with one
slide because you guys are students um
my advice for
for research um you know just learn
learn learn
this deep learning course is one don't
stop here continue
um one in very important aspect
is just be passionate about your work
and never lose that passion because
that's where
you're really going to be productive and
you're really going to do good stuff
if you're not excited about what the
research you're doing though you know
choose something else through something
else don't rush for papers
focus on getting really good papers as
opposed to the number of papers that's
not a good metric
right hunting citations maybe also not
the best metrics right some
some not so good papers have a lot of
citations some good papers don't have a
lot of citations
you're going to be known for the good
work that you do
um find collaborations find
collaborators and that's particularly
kind of in my style of research i want
to solve
real problems i want to solve problems
which means that
how to solve it is not clear and
sometimes we need to go to physics
sometimes we need to go to graphics
sometimes we need to go to
nlp whatever and i have no idea about
some of those
domains and you just want to learn from
experts so it's really good to find
collaborators
and the last point which you know i have
always used as guidance
it's very easy to get frustrated because
99
of the time things won't work but just
remember to have fun
um because research is really fun and
that's all from me whether you guys have
some questions
i've been at google for
16 years um the last six years i've been
in life sciences and healthcare
i i generally like running more
interactive classes
given the size of the group we thought
polls might work
so i'll launch a couple of polls
throughout the talk
and i'll try and keep an eye on chat as
well if you guys have questions but
um i might save them for the end as well
so let me talk through the agenda a
little bit
i'm hoping to give you uh
some information about ai in particular
deep learning and healthcare
and i will be using ai and deep learning
interchangeably because that's just
the name of our team is google ai
but the examples you'll be seeing are
all deep learning examples
and as you know ai does include other
things
like robotics and non-neural network
approaches so
i just wanted to be clear that when i
use them i don't need to be conflating
them entirely
once i cover what some of the key
applications are for
what we've done in ai and healthcare i'd
like to discuss with you
what the kind of unique opportunity i
think we have
because of deep learning to be able to
uh
create a much more equitable society
while we're deploying
ai models and we can talk about how
that's possible
and finally i'll touch on one last set
of applications for ai and healthcare at
the end here
so on the in terms of
uh the history behind
ai and healthcare we are benefiting from
the
fact that we have uh the maturation of
deep learning
um and especially the end-to-end
capabilities where we can learn directly
from the raw data
this is extremely useful for advances in
computer vision and speech recognition
which is highly valuable in the field of
medical
the other area as you all know is the
increase in localized compute power via
gpus um so that's allowed for neural
networks to outperform
non-neural networks um in the past
and then the third is the value of all
these open source
large label data sets and internet being
one for non-health related areas but
there is
uh public data sets like uk bio bank
and even mimic which has been truly
helpful
and it's uh was developed actually um
and produced
at the mit labs so
you'll be hearing about some of the
applications of ai in healthcare next
uh one of things that we do is to make
sure we look at
the needs in the industry and match that
up to the tech capabilities
healthcare specifically has enormous
amounts of
complex data sets annually it's
estimated to be generating on the order
of several thousand exabytes of
healthcare data
a year um just to put that in
perspective a bit
it's estimated that if you were to take
the internet data
um that's around something with more
like hundreds of exabytes
so it's it's several thousand times more
um
and what we're looking at in terms of uh
those applications you'll see in bit
is the pattern detection um and the
ability to recognize
for things like lesions and uh tumors
and
um really nuanced subtle imagery
another area that it's useful for is
just the
addressing the limited medical expertise
globally
if you look to the right what you'd like
to see is
uh one medical specialist like a
radiologist to about 12 000
people in the population but and
what you can see on the graph to the
right is that in developing countries
it looks more like one to a hundred
thousand or one to a million even
and so the benefit of ai and healthcare
is that it can help scale up
running some of these complex tax tasks
that are valuable
that middle experts are capable of
the third is uh really addressing human
inconsistencies and we'll talk a little
bit about
this especially when we're talking about
generating labels um ai models don't
obviously suffer from
recency or cognitive biases and they are
also
able to work tirelessly which is an
issue when
when you have to work overtime uh as
in medical field which often happens
let me just talk a little bit through
the next application which is lung
cancer
uh the application what we developed was
a computer diagnostic
uh and in this case it was to help
screen
uh for lung cancer using low-dose ct
scans
um what you normally see um is
the survival rates increasing
dramatically if you catch it at earlier
stages
but about 80 percent of lung cancers are
not caught early
and what they use usually to do these
screenings are these
low-dose ct scans that if you look in
this diagram
to the right is these three-dimensional
uh
imaging that happens to your entire body
it creates hundreds of images for the
radiologists to look through
and uh typically the
actual um uh
lung cancer signs are very subtle so
what our models were able to do
um when we looked at this was to
actually
not just outperform the state of the art
but actually more importantly we
compared it to the radiologists to see
if there was an absolute reduction in
both false positives
and false negatives so false positives
will lead to
overutilization of the system and false
negatives will lead to
uh not being able to catch the cancer
early enough
and usually want to see both both
reduced
pathology is another area that's a hard
deep learning problem
and even more complex data this is
on the left you can see when you take a
biopsy you have slices of
the body tissue and these are magnified
up to 40 times
and creates about 10 to 15 megapixels of
information per slide
the part that is inherently complex is
when you're doing pathology you want to
know both the
magnified uh level highly magnified
level of this
tissue so that you can characterize the
lesion and you also need to
understand um the overall tissue
architecture
to provide context for it um and so
that's at a lower power so you have a
multi-scale problem
and it is also inherently complex
to be able to differentiate between
benign and malignant tumors
i there's hundreds of the different
pathologies that can affect the
tissue and so being able to visually
differentiate is very challenging
we built the model to detect breast
cancer from
pathology images and the pathologists
actually had no false positives
the model was able to capture more of
the cancer lesions so it was greater
than 95
compared to 73 that pathologists were
getting but it also increased the number
of false positives
um this meant that what we tried uh then
was to actually combine
uh and have the model and pathologists
work together um to see if the accuracy
could improve and it absolutely did
um and this com combined effort led to
also
development of an augmented microscope
where you can see the model
um detecting the patches inside the
microsoft microscope view itself
and we'll come back to the fact that the
models had certain weaknesses
and how we dealt with that later uh
genomics is another area that's uh
benefited significantly from
uh deep learning um it's worth noting
that
uh when you do um whole genome sequences
what you're you're doing is tearing up
your um dna into
a billion reads of about 100 bases i
mean there's about a 30x over sampling
with errors when you do that
um when you try and uh figure out the
sequence what you're trying to do is
something like
i take 30 years of a sunday newspaper 30
copies each with
errors introduced and then shred them
into 20 word snippets and then you try
and put them back together
that's essentially what's happening when
you're doing your sequencing
um and so we recast this problem as a
deep learning problem
uh we looked at how
image recognition and and specifically
convolutional
neural networks would be able to perform
in the space
and developed a tool called deep variant
which is open sourced and
available for for anyone to use
and we've been improving it over time
this is
proven to be uh highly accurate um
the usfda runs a precision fta
competition
every few years and it's uh outperformed
uh mostly won the awards for three out
of four
accuracy areas and you can see on the
right that when you
it's quite visually obvious when you
actually get an
error a false variant in the sequencing
um so this is a clever way to actually
be able to
rapidly detect errors and variant calls
so we talked about the different needs
that um are in the medical field and one
of them was
the limited medical expertise there's
one way to help them which is scaling up
the tasks that they run so that they can
be automated this is another way of
addressing it which is returning time to
the doctors
what's happened is what you're seeing on
in this picture is a girl who drew
um her experience when visiting a doctor
you can see the doctor's actually facing
the computer to the left this sparked a
lot of discussion within the health care
industry about the cost of technology
and how it's interfering with patient
care
um the doctors now at this point spend
something on the order of six hours a
day interacting with their electronic
health records to get
data entered one of the areas that's
ripe for being able to support
medical doctors is
scribes human scribes have been deployed
medical dictation has
gone much better the automatic speech
recognitions now have end-to-end models
they're highly accurate
and it's improved significantly also on
natural language processing
so these are all ways that um is more
like an assistive kind of ai
to help doctors relieve the burden of
documentation from them
i'm going to launch the poll right now
just to see what people
think is the most valuable application
let me see here if
i can do that and
as i just to quickly recap there was um
computer diagnostics which are useful
for screening and diagnoses
um there is and that was demonstrated
with the radiology
um there was approved prognosis um
that's
pathology is useful for um determining
therapeutics
um being able to determine treatment
efficacy and and the progression of the
disease
um and that's what both pathology and
genomics is highly utilized for
and then returning time to experts is
really the ai assistance through medical
dictation describing
okay great so let me just keep going
while the is going um
i want to talk about how we can actually
achieve a greater moonshot so let me
take a step back here where um
we look at how the healthcare the world
of healthcare looks right now
it's tremendously filled with
fragmentation
it's fairly impersonal and it's
inequitably distributed
and one of the things i noted was that
in tech we do amplify
a system if you apply it to it so tech
is is
a way to both augment and scale up what
exists
um and so if you have if you're applying
it to a broken system
with perverse incentives it won't fix
the system inherently it will accelerate
it
um but at the core of machine learning
um and
these deep learning technologies what
we're doing is uh
we're looking at the data very carefully
and uh
utilizing that to build predictions and
um and determine
outcomes in this case given that the
world is not full of equity
you run the risk of training the wrong
models
we published also a paper to help
address this
so societal inequities and biases are
often codified in the data that we're
using
we actually have the opportunity to
examine those historical biases and
proactively promote
a more equal future when we're
developing
the models you can do that by correcting
for bias in the training data
you can also uh correct bias in the
model design
and the problem formulation which
and what you're trying to solve for and
we'll talk about that in a bit
uh and and finally if if none of that
is applicable then you all you can also
test and ensure for equal outcomes and
resource allocations
at the end of when you're deploying the
ai models
so this is um i used to work in
google x which is
google's effort to do moon shots the way
we design define
moon shots is the intersection of a huge
problem
breakthrough technology and radical
solutions
and a huge problem here is that the
world
is uncertain impersonal and it's also
entire accuracy we have the benefit of a
breakthrough tech right now which is
ai and deep learning um and i'm just
gonna say
digital mobile tools is actually
breakthrough tech for healthcare because
they tend to lag about a decade behind
other industries do the regulations
safety privacy and quality needs
and so a radical solution here is is we
we actually think about not just
improving the quality
of care that we're delivering but making
sure that when we do that we also make
it more equitable
um and i there's
at every point in time when i see a
technological wave happen
um i do realize that at this point that
it's an opportunity for us to reshape
our future
so in the case of um deep learning
i i'd like to talk about the
opportunities
for actually moving um so i didn't
realize
the slides weren't advancing i want to
talk about the opportunity to actually
make the ai models much more
um equitable and how we would do that
so the two key areas i'll talk about is
community participation
um and how that's gonna affect the
models and in the data
evaluation um and then also planning for
model limitations and how you can do
that
effectively with ai
one of the things that we did was work
with the uh
regions that we were directly going to
deploy the models
with and so on the left here you see us
working with um the team in india
and on the right it's our team working
with those in in thailand
i what we found was that the social
economic
situation absolutely mattered in terms
of where you would deploy the model
um an example is while we developed the
model
with the ophthalmology
centers and that's where the eye disease
is happening and diabeteopathy is the
leading cause of growing cause of
blindness
um in the world and this is where the
models were developed but actually the
the use case was most acute in uh
the diabetic centers so the
endocrinology offices
um and people were not making the 100
meter distance to be able to go from
the endocrinology offices to the
ophthalmology offices
because of access issues and uh
challenges with
um with lines and and and so on so forth
so
so this is an area that we explo uh
explored using user research extensively
to make sure that
we thought through where the ai models
would land and how that would impact
the users
another area that we looked at
is when we're generating labels for the
models you can see on the left
that as classically you you would expect
when you get more data on the models
continues to improve so
it kind of flattens out here at 60 000
images
and and at some point that's sufficient
and you're not going to get much more
improvement from that um what you
actually benefit from if you look to the
right graph
is improvement of the what we the
quality of the labels or the
what we refer to as the grades on the
images um
each doctor gives an image and grade
which is their diagnostic opinion
of what they think they're seeing um
as we got multiple opinions on single
images and were able to reconcile that
we were able to continuously improve
the model output um and improve the
accuracy
so this is
something that's often said in the
healthcare spaces if you
ask three doctors you get four opinions
because
um even the doctors themselves may not
be consistent with themselves over time
uh the way that this is addressed um in
some
countries is to use this delphi method
which is
which was developed during the cold war
it helps
determine consensus where individual
opinions vary
and we developed a tool to do
asynchronous adjudication of different
opinions
this has led to much higher ground truth
uh
data creation and and it's because of
the fact that doctors actually
sometimes just miss what what another
doctor notices and so they
generally do reconcile and are able to
come to agreement
on what the actual um severity or
diagnosis should be
um and so this was something that we saw
uh really and that was really impactful
because
um when we were doing the analysis with
ophthalmologists we would see things
like 60
consistency across the doctors um and
this was
a way to actually address that level of
variance
and here's the last area um that i want
to talk about for community engagement
um this is around if you go even further
upstream
to the problem formulation this is a
case where
they didn't think through uh the inputs
to their models and algorithms
this algorithm was trying to determine
the utilization needs of
the community and they were using the
health costs as a proxy for the actual
health needs
um this has led to uh
inadvertently a racial bias because less
money was spent on black patients
and um and this was caught after the
fact
and so if you just click one more time
um
this is this is one of the key areas
where um
having input from the community uh would
have actually got that much earlier on
when they were doing the algorithm
development this is something that we
practice now frequently um and i know
you guys are working on projects so
i it'd be uh one of the polls that i
want to put out there which just
let me see if we can get it to launch is
uh which one of these
um uh approaches are actually
potentially
relevant at all for um the
projects that you guys are working on
okay great i'll keep going with the
toxin while this is being saved and be
nice to look back on it
i on the left here um i mentioned
earlier how our pathology
models had certain weaknesses
um in terms of false positives um
but it also was capturing more of the
cancer allegiance than the pathologists
were so we developed
a way to try to explain the models um
through similar image lookup
and what this has allowed to have happen
was um it showed it uses a cluster
algorithm and
is able to find features that were not
known um
as before to pathologists
that might be meaningful indicators of
the actual uh
diagnosis or prognosis um and in this
case
uh pathologists have started to use the
tool
to learn from it and um and there's
also the benefit of the pathologist
being able to recognize any issues with
the models and
inform the models to improve so you get
a virtuous cycle
of uh the models and the pathologists
learning from each other
on the right is another way that we
use to explain the model output you see
saliency maps
um which is a way to just be able to
identify which features are
the model is actually paying attention
to and in this case which pixels
um the model is paying attention to and
and light those up
we do this so that we know that the um
the way that the model is actually
determining
the uh the diagnostic
whether it's a particular skin condition
um
that they're looking at the actual skin
abnormalities and not some
side potential unintentional correlation
to skin tone
or our demographic information and so
this has been valuable to you
as a way of checking the models that way
and the last that i mentioned is is
doing monolith
model evaluation for equal outcomes um
there's something
for in the dermatology space known as
the fitzpatrick skin type
it allows you to see the different skin
tones and what we do
is to have test sets that are in the
different skin tones to do the
model evaluation to see if we do get
equal outcomes
and this is something where as
the model developer and you have to make
some hard choices
if you see that your models aren't
performing well
off a particular um category or
demographic
i ideally what happens is you supplement
your data set so that you can
improve your models further to
appropriately address those
regions or you may have to make a
decision to limit your model
output so that there can be equal
outcomes
and sometimes you don't actually choose
not to deploy the models
and so these are some of the kind of
real world implications
of developing ai models in the
healthcare space
the last application i wanted to talk
through with this group is
the concept of health care typically in
the past healthcare
is thought of for patients
and while every patient is a person not
every person is a patient
and patients typically are considered
on the left here people who are sick or
at risk
they're entering the health care system
the models are quite different when
you're thinking about
people of this nature whether they have
acute or chronic diseases
and they're the ones that we talked
about a bit earlier which are screening
diagnostics
prognosis treatment those are what the
models tend to focus on
um if you're looking at people
uh they are considered quote unquote
well
um but their health is impacted every
single day by
what we call like social determinants of
health which are your environmental and
social circumstances
your behavioral and lifestyle choices
um and uh how your genes are interacting
with the environment
and um the models here look dramatically
different in terms of how
you would approach the problem uh they
tend to focus on preventative care
so eating while sleeping well exercising
and they also focus on public health
which i think
has been a big um known issue now with
coronavirus and and of course screening
is actually
consistent across both sides so
um when we talk about uh
public health there's can be things like
epidemiological models
which are extremely valuable but there's
also
things that are happening right now
especially probably one of the biggest
global threats to public health is
climate change
and so one of the things that's
happening in places like
india is flood forecasting uh for public
health alerts
in in india there's a lot of alert
fatigue actually and so
it's it's actually unclear when they
should care about the alerts or not
what this team did was they focused on
building a scalable high resolution
hydraulic model
using convolutional neural nets to
estimate inputs like
snow levels soil moisture estimation and
permeability
these hydraulic bundles uh simulates the
water behavior
across floodplains um and were high far
more accurate than what was being used
before
and this has been deployed now to help
with
alerting and across the india regions
for during the monsoon seasons
um let's see
and so i just want to leave this group
with with the idea that uh
on the climate change side there is a
lot going on right now
um nature is uh essential to health and
plant but also the people that live on
it so
uh we currently rely on these ecosystem
services
and what that means is people rely on
things like
clean air water supply pollination of
agriculture for food
land stability and climate regulations
and this is an area that's ripe for ai
to be able to
help understand far better and value
those services
um that we currently don't pay a lot for
um but we'll probably have to in the
future
and so this last slide
let me see if we can get it to appear is
for the pull
and just i wanted to compare and
understand if the perception around
health is
any different in terms of what might be
the most exciting for
uh ai to be applied to thanks for
launching the final poll
and the last thing i want to leave the
group with is
none of the work that we do in the in
healthcare is
possible there's a huge team and a lot
of collaboration that happens across
medical research organizations and
research institutes and healthcare
systems um and so that is
uh you know this is our team as it's
grown over the years and
um in different forms it's not all of
our team even
anymore but this is certainly where a
lot of the work has been
generated from
let me take a look at the questions in
chat now
and so that i'll just recap what the
pull results were so it looks like
um the diagnostic models um
uh 54
hmm oh yeah so i guess you guys can do
multiple
choice uh you can pick multiple ones so
50 but 60 people
half the people um felt that the
diagnostics and the
therapeutics were valuable um and less
interested in
but still valuable the assistance
thanks for filling those out um
definitely let me look at the questions
given the fast advancement in terms
of new models what is the main
bottleneck to
scaling ml diagnostic solutions to more
people around the world
i it's the regulatory
automation of meeting the regulatory
needs
um the long pull for diagnostics
is ensuring patient safety
proper regulation usually you go through
fda rce marks
and that can take uh time there's
quality management systems that have to
be built to make sure that
the system is robust from a development
perspective so it's
software as a medical device
and this is always going to be true in
the case of
when you're dealing with patients
in terms of the other part maybe is the
open source
data sets having more labeled data sets
out there
so that uh everyone can have access and
move that space forward uh
is is valuable second question
here good day sets are essential to
developing useful equitable models
what effort and technologies do we need
to invest in to continue collecting data
sets and for more models
uh so
one of the things that's happening is
developing a scalable
labeling infrastructure um that's that's
one way to
be able to generate
better data sets but raw data is also
ones that are directly
reflecting the outcomes is valuable so
an example is if you're thinking about
data that's coming straight from the
user in terms of
their vital signs or their
physiological signals these are things
that
are as close to ground truth as you can
get about
the individual's well-being and um
and obviously uh what we saw with cobia
19
was it was even harder to get
information like
um how many deaths were actually
happening
um and what were the causes of those
deaths and so
these are the kinds of data sets that
need to
um these pipelines need to be thought of
in the context of
how can they be supporting public health
goods
and and how does that did it accurately
get out the door so we do have
an effort right now that um lots of
people pulled into you especially for
coronavirus which was
um uh on github now which i can provide
a link for later
um and it's volunteers um
who have built a transparent data
pipeline
for the source of the data provenance is
very important to keep track of when you
create these data sets to make sure
you know where what what the purpose of
the data is and
who's how reliable it is and where the
source is coming from
so these are the kinds of things that
need to be built out to inform the
models that you build
this last question how do you facilitate
conversation awareness around potential
algorithmic bias
related to the products you were
developing
um several things
one is that the team you build as much
as
of them that can be reflective of uh
the be representative of a popular
broader population
is actually more meaningful than than i
think people realize
um there's so what i mean by that is if
you have a diverse team working on it or
you
bring in people who uh can be
contributors or
um part of a consortium that can reflect
on
the problem space that you're trying to
tackle that is actually
um a really good way to hear and
discover things that you might
not have ever thought of before but
again
it can start with the team that you
build and then the
network around you that you are actually
getting feedback loops from
and you know if you can't afford it
uh you you would want to do that in a
way that
is quite measurable and quantitative but
if you can it's
it's actually still quite meaningful to
you um
to proactively just have these
conversations about what space you're
going into and how you're going to think
about the inputs to your models
um all right so thank you
uh it looks like those were majority of
questions